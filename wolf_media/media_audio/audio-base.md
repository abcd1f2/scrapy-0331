## media-audio audio base
- **概述：**
>       主要讲解音频的相关概念
>
>       奈奎斯特采样定理，若要使信号无失真保留，采样频率须高于两倍信号带宽。采样频率越高，音质越好，但是数据量越
>
>

- **采样：**
>       根据奈奎斯特抽样理论，数字音响系统可恢复的音响频率只能达到采样频率的一半。即用44kHz的采用频率对声音进行采样时，那么录制的声音最高频率只有22kHz
>       量化位数（采样精度）：
>           8位和16位两种
>       声道数：
>           声道数是指一次采样所记录产生的声音波形个数，分为单声道和双声道。
>           如果是单声道，则只产生一个声音波形，双声道（立体声）产生两个声音波形
>

- **webRTC音频处理：**
>       webRTC音频处理：
>           1、回声消除
>               AEC和AECM算法
>           2、噪声抑制
>               噪声频谱可以使用如语音/噪声似然函数进行估计。将接收到的每帧信号和频率分量分类为噪声或语音
>           3、自动增益
>               最简单的硬性增益处理是对所有音频采样乘上一个增益因子，它也等同于在频域每个频率都同时乘上这个增益因子，但由于人的听觉对所有频率的感知不是线性的，
>                   是遵循等响度曲线的，导致这样处理后，听起来感觉有的频率加强了，有的频率削弱了，导致语音失真的放大
>               目的：
>                   手机等设备采集的音频数据往往有时候响度偏高，有时候响度偏低，造成声音忽大忽小，影响听众的主观感受。
>                   自动增益控制算法根据预先配置的参数对输入声音进行正向/负向调节，使得输出的声音适宜人耳的主观感受。
>           4、活动检测
>               检测语音（GMM）
>               静音检测的基本原理：计算音频的功率谱密度，如果功率谱密度小于阈值则认为是静音，否则认为是声音。静音检测广泛应用于音频编码、AGC、AECM等
>           5、舒适噪声
>               舒适噪声产生的基本原理：根据噪声的功率谱密度，人为构造噪声。广泛适用于音频编解码器。
>               在编码端计算静音时的白噪声功率谱密度，将静音时段和功率谱密度信息编码。在解码端，根据时间信息和功率谱密度信息，重建随机白噪声
>               目的：
>                   完全静音时，为了创造舒适的通话体验，在音频后处理阶段添加随机白噪声
>


- **音频基础概念：**
>       音调：
>           泛指声音的频率信息，人耳的主观感受为声音的低沉（低音）或者尖锐（高音）
>           与振动的基本频率有关，频率越大，音高越高
>       音频：
>           指人耳可以听到的声音频率在20HZ~20kHz之间的声波，称为音频
>       响度：
>           声音的强弱
>           音乐的强弱。与振动幅度有关，振动幅度越大，响度越大。常以分贝（dB）为单位
>       音色：
>           声音的特质
>           不同乐器、演唱者发出的声音，其音色是不同的，基本与周期内波形有关，也就是之前所述的波形叠加的组成分量、大小不同
>       采样率：
>           声音信息在由模拟信号转化为数字信号过程中的精确程度，采样率越高，声音信息保留的越多
>           取样频率, 指每秒钟取得声音样本的次数。采样频率越高,声音的质量也就越好,声音的还原也就越真实，但同时它占的资源比较多。
>               由于人耳的分辨率很有限,太高的频率并不能分辨出来。22050 的采样频率是常用的, 44100已是CD音质。
>           数字音频领域，常用的采样率有：
>               8,000 Hz - 电话所用采样率, 对于人的说话已经足够
>               11,025 Hz
>               22,050 Hz - 无线电广播所用采样率
>            　 32,000 Hz - miniDV 数码视频 camcorder、DAT (LP mode)所用采样率
>            　 44,100 Hz - 音频 CD, 也常用于 MPEG-1 音频（VCD, SVCD, MP3）所用采样率
>            　 47,250 Hz - Nippon Columbia (Denon)开发的世界上第一个商用 PCM 录音机所用采样率
>            　 48,000 Hz - miniDV、数字电视、DVD、DAT、电影和专业音频所用的数字声音所用采样率
>            　 50,000 Hz - 二十世纪七十年代后期出现的 3M 和 Soundstream 开发的第一款商用数字录音机所用采样率 50,400 Hz - 三菱 X-80 数字录音机所用所用采样率
>            　 96,000 或者 192,000 Hz - DVD-Audio、一些 LPCM DVD 音轨、BD-ROM（蓝光盘）音轨、和 HD-DVD （高清晰度 DVD）音轨所用所用采样率
>            　 2.8224 MHz - SACD、 索尼 和 飞利浦 联合开发的称为 Direct Stream Digital 的 1 位 sigma-delta modulation 过程所用采样率
>       采样精度：
>           声音信息在由模拟信号转化为数字信号过程中，表示每一个采样点所需要的字节数，一般为16bit（双字节）表示一个采样点
>           采样值或取样值(就是将采样样本幅度量化)。它是用来衡量声音波动变化的一个参数，也可以说是声卡的分辨率。它的数值越大，分辨率也就越高，所发出声音的能力越强。
>           每个采样数据记录的是振幅, 采样精度取决于采样位数的大小：
>               1、1字节(也就是8bit) 只能记录256个数, 也就是只能将振幅划分成256个等级
>               2、2字节(也就是16bit) 可以细到65536个数, 这已是CD标准了
>               3、4字节(也就是32bit) 能把振幅细分到4294967296个等级, 实在是没必要了
>           如果是双声道(stereo), 采样就是双份的, 文件也差不多要大一倍
>       声道数：
>           即声音的通道的数目。
>           常有单声道和立体声之分，单声道的声音只能使用一个喇叭发声（有的也处理成两个喇叭输出同一个声道的声音），
>               立体声可以使两个喇叭都发声（一般左右声道有分工） ，更能感受到空间效果，当然还有更多的通道数。
>           相关的几路声音数量，常见的如单声道、双声道、5.1声道
>       帧：
>           帧记录了一个声音单元，其长度为样本长度(采样位数)和通道数的乘积
>       帧长：
>           每帧内的样本点数，即“每帧时间长度×采样频率”
>           每帧时间大约为10—30ms，过大无法抓住帧间差别，即音乐随时间变化的总体特性；过小则无法抓出帧内声音信号的特征
>           一般而言，帧长必须能够包含数个声音信号的基本周期。此外，帧长通常取为2的整数次幂，若不是，则在进行傅立叶变换时，需补零至2的帧数次幂，以便使用FFT，这是由FFT的算法决定的
>       帧率：
>           每秒出现的帧数。等于采样频率除以帧距
>           是用于测量显示帧数的量度
>           所谓的测量单位为每秒显示帧数(Frames per Second，简称：FPS）或“赫兹”（Hz）
>       周期：
>           音频设备一次处理所需要的帧数，对于音频设备的数据访问以及音频数据的存储，都是以此为单位
>       音频帧长：
>           音频处理或者压缩所操作的一段音频信息，常见的是10ms，20ms，30ms
>       交错模式：
>           数字音频信号存储的方式。数据以连续帧的方式存放，即首先记录帧1的左声道样本和右声道样本，再开始帧2的记录
>       非交错模式：
>           首先记录的是一个周期内所有帧的左声道样本，再记录所有右声道样本
>       比特率：
>           每秒的传输速率(位速, 也叫比特率)。如705.6kbps或705600bps, 其中的b是bit, ps是每秒的意思，表示每秒705600bit的容量
>       码率（Bit Rate）：
>           指视频或音频文件在单位时间内使用的数据流量，该参数的单位通常是Kbps，也就是千比特每秒
>       音频解码器和功放前后级：
>           音频解码器是将二进制的数字编码信息解译为模拟信号（逻辑电平），来输出给功放或有源音箱产生声音
>           音频解码器分硬解码器和软解码器二种。软解码器是以软件的方式实现解码，如计算机中安装的音频播放软件中就带着软解码器。
>               而硬解码器则是将解码的工作交给特定的解码芯片来完成
>       播放时间：
>           一个AAC原始帧包含一段时间内1024个采样及相关数据，用1024主要是因为AAC是用的1024点的mdct，mp3每帧均为1152个字节
>           AAC：
>               音频帧的播放时间=一个AAC帧对应的采样样本的个数/采样频率(单位为s)
>           MP3：
>               每帧播放时长 = 1152 * 1000 / sample_rate （单位：ms）
>       帧重叠：
>           帧之间重叠的样本点数
>           如果希望相邻帧之间的变化不是太大，可以允许帧之间有重叠，重叠部分可以是帧长的1/2到1/3不等。重叠部分越多，计算量越大
>       帧距：
>           此帧起点与下帧起点之间的距离点数
>           当不存在跳帧，即每个样本点至少位于一帧中的情况下，帧重叠不小于零，帧距等于帧长减去帧重叠
>
>

- **音频特征处理：**
>       其中音高、音色从时域都很难分析，这就需要用到频域分析。
>       最常用的方法是进行快速傅立叶变换（FFT），将信号从时域转到频域上进行处理，它可以分析出每帧信号在不同频率分量上的强度。
>       对提取特征而言，做法是将音乐分帧，对每一帧进行频谱分析，算出每帧的信号如何拆解成在不同频率的分量，然后再进行比对或进一步操作
>
>
>
>

- **音频编码基础：**
>       音频的另一个广泛应用的领域：音频编码
>       1、固定码率的编码标准
>           如G.711或者G.722，说明这两个编码标准是固定码率编码标准
>           其他如Opus、Speex，这类编码标准是可变码率的编码标准
>       2、频带方面看
>           G.711、G.722、AMR和iLBC等标准适用于narrowband（8khz采样率）和wideband（16khz采样率）范围，针对普通的语音通话场景
>           AAC和MP3适用于fullband（48khz采样率）范围，针对特殊的音乐场景
>           Opus适用于整个频带，可以进行最大范围的动态调节，适用范围最广
>       3、收费情况
>           适用于互联网传输的iLBC、Speex和Opus都是免费且开源的
>           适用于音乐场景的MP3和AAC，需要license授权，而且不开源
>
>

- **编码的基本手段：**
>       常见的编码基本手段：
>           1、量化
>           2、量化器
>       量化和量化器：量化是把离散时间上的连续信号，转化为离散时间上的离散信号
>       常见的量化器：均匀量化器、对数量化器、非均匀量化器
>       量化过程追求的目标：最小化量化误差，并尽量减低量化器的复杂度（两者比较矛盾）
>
>       常见的量化器的优缺点：
>           1、均匀量化器：最简单，性能最差，仅适用于电话语音
>           2、对数量化器：比均匀量化器复杂，也容易实现，性能比均匀量化器好
>           3、非均匀量化器：根据信号的分布情况，来设计量化器。信号密集的地方进行细致的量化，稀疏的地方进行粗略量化
>

- **语音编码器：**
>       语音编码器分为三类：
>           1、波形编码器
>               波形编码器以构造出背景噪单在内的模拟波形为目标。作用于所有输入信号，因此会产生高质量的样值并且耗费较高的比特率。
>               而声码器 （vocoder）不会再生原始波形。这组编码器会提取一组参数，这组参数被送到接收端，用来导出语音产生模形。声码器语音质量不够好。
>               混合编码器，它融入了波形编码器和声器的长处
>           2、声码器
>           3、混合编码器
>
>       1、波形编码器：
>           波形编码器的设计常独立于信号。所以适应于各种信号的编码而不限于语音。
>           (1)、时域编码：
>               PCM：pulse code modulation
>                   是最简单的编码方式。仅仅是对信号的离散和量化，常采用对数量化
>               DPCM：differential pulse code modulation
>                   差分脉冲编码，只对样本之间的差异进行编码。前一个或多个样本用来预测当前样本值。用来做预测的样本越多，预测值越精确。真实值和预测值之间的差值叫残差，是编码的对象
>               ADPCM：adaptive differential pulse code modulation
>                   自适应差分脉冲编码。即在DPCM的基础上，根据信号的变化，适当调整量化器和预测器，使预测值更接近真实信号，残差更小，压缩效率更高。
>
>           (2)、频域编码：
>               频域编码是把信号分解成一系列不同频率的元素，并进行独立编码
>               sub-band coding：
>                   子带编码是最简单的频域编码技术
>                   是将原始信号由时间域转变为频率域，然后将其分割为若干个子频带，并对其分别进行数字编码的技术。
>                   它是利用带通滤波器(BPF)组把原始信号分割为若干(例如m个)子频带(简称子带)。将各子带通过等效于单边带调幅的调制特性，将各子带搬移到零频率附近，分别经过BPF(共m个)之后，
>                   再以规定的速率(奈奎斯特速率)对各子带输出信号进行取样，并对取样数值进行通常的数字编码，其设置m路数字编码器。将各路数字编码信号送到多路复用器，最后输出子带编码数据流。
>                   对不同的子带可以根据人耳感知模型，采用不同量化方式以及对子带分配不同的比特数
>               transform coding：
>                   DCT编码
>
>       2、声码器
>           channel vocoder:
>               利用人耳对相位的不敏感
>           homomorphic vocoder：
>               能有效地处理合成信号
>           formant vocoder:
>               以用语音信号的绝大部分信息都位于共振峰的位置与带宽上
>           linear predictive vocoder：
>               最常用的声码器
>
>       3、混合编码器
>           波形编码器试图保留被编码信号的波形，能以中等比特率（32kbps）提供高品质语音，但无法应用在低比特率场合。
>           声码器试图产生在听觉上与被编码信号相似的信号，能以低比特率提供可以理解的语音，但是所形成的语音听起来不自然。
>           混合编码器结合了2者的优点：
>               (1)、RELP:
>                   在线性预测的基础上，对残差进行编码。机制为：只传输小部分残差，在接受端重构全部残差（把基带的残差进行拷贝）
>               (2)、MPC：multi-pulse coding
>                   对残差去除相关性，用于弥补声码器将声音简单分为voiced和unvoiced，而没有中间状态的缺陷
>               (3)、CELP：codebook excited linear prediction
>                   用声道预测其和基音预测器的级联，更好逼近原始信号
>               (4)、MBE：multiband excitation
>                   多带激励，目的是避免CELP的大量运算，获得比声码器更高的质量
>

- **语音通讯编码标准：**
>       目前传统的视频通讯中主要采用的是G.711，G.722，G.721，G.728等音频标准，音频宽度仅有50Hz~7kHz单声道。
>       目前国际上对音频处理技术上标准较多，对下一代实时交互音频处理上可以采用MPEG-1 Layer 2或AAC系列音频
>

- **待续：**
>       参考：https://blog.csdn.net/aoshilang2249/article/details/38469051     音频 属性详解(涉及采样率、通道数、位数、比特率、帧等)
>           http://www.52im.net/thread-242-1-1.html     即时通讯音视频开发（七）：音频基础及编码原理入门
>           http://www.52im.net/thread-243-1-1.html     即时通讯音视频开发（八）：常见的实时语音通讯编码标准
>           https://github.com/shichaog     WebRTC-audio-processing
>           https://shichaog1.gitbooks.io/hand-book-of-speech-enhancement-and-recognition/content/  该书分成三个部分，分别是语音增强、工程实现以及语音识别
>           https://www.cnblogs.com/cpuimage/category/1147362.html  音频处理（含WebRTC音频采样算法、基于RNN的音频降噪算法等）
>           https://www.cnblogs.com/mod109/category/1057295.html
>           https://zhuanlan.zhihu.com/p/44576758   音频处理与压缩技术
>           http://blog.sina.com.cn/s/blog_5d054eff0100puzx.html    音频分析中用到的一些基本概念
>
>
>
>
>
>
>
>
>
>
>
>
>
>
