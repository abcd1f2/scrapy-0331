## RL MDP
- **概述：**
>       在RL中，agent与environment一直在互动。在每个时刻t，agent会接收到来自环境的状态s，基于这个状态s，agent会做出动作a，然后这个动作作用在环境上，于是agent可以接收到一个奖赏Rt+1，并且agent就会到达新的状态。
>
>       其实agent与environment之间的交互就是产生了一个序列：S0,A0,R1,S1,A1,R2,...，称这个为序列决策过程。而马尔科夫决策过程就是一个典型的序列决策过程的一种公式化。
>       Markov：
>           本来直观上讲，下一个状态的产生跟所有历史状态是有关的，也就是等式右边所示。但是Markov的定义则是忽略掉历史信息，只保留了当前状态的信息来预测下一个状态，这就叫Markov
>
>       最优状态动作值函数：
>           最优状态动作值函数q∗(s,a)由两部分组成，即：
>               1、是即时奖励
>               2、是所有能够到达状态s’的最优状态值函数按状态转移概率求和
>
>
>
>
>
>
>
>
>
>
>
>

- **待续：**
>       参考：
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
