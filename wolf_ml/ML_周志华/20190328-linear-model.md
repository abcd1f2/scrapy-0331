## 周志华-机器学习 - linear-model
- **概述：**
>       基于均方误差最小化来进行模型求解的方法称为最小二乘法。
>           在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧式距离之和最小
>       求解w和b使损失函数最小化的过程，就是线性回归模型的最小二乘"参数估计"
>
>

- **线性模型：**
>       对数线性模型：
>           对于标准的线性回归中，可否令模型预测值逼近y的衍生物（即y的一些非线性变换值）呢？
>           如果我们认为示例所对应的输出标记是在指数尺度上变化，那就可将输出标记的对数作为线性模型逼近的目标，
>           即lny = wx + b，这就是对数线性模型，实际上是在试图使e^(wx+b)逼近y，实质上是在求输入空间到输出空间的非线性函数映射
>           y = g^(-1) * (wx + b)，称为广义线性模型，g(x)表示单调可微函数，称为联系函数
>           对数线性模型是广义线性模型在g(x)=lnx时的特例
>
>       对数几率回归：
>           ln(y/(1-y)) = wx + b
>           实际上是用线性回归模型的预测结果去逼近真实标记的对数几率，因此称为对数几率回归
>           注：
>               因为它是直接对分类可能性进行建模，无需事先假设数据分布，这样就避免了假设分布不准确所带来的问题
>
>

- **线性判别分析LDA：**
>       LDA也常被视为一种经典的监督降维技术
>
>

- **多分类：**
>       最经典的常用三种拆分策略：
>           一对一（one vs one，简称OVO）
>               拆分成n(n-1)/2个分类结果，最终结果通过投票产生，即把预测的最多的类别作为最终分类结果
>           一对其余（one vs rest，简称OVR）
>               若仅有一个分类器预测为正类，则对应的类别标记为最终分类结果。
>               若有多个预测为正类，则通常考虑各个分类器的预测置信度，选择置信度最大的累呗标记作为分类结果
>               只需要n个分类器
>               一对一 VS 一对其余：
>                   一对其余使用全部训练样例，而一对一仅用到两个类训练集，在类别很多时，一对一训练开销通常比一对其余更小，预测性能取决于数据分布，在多数情况下两者差不多
>           多对多（many vs many 简称MVM）
>               每次将若干个类作为正类，若干个类作为反类
>               MVM的正反类必须有特殊的设计，不能随意选取。
>               常用方法：
>                   1、一种常用的MvM技术：纠错输出码（error correcting output codes），简称ECOC
>                   2、DAG
>               ECOC步骤：
>                   1、对N个分类进行M次划分，产生M个分类器
>                   2、M个分类器对测试样本进行预测，得到M个预测标记，将其组成编码，这个编码与N个类别各自的编码进行比较，返回其中距离（如欧氏距离）最小的类别作为预测结果
>

- **稀疏表示：**
>       稀疏性问题本质上对应了L0范数的优化，LASSO通过L1范数来近似L0范数，是求取稀疏解的重要技术
>
>
>
>
>
>
>
>
>
>
>
>

- **类别不平衡**
>       上采样：
>           向上提升，将少样本多次采样
>           上采样不能简单的进行重复采样，否则会招致严重的过拟合，
>           上采样的代表算法SMOTE是通过对训练集里的正例进行插值来产生额外的正例
>       下采样：
>           下采样若随机丢弃，则可能丢失一些重要信息，
>           下采样代表算法EasyEnsemble则是利用集成学习机制，将样例划分成若干个集合供不同学习器使用，这样在学习看来都进行了下采样，但是全局看来不会丢失重要信息
>
>
>
>
>
>
>
>
>
>
>

- **待续：**
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
