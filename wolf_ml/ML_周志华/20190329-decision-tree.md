## 周志华-机器学习 - decision-tree
- **概述：**
>
>
>
>
>
>
>
>

- **属性划分：**
>       ID3：信息增益
>           Gain(D,a)
>           信息增益对可取值数目较多的属性有所偏好
>       C4.5：信息增益率
>           Gain_ratio(D,a) = Gain(D,a)/IV(a)，其中IV(a)称为a的固有值，属性a的可能取值越多，IV(a)值通常越大
>           增益率准则对可取值数目较少的属性有所偏好，因此C4.5并不是直接选择增益率最大的候选划分属性，而是使用一个启发式方法，
>               先从划分属性中找到信息增益高于平均水平的属性，再从中选择增益率最高的 ！！！！
>       CART：基尼指数
>           Gini(D) 基尼值
>           Gini_index(D,a) 基尼指数
>           Gini(D)反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率。因此，Gini(D)越小，则数据集D的纯度越高。
>           选择划分后基尼指数最小的属性作为最优划分属性
>

- **过拟合：**
>       剪枝：
>           剪枝是decision-tree对付过拟合的主要手段。
>       剪枝策略：
>           1、预剪枝
>               在decision-tree生成过程中，对每个节点在划分前进行估计，若划分不能带来决策树泛化性能的提升，则停止划分，并将当前节点记为叶节点
>               剪枝规则：
>                   分别计算划分前和划分后的准确率，若准确率下降则停止划分
>               优点：
>                   减少了决策树的训练时间开销和测试时间开销，降低了过拟合风险
>               缺点：
>                   有些分支当前划分虽不能提高精度，但是在其基础上的后续划分可能导致性能的提高，预剪枝是基于"贪心"策略，所以带来了欠拟合的风险
>           2、后剪枝
>               先生成一颗完整的决策树，然后自底向上对非叶子节点进行考察，若将该节点对应的子树替换为叶节点能带来泛化性能的提升，则子树替换为叶节点
>               剪枝规则：
>                   自底向上方法对树中的所有非叶节点进行逐一考察，准确率有提高则进行剪枝
>               优点：
>                   后剪枝比预剪枝保留了更多的分支，一般情况下，后剪枝的欠拟合风险更小，泛化能力往往优于预剪枝
>               缺点：
>                   训练时间更长
>
>       如何判断泛化性能的提升？
>           通过对模型的性能评估，验证集进行性能评估
>
>

- **连续和缺失值：**
>       连续值处理：
>           1、二分法，简单粗暴
>           2、对于连续属性a，可考察包含n-1个元素的候选划分点集合，然后选取最优的划分点进行样本集合的划分
>               可根据信息增益选择最优划分点
>           **注意：与离散属性不同，若当前节点划分属性为连续属性，该属性还可作为其后代节点的划分属性。**！！！！！
>
>       缺失值处理：
>           缺失值处理，需要解决两个问题，
>           1、如果在属性值缺失的情况下进行划分属性选择？
>               使用含有该属性的子集的信息增益进行划分，但是划分时，需要带上子集的比例，以及含有该属性的比例，某一类的比例等
>           2、给定划分属性，若样本在该属性上的值缺失，如何进行样本划分？
>               样本在划分属性上缺失值，则同一样本以不同的概率（当前划分样本数占当前含属性值样本集合的比例）划入到不同的子节点中去（C4.5采用此方案）
>

- **多变量决策树：**
>       决策树所形成的分类边界有一个明显的特点：轴平行，即分类边界由若干个与坐标轴平行的分段组成，这样的分类边界使得学习结果有较好的可解释性
>       能实现斜划分，甚至更复杂划分的决策树，非叶子节点不再是针对某个属性，而是对属性的线性组合进行测试，即每个非叶子节点是一个线性分类器
>       主要算法有：OC1
>
>

- **待续：**
>       参考：[机器学习] 周志华
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
