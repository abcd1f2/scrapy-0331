## 周志华-机器学习 - svm
- **概述：**
>
>
>
>
>

- **svm**
>       对训练样本局部扰动的"容忍"性最好
>
>
>
>

- **SMO算法：**
>       SMO基本思想就是先固定ai之外的其他参数，然后求ai的极值。
>       SMO每次选择两个变量ai和aj，并固定其他参数，在参数初始化后，SMO不断执行如下两个步骤：
>           1、选取一对需要更新的变量ai和aj
>           2、固定ai和aj以外的参数，根据对偶形式，获取更新后的ai和aj
>           最后得到参数w
>       SMO算法之所以高效，是由于在固定其他参数后，仅优化两个参数的过程能做到非常高效。
>           SMO采用了一个启发式：使选取的两个变量锁对应样本之间的间隔最大。一种直观解释是，这样的两个变量很很大差别，
>           与对两个相似的变量进行更新相比，对他们进行更新会带来目标函数值更大的变化
>
>       如何确定b呢？
>           理论上，可选取任意支持向量并通过求解y*f(x)=1得到b，但是实际处理中，常采用一种更鲁棒的做法：
>               使用所有支持向量求解的平均值
>

- **核函数：**
>       如果原始空间是有限维，那么一定存在一个高维特征空间使样本可分
>       由于特征空间维数可能很高，甚至是无穷维，因此直接计算映射后的点内积是困难的。
>           为了避免计算映射后的内积，可以设想一个核函数，即xi和xj在特征空间的内积等于它们在原始样本空间中通过核函数计算的结果。
>           有了这样的函数，我们就不必直接去计算高维甚至无穷维空间的内积
>       核函数：
>           只要一个对称函数所对应的核矩阵半正定，它就是能做核函数。
>
>       常用核函数：
>           线性核
>           多项式核
>           高斯核
>           拉普拉斯核
>           Sigmoid核
>
>       构造核函数：
>           可以通过核函数进行组合
>           1、线性组合
>               如果k1和k2是核函数，则其线性组合 a1*k1+a2*k2 也是核函数
>           2、核函数直积
>               k1(x,z)*k2(x,z) 也是核函数
>           3、对于任意函数的组合
>               g(x)*k1(x,z)*g(z) 也是核函数
>
>
>       选择核函数经验：！！！
>           文本数据通常采用线性核
>           情况不明时先尝试高斯核
>

- **软间隔与正则化**
>       缓解过拟合，增加鲁棒性和泛化性，运行支持向量机在一些样本上出错，引入了软间隔
>       替代损失函数一般具有较好的数学性质，它们通常是凸的连续函数且是替代损失l的上界，常用的替代损失函数有：
>           hinge损失
>           对数损失
>           对率损失
>       软间隔和硬间隔下的对偶问题，唯一的差别就在对偶变量ai的约束不同，前者是0<=ai<=C，后者是0<=ai
>

- **支持向量回归：**
>       
>
>
>
>
>
>
>
>
>
>

- **待续：**
>       参考：[机器学习] 周志华
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
