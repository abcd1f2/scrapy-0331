## 周志华-机器学习 - svm
- **概述：**
>
>       定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机。
>
>       SVM属于一般化线性分类器，这类分类器的特点就是他们能够同时最小化经验误差（训练误差）与最大化几何边缘区
>
>       SVM的学习策略就是间隔最大化，可形式化为一个求解凸二次规划问题，也等价于正则化的合页损失函数的最小化问题。
>       SVM的学习算法是求解凸二次规划的最优化算法。
>
>       线性可分SVM，又称硬间隔SVM，当训练数据线性可分时
>       线性支持SVM，又称软间隔SVM，软间隔最大化，当训练数据近似线性可分
>       非线性SVM，使用kernel trick以及软间隔最大化，当训练数据线性不可分
>

- **对偶：**
>       应用拉格朗日的对偶性，通过求解对偶问题得到原始问题的最优解，这就是线性可分SVM的对偶算法。
>       引入对偶算法，有两个优点：
>           1、对偶问题往往更容易求解
>           2、自然引入核函数，推广到非线性分类问题
>

- **模型应用需要注意的点：**
>       特征标准化
>
>
>
>

- **svm**
>       对训练样本局部扰动的"容忍"性最好
>

- **SMO算法：**
>       SMO基本思想就是先固定ai之外的其他参数，然后求ai的极值。
>       SMO每次选择两个变量ai和aj，并固定其他参数，在参数初始化后，SMO不断执行如下两个步骤：
>           1、选取一对需要更新的变量ai和aj
>           2、固定ai和aj以外的参数，根据对偶形式，获取更新后的ai和aj
>           最后得到参数w
>       SMO算法之所以高效，是由于在固定其他参数后，仅优化两个参数的过程能做到非常高效。
>           SMO采用了一个启发式：使选取的两个变量锁对应样本之间的间隔最大。一种直观解释是，这样的两个变量很很大差别，
>           与对两个相似的变量进行更新相比，对他们进行更新会带来目标函数值更大的变化
>
>       如何确定b呢？
>           理论上，可选取任意支持向量并通过求解y*f(x)=1得到b，但是实际处理中，常采用一种更鲁棒的做法：
>               使用所有支持向量求解的平均值
>

- **核函数：**
>       如果原始空间是有限维，那么一定存在一个高维特征空间使样本可分
>       由于特征空间维数可能很高，甚至是无穷维，因此直接计算映射后的点内积是困难的。
>           为了避免计算映射后的内积，可以设想一个核函数，即xi和xj在特征空间的内积等于它们在原始样本空间中通过核函数计算的结果。
>           有了这样的函数，我们就不必直接去计算高维甚至无穷维空间的内积
>       核函数：
>           只要一个对称函数所对应的核矩阵半正定，它就是能做核函数。
>
>       常用核函数：
>           线性核
>           多项式核
>           高斯核
>           拉普拉斯核
>           Sigmoid核
>
>       构造核函数：
>           可以通过核函数进行组合
>           1、线性组合
>               如果k1和k2是核函数，则其线性组合 a1*k1+a2*k2 也是核函数
>           2、核函数直积
>               k1(x,z)*k2(x,z) 也是核函数
>           3、对于任意函数的组合
>               g(x)*k1(x,z)*g(z) 也是核函数
>
>
>       选择核函数经验：！！！
>           文本数据通常采用线性核
>           情况不明时先尝试高斯核
>

- **哪些模型需要做特征的归一化：**
>       概率模型(树形模型)不需要归一化：
>           因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、RF
>       模型特征需要归一化：
>           如果模型使用梯度下降法求最优解时，归一化往往是非常有必要的，否则很难收敛甚至不能收敛
>           Adaboost、SVM、LR、KNN、KMeans等最优化问题需要归一化
>

- **软间隔与正则化**
>       缓解过拟合，增加鲁棒性和泛化性，运行支持向量机在一些样本上出错，引入了软间隔
>       替代损失函数一般具有较好的数学性质，它们通常是凸的连续函数且是替代损失l的上界，常用的替代损失函数有：
>           hinge损失
>           对数损失
>           对率损失
>       软间隔和硬间隔下的对偶问题，唯一的差别就在对偶变量ai的约束不同，前者是0<=ai<=C，后者是0<=ai
>

- **支持向量回归：**
>
>
>
>
>
>
>
>
>
>
>

- **待续：**
>       参考：[机器学习] 周志华
>            [统计学习方法] 李航
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
