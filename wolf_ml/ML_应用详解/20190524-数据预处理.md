## ML_应用详解 - 数据预处理
- **概述**：
>
>
>
>
>
>
>
>

- **归一化和标准化的区别：**
>       归一化：
>           对不同特征维度的伸缩变换的目的是使各个特征维度对目标函数的影响权重是一致的，即使得那些扁平分布的数据伸缩变换成类圆形。
>               也改变了原始数据的一个分布
>           优点：
>               1、提高迭代求解的收敛速度
>               2、提高迭代求解的精度
>       标准化：
>           对不同特征维度的伸缩变换的目的是使得不同度量之间的特征具有可比性。同时不改变原始数据的分布
>               标准化就是一种对样本数据在不同维度上进行一个伸缩变化（而不改变数据的几何距离），也就是不改变原始数据的信息（分布）。
>               这样的好处就是在进行特征提取时，忽略掉不同特征之间的一个度量，而保留样本在各个维度上的信息（分布）
>           优点：
>               1、使得不同度量之间的特征具有可比性，对目标函数的影响体现在几何分布上，而不是数值上
>               2、不改变原始数据的分布
>

- **异常特征样本清洗：**
>       如果没有专业知识，如何筛选出这些异常特征样本呢？常用的方法有两种。
>           1、第一种是聚类
>               比如可以用KMeans聚类将训练样本分成若干个簇，如果某一个簇里的样本数很少，而且簇质心和其他所有的簇都很远，那么这个簇里面的样本极有可能是异常特征样本了。我们可以将其从训练集过滤掉。
>           2、第二种是异常点检测方法
>               主要是使用iForest或者one class SVM，使用异常点检测的机器学习算法来过滤所有的异常点。
>       注：
>           某些筛选出来的异常样本是否真的是不需要的异常特征样本，最好找懂业务的再确认一下，防止将正常的样本过滤掉了
>
>

- **数据不平衡梳理：**
>       一般是两种方法：权重法或者采样法
>       权重法：
>           权重法是比较简单的方法，我们可以对训练集里的每个类别加一个权重class weight。
>               sklearn中，绝大多数分类算法都有class weight和 sample weight可以使用
>           注：权重法做了以后发现预测效果还不好，可以考虑采样法
>       采样法：
>           上采样和下采样
>
>       缺点：
>           两种常用的采样法很简单，但是都有个问题，就是采样后改变了训练集的分布，可能导致泛化能力差。
>       改进：
>           有的算法就通过其他方法来避免这个问题，比如SMOTE算法通过人工合成的方法来生成少类别的样本
>           imbalance-learn这个Python库中的SMOTEENN类来做SMOTE采样
>

- **特征表达：**
>       缺失值处理：
>           如果是连续值，那么一般有两种选择：
>               1、选择所有有该特征值的样本，然后取平均值，来填充缺失值
>               2、取中位数来填充缺失值。
>           如果是离散值：
>               1、则一般会选择所有有该特征值的样本中最频繁出现的类别值，来填充缺失值
>               2、模型预测
>
>       特殊特征处理：
>           时间原始特征，举例几种有代表性的方法：
>               1、使用连续的时间差值法，即计算出所有样本的时间到某一个未来时间之间的数值差距，这样这个差距是UTC的时间差，从而将时间特征转化为连续值
>               2、根据时间所在的年，月，日，星期几，小时数，将一个时间特征转化为若干个离散特征，这种方法在分析具有明显时间趋势的问题比较好用。
>               3、权重法，即根据时间的新旧得到一个权重值。
>                   比如，对于商品，三个月前购买的设置一个较低的权重，最近三天购买的设置一个中等的权重，在三个月内但是三天前的设置一个较大的权重
>
>           地址特征：
>               比如“广州市天河区XX街道XX号”，这样的特征我们应该如何使用呢？处理成离散值和连续值都是可以的。如果是处理成离散值，则需要转化为多个离散特征，比如城市名特征，区县特征，街道特征等。但是如果我们需要判断用户分布区域，则一般处理成连续值会比较好，这时可以将地址处理成经度和纬度的连续特征。
>
>
>       离散特征的连续化处理：
>           1、最常见的离散特征连续化的处理方法是独热编码one-hot encoding
>           2、特征嵌入embedding。
>               这个一般用于深度学习中
>               也可以用word2vec将词转化为词向量
>
>       连续特征的离散化处理：
>           1、根据阈值进行分组
>           2、高级一些的方法
>               比如使用GBDT。在LR+GBDT的经典模型中，就是使用GDBT来先将连续值转化为离散值
>               那么如何转化呢？比如我们用训练集的所有连续值和标签输出来训练GBDT，最后得到的GBDT模型有两颗决策树，第一颗决策树有三个叶子节点，第二颗决策树有4个叶子节点。如果某一个样本在第一颗决策树会落在第二个叶子节点，在第二颗决策树落在第4颗叶子节点，那么它的编码就是0,1,0,0,0,0,1，一共七个离散特征，其中会有两个取值为1的位置，分别对应每颗决策树中样本落点的位置。
>
>

- **特征选择：**
>       1、找相关业务人员人工确定某些特征
>       2、数学方法选择特征
>           a、过滤法
>               方差、互信息等方法筛选特征
>           b、包装法
>               使用模型训练过滤掉无用的特征
>               先用所有特征训练模型，然后删掉其中一个特征，继续训练模型，看看模型的效果
>           c、嵌入法
>               使用L1/L2过滤特征，或者LR、GBDT模型判断选择特征
>       3、高级特征
>           寻找高级特征最常用的方法有：
>               若干项特征加和
>               若干项特征之差
>               若干项特征乘积
>               若干项特征除商
>           ((刘建平)个人经验是，聚类的时候高级特征尽量少一点，分类回归的时候高级特征适度的多一点)
>
>
>
>
>
>
>
>
>
>
>
>

- **待续：**
>       参考：https://www.cnblogs.com/pinard/p/9093890.html
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
