### TensorFlow学习笔记 -- 优化方法
- **概述**：
>       优化算法：基于学习效率的改变        
>       tf中的优化方法：
>           1、SGD:
>
>           2、Momentum:
>
>           3、Adagrad:
>
>           4、RMSProp:
>               Momentum(部分) + Adagrad
>           5、Adam:
>               Momentum + Adagrad
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>

- **待续：**
>
>
>
>
>
>
>
>
>
>
>
>
