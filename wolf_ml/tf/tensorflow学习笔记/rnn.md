### TensorFlow学习笔记 -- RNN
- **概述**：
>       RNN：
>           RNN主要用途是处理和预测序列数据。RNN的来源就是为了刻画一个序列当前的输出与之前信息的关系。
>           从网络结构上，RNN会记忆之前的信息，并利用之前的信息影响后面节点的输出。即RNN的隐藏层之间的节点是有连接的，隐藏层的输入不仅包括输入层的输出，还包括上一时刻隐藏层的输出。
>           RNN理论上可以被看做是统一神经网络结构被无限复制的结果。但是出于优化的考虑，目前RNN无法做到真正的无限循环。
>
>           和CNN网络中过滤器中参数是共享的类似，在RNN网络中，循环结构中的参数在不同时刻也是共享的。
>
>           RNN中的状态是通过一个向量来表示的， 这个向量的维度称为RNN隐藏层的大小，假设为h。假设输入向量的维度为x，那么网络输入的维度为h+x，
>           那么从输入到隐含层一共有(h+x)*h+h个参数，(h+x)*h为矩阵参数，h为隐层状态参数
>
>           RNN的损失函数的区别在于因为它每个时刻都有一个输出，所以循环神经网络的总损失为所有时刻（或者部分时刻）上的损失函数的总和。
>
>       LSTM：
>           为了使LSTM保存长期的记忆。遗忘门和输入门至关重要，他们是LSTM结构的核心。
>           遗忘门：作用是让循环神经网络忘记之前没有用的信息。遗忘门会根据当前的输入x(t)，上一时刻状态c(t-1)和上一时刻输出h(t-1)共同决定哪一部分记忆需要被遗忘。
>           输入门：当循环神经网络忘记了之前的部分状态后，需要从当前的输入补充最新的记忆。输入门会根据x(t)、c(t-1)和h(t-1)决定哪些部分将进入当前时刻的状态c(t)。
>           通过遗忘门和输入门，LSTM结构可以更加有效的决定哪些信息应该被遗忘，哪些信息应该被保留。
>           输出门：LSTM结构在计算得到新的状态c(t)后需要产生当前时刻的输出，输出门根据c(t)、h(t-1)和x(t)来决定该时刻的输出h(t)。
>
>
>       bidirectional-rnn 双向循环神经网络：
>           有些问题中，当前时刻的输出不仅和之前的状态有关系，也和之后的状态有关。这时就需要使用双向循环神经网络来解决问题。
>           双向循环神经网络的主体结构就是两个单向循环神经网络的结合。
>           在每一个时刻t，输入会同时提供给两个方向相反的循环神经网络，而输出则是由两个单向循环神经网络共同决定。
>
>       deep-RNN 深层循环神经网络：
>           为了增强模型的表达能力，可以将每一时刻上的循环体重复多次。
>           每一层中的循环体参数是一致的，不同层中的参数可以不同。TensorFlow中提供了MultiRNNCell来实现深层循环神经网络的前向传播过程。
>           rnn.MultiRNNCell([lstm]*num_layers)中num_layers表示有多少层，即从输入xt到输出ht需要经过多少个LSTM结构
>
>       RNN的dropout：
>           在CNN中使用了dropout的方法，通过dropout方法可以使CNN更加健壮。在RNN中使用dropout也有同样的功能。
>           一般CNN只在最后的全连接层中使用dropout，RNN一般只在不同层循环体结构之间使用dropout，而不在同一层循环体结构间使用。
>           即从时刻t-1传递到时刻t时，RNN不会进行状态的dropout；而在同一时刻t中，不同层RNN之间会使用dropout。
>
>       BiLSTM-CRF：
>
>
>
>
>
>
>
>
>
>
>
>
>
>

- **待续**
>
>
>
>
>
>
>
>
>
>
>
>
