### 七月在线NLP -- lda主题模型
- **概述：**：
>       **LDA是一种无监督的贝叶斯模型**
>       **是一种典型的词袋模型（可以使用ngram进行优化）**
>       lda的另一个有点：对于每一个主题均可以找出一些词语来描述
>
>
>       贝叶斯模型：
>           用概率作为可信度
>           每次看到新数据，则更新可信度
>           P(y|x) ∝ P(y) * P(x|y)
>           后验概率 ∝ 先验 * 似然

- **LDA：**
>        LDA：
>           **P(单词|文档) = P(单词|主题) * P(主题|文档)**
>
>           生成模型过程：
>               1、对每一篇文档，从主题分布中抽取一个主题
>               2、从上述被抽到的主题所对应的单词分布中抽取一个单词
>               3、重复遍历上过过程，直到遍历文档中的每一个单词
>
>               详解：
>                   1、对于文档集D的每个文档d，对应到不同Topic主题的概率θd<p1...pk>，其中pi表示d对应每个主题ti的概率，
>                       计算方法：pi=ni/n，ni表示d中对应第i个主题的词的数目，n的d中词的总数，即 P(主题|文档)
>                   2、没有每个主题t，生成不同单词的概率ψt<p1...pt>，其中，pi表示主题t生成第i个单词的概率，
>                       计算方法：pi=Ni/N，Ni表示对应主题中第i个单词的数目，N表示对应主题中所有单词的数目，即 P(单词|主题)
>
>               算法学习过程：
>                   标准的学习过程
>                   首先随机给θd和ψt赋值
>                   1、针对文档ds中的第i个单词wi，如果另wi对应的topic为tj，则Pj(wi|ds)=P(wi|tj)*P(tj|ds)
>                   2、枚举T中的topic，得到所有的pj(wi|ds)，然后根据概率结果为ds中的第i个单词选择一个topic，选择概率最大的pj
>                   3、如果ds中的第i个单词wi在这里选择了一个与原先不同的topic（i在遍历ds中所有单词，而tj理应不变），就会对θd和ψt有影响了（根据上面的详解中的统计结果进行更新）。
>                       它们的影响又会影响到p(w|d)的计算，对D中所有的d中的所有w进行一次p(w|d)的计算并重新选择topic看做一次迭代。这样进行n次循环迭代，直至收敛得到LDA。
>
>           LDA公式：
>               一个函数：gamma函数
>                   阶乘函数在实数上的推广
>               四个分布：二项分布、多项分布、beta分布、Dirichlet分布
>                   二项分布：
>                       伯努利分布，又称为两点分布，只有0-1值结果，是一个离散型的随机分布。
>                       二项分布是从伯努利分布推进的，二项分布即重复n次的伯努利实验
>                   多项式分布：
>                       二项分布扩展到多维的情况，有多种离散值可能(1,2,3...k)
>                   beta分布：
>                       二项分布的共轭先验分布，就是和二项分布一样的形式
>                   Dirichlet分布（狄利克雷）：
>                       是beta分布在高纬度上的推广
>               一个概念和一个理念：共轭先验和贝叶斯框架
>                   共轭分布：指先验概率和后验概率具有一样的函数形式
>                   共轭先验：保持先验和后验具有一样的函数形式的函数
>               两个模型：
>                   pLSA和LDA
>               一个采样：
>                   Gibbs采样
>
>           贝叶斯的思考方式：
>               先验分布π(θ)（可能是随机值） + 样本信息X -> 后验分布π(θ|x)
>
>           主题模型（循序渐进）：
>               unigram model：文档生成模型
>                   单词到文档的生成概率，所有词为W=(w1,w2,...wn)，用p(wi)表示词wi的先验概率，生成文档W的概率为：
>                   P = p(w1)p(w2)p(w3)...p(wn)  所有单词概率乘积
>               Mixture of unigrams model：
>                   给某个文档选择一个主题z，根据该主题生成文档，该文档中的所有词都来自于一个主题，假设主题z1,z2...zk
>                   P = P(z1)*P(w1|z1)*P(w2|z1)...P(wk|z1) + P(z2)*P(w1|z2)*P(w2|z2)...P(wk|z2) + ...
>               pLSA模型：
>                   **在Mixture of unigrams模型中，一篇文章只给出一个主题，但是实际中，一篇文章有多个主题，只是每个主题的概率不同**
>                   出现的几率不太一样
>                   文章 -> 主题   即θd分布
>                   主题 -> 单词   即ψt分布
>               原理：
>                   通过观测，得到了[知道主题是什么，就是什么单词]的生成模型，根据贝叶斯定理，可以反推[看见了什么单词，就知道什么主题]
>
>           LDA模型：
>               **LDA是在pLSA基础上加层贝叶斯框架，即LDA是pLSA的贝叶斯版本**
>               LDA和pLSA的区别：
>                   LDA中的随机选择过程服从狄利克雷分布
>                   LDA的随机过程的随机性
>                   1、LDA从狄利克雷分布选取主题
>                   2、LDA从狄利克雷分布选取文档中的词
>
>
>
>
>
>
>
>
>
>
>
>

- **待续：**
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
