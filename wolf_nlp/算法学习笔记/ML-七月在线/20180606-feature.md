### 七月在线ML -- feature
- **概述：**：
>       数据和特征决定模型的上限
>       更好的特征作用：
>           更强的灵活性
>           更好的效果
>           更简单的算法
>
>       数据特征的重要性：
>           具体业务结合
>
>       阿里天池：
>           各种交叉特征做到上万维的特征
>

- **数据与特征处理：**
>       数据清洗：
>           garbage in,garbage out
>           算法大多数时候就是一个加工机器，最后的产品取决于原材料的好坏
>           去掉脏数据
>       数据采用：
>           正负样本的不均衡问题
>               LR对正负样本比重比较敏感
>               解决方法：
>                   正样本 >> 负样本  数据量大 使用下采样方法（如原有5份数据，现在采集1份数据做处理）
>                   正样本 >> 负样本  数据量小
>                       oversampling（自己构造数据）
>                       采集更多数据
>                       修改损失函数（也叫代价敏感学习），如负样本偏少，则可在计算负样本的损失函数时，添加计算权重
>           数据采样
>           分层抽样
>               在每一类中抽取原来比例的数据
>
>       特征处理：
>           1、数值型（连续值）
>               幅度调整/归一化，LR等对数据敏感，比如房子面积，可以使用当前面积除以range(max-min)
>               log等变化，指数数据的转换
>               统计值max、min、mean、std
>               离散化(pandas库有相关处理)
>                   1、等步长（工程中使用）
>                   2、等频（样本量切分 工程中使用）
>               hash分桶
>               每个类别下对应的变量统计值histogram（分布状况 直方图映射）
>                   通过爱好比重分布 映射为样本的向量
>               数值型 =》 类别型
>                   one-hot编码或哑变量
>                       （比如衣服颜色红、黄、蓝三种颜色，不能直接编码成1、2、3，因为这样就认为的加上了一个大小关系的数据，需要平等的编码三个类别，如[1,0,0]、[0,1,0]、[0,0,1]）
>                       独热向量编码后，会增加特征的维度，变成一个稀疏的特征向量
>                       造成维度灾难（比如电商的用户id，如果直接用独热向量编码，维度直接爆掉，解决方案：比如有1000w用户，先聚类成1000类，然后对每一个用户，先查询具体在哪一类，所以最后仅对1000类进行独热向量编码）
>                   Hash与聚类处理
>                       比如先设置n个桶，每个桶代表一个类型，计算每个样本属于每个类型的概率或者频度等，这样就将特征规整为了一个n维度的特征向量
>                   统计每个类别变量下的各个target的比例，转成数值型
>           2、类别型
>           3、时间型
>               既可以看做连续值，也可以看做离散值
>               连续值：浏览网页时长等
>               离散值：一天中的时间点、星期几、工作日、周末等
>           4、文本型
>               ont-hot
>               tf-idf（反应词的重要性）
>               word2vec
>           5、统计型
>               加减平均
>               分位线
>               次序
>               比例型
>           6、组合特征型
>               多个特征组合成新的特征
>
>       特征的选择：
>           原因：
>               部分特征的相关度太高，消耗计算性能
>               部分特征对预测结果由负影响
>           特征选择 和 降维：
>               特征选择：剔除和预测结果关系不大的特征
>                   1、过滤型：
>                      评估单个特征和结果值之间的相关程度，留下top相关的特征
>                       缺点：没有考虑到特征之间的关联作用，可能把有用的关联特征剔除
>                   2、包裹型：
>                       把特征选择看做一个特征子集搜索问题，筛选各种特征子集，用模型评估效果
>                       RFE算法（递归的特征删除）
>                   3、嵌入型
>                       最常用的方式是用正则化来做特征选择
>                       L1和L2正则的区别：
>                           L1是截断型
>                               可能把某些不是很重要的特征的权重置为0（将特征不相关的权重置为0，减小了计算量）
>                           L2是缩放型
>                               缩放权重系数
>               降维：线性变换等
>
>

- **特征工程之特征选择：**
>       特征来源，常见有两种：
>           1、业务已经整理好各种特征数据，我们需要找出适合需要的特征
>           2、从业务特征中，去寻找高级数据特征
>
>       选择特征的常见步骤：
>           1、找领域内的专家，让他们提供一些建议，这些特征就是我们需要的特征候选集
>           2、候选集可能很大，我们需要找到候选集中比较重要的特征集合
>           3、最简单的是方差筛选。方差越大，认为是比较有用的，极端情况下，方差为0，即所有样本该特征的取值都一样，那么对于训练模型没有任何作用。(sklearn->VarianceThreshold)
>
>       特征选择方法总结：
>           1、过滤法
>               原理：
>                   该方法比较简单，按照特征的发散性或者相关性指标对各个特征进行评分，设定评分阈值或选择阈值的个数，来选择合适的特征。
>               具体方法：
>                   方差法
>                   相关系数
>                   假设检验，比如卡方检验、F检验、t检验
>                   互信息（从信息熵的角度分析各个特征和输出值之间的关系评分）
>               以上就是过滤法的主要方法，个人经验是，在没有什么思路的 时候，可以优先使用卡方检验和互信息来做特征选择
>           2、包装法
>               原理：
>                   根据目标函数，通常是预测效果评分，每次选择部分特征，或者排除部分特征
>               具体方法：
>                   递归消除特征法（使用一个机器学习模型进行多伦训练，每轮训练后，消除若干权值系数的对应的特征，在基于新的特征集进行下一轮训练。sklearn->RFE）
>           3、嵌入法
>               原理：
>                   此方法稍微复杂一些，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据权值系数从大到小来选择特征，
>                   类似于过滤法，但是是通过模型来确定特征的优劣
>                   和RFE的区别是它不是通过不停的晒掉来进行训练，而是使用功能特征全集。（sklearn->SelectFromModel）
>               具体方法：
>                   最常用的是使用L1和L2正则来选择特征
>                       正则化惩罚项越大，那么模型的系数就会越小。当正则化惩罚项大到一定的程度的时候，部分特征系数会变成0，当正则化惩罚项继续增大到一定程度时，所有的特征系数都会趋于0.
>                       但是我们会发现一部分特征系数会更容易先变成0，这部分系数就是可以筛掉的。也就是说，我们选择特征系数较大的特征。常用的L1正则化和L2正则化来选择特征的基学习器是逻辑回归。
>                   决策树或者GBDT
>       寻找高级特征：
>           寻找高级特征是模型优化的必要步骤之一，得到基准模型后，再寻找高级特征进行优化。
>           常用高级特征：
>               1、若干项特征加和
>               2、若干项特征之差
>               3、若干项特征之乘积
>               4、若干项特征除商
>           个人经验是，它需要你根据你的业务和模型需要而得，而不是随便的两两组合形成高级特征，这样容易导致特征爆炸，反而没有办法得到较好的模型。聚类的时候高级特征尽量少一点，分类回归的时候高级特征适度的多一点。
>
>
>

- **特征工程之特征表达:**
>       主要包括以下几个方面：
>           1、缺失值处理
>           2、特殊的特征处理
>           3、离散特征的连续化和离散化，连续特征的离散化处理
>
>       缺失值处理：
>           分两种情况，离散值和连续值（sklearn->preprocessing.Imputer）
>           连续值，一般有两种选择：
>               a、所有样本该特征的平均值填充
>               b、中位数填充
>           离散值：
>               一般会选择所有该特征值的样本中最频繁出现的类别值进行填充
>
>       特殊的特征处理：
>           比如日期时间20180808
>           常见的处理方法：
>               1、连续的时间差值法
>                   计算出所有样本的时间到某一个未来时间之间的数值差距，这样这个差距是UTC的时间差，从而将时间特征转化为连续值
>               2、根据时间的年、月、日、星期几、小时数等，转换为若干离散特征
>                   这种方法在分析具有明显时间趋势的问题比较好用
>               3、权重法
>                   根据时间的新旧得到一个权重值，比如对于商品，三个月前购买的设置一个比较低的权重，最近三天购买的设置一个中等权重，三个月内但是三天前的设置一个比较大的权重
>                   可以根据具体问题具体分析
>               4、对地理特征
>                   比如"广州市天河区XX街道XX号"，处理成连续和离散值都是可以的，但是如果需要判断用户分布区域，则一般处理成连续值比较好
>                   处理成离散值：
>                       比如城市名特征，区县特征，街道特征等
>                   处理成连续值：
>                       可以将地址处理成经度和纬度的连续特征
>
>       离散特征的连续化处理：
>           1、最常见的离散特征连续化方法：one-hot encoding，
>               直接转化成one-hot编码即可
>           2、特征嵌入embedding
>               一般用于深度学习中
>               如在tensorflow中，我们可以先随机初始化一个特征嵌入矩阵，对于每个id，可以用tf.nn.embedding_lookup找到id的特征嵌入向量。特征嵌入矩阵会在反向传播的迭代中优化。
>
>       离散特征的离散化处理：
>           1、one-hot编码
>           2、虚拟编码dummy coding，使用较少，one-hot使用更广泛
>
>       连续特征的离散化处理：
>           1、常用的方法是根据阈值进行分组，变成离散值
>               还有高级一些的，比如使用GBDT，在LR+GBDT的经典模型中，就是使用GBDT将连续值转为离散值。
>               如我们用训练集的所有连续值和标签输出来训练GBDT，最后得到的GBDT模型有两颗决策树，第一颗决策树有三个叶子节点，第二颗决策树有4个叶子节点。如果某一个样本在第一颗决策树会落在第二个叶子节点，
>                   在第二颗决策树落在第4颗叶子节点，那么它的编码就是0,1,0,0,0,0,1，一共七个离散特征，其中会有两个取值为1的位置，分别对应每颗决策树中样本落点的位置。在sklearn中，
>                   我们可以用GradientBoostingClassifier的 apply方法很方便的得到样本离散化后的特征，然后使用独热编码即可。
>
>

- **共线性问题：**
>       因为需要同时考虑多个预测因子，所有多重共线性是不可避免的，很多情况下，多重共线性是一个普遍现象。
>       在构造预测模型时，对于多重共线性问题，既不能不加控制，也不能一刀切。
>
>       共线性问题，有如下几种检验方法：
>           相关性分析
>           方差膨胀因子VIF
>           条件数检验
>
>       常用方法规避共线性问题：
>           1、PCA等降维法
>               主成分分析法作为多远统计分析的一种常用方法，在处理多变量问题时，具有一定的优越性，其降维的优势是明显的，
>                   主成分回归方法对于一般的多重共线性问题还是适用的，尤其是对共线性较强的变量之间。
>               当采取主成分提取了新的变量后，往往这些变量间的组内差异小而组间差异大，起到了消除共线性的问题。
>           2、逐步回归法
>               逐步回归是一种常用的消除多重共线性、选取"最优"的回归方程的方法。
>               其做法是将逐个引入自变量，引入的条件是该自变量经F检验是显著的，每引入一个自变量后，对已选入的变量进行逐个检验，
>                   如果原来引入的变量由于后面变量的引入而变得不再显著，那么就将其剔除。
>           3、岭回归、L2正则化（ridge regression）
>               岭回归是一种可用于共线性数据分析的有偏估计回归方法，它是一种改良的最小二乘估计法，通过放弃最小二乘法的无偏性，
>                   以损失部分信息、降低精度为代价获得回归系数更为符合实际、更可靠的回归方法，对条件数很大（病态数据）的拟合要强于最小二乘法。
>           4、LASSO回归、L1正则化
>               L1范数没有L2范数那么圆润，毕竟存在不可导点，而且在L1范数下LASSO回归也给不出解析解啦，但是相对于岭回归，LASSO估计的参数能更容易收敛到0
>
>

- **待续：**
>       参考：https://www.jianshu.com/p/ef1b27b8aee0   讲讲共线性问题
>           https://www.cnblogs.com/pinard/p/9032759.html   特征工程之特征选择
>           https://www.zhihu.com/question/29316149/answer/110159647    特征工程到底是什么？
>           https://www.zhihu.com/question/28641663/answer/110165221    机器学习中，有哪些特征选择的工程方法？
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
