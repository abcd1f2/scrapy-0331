### 七月在线机器学习 -- regression
- **概述：**：
>       降维属于无监督学习
>       loss function：
>           平方损失
>           log损失
>           hirge 损失
>           指数损失
>           交叉熵损失
>       线性回归：
>           loss function：平方损失
>           做分类抵抗噪声能力差
>
>       梯度下降：
>           优化算法
>
>       学习率：
>           也叫超参数（影响是否收敛的重要条件，如果不收敛，需要查看是否学习率设置合适）
>           设置很大时，会在极小值附近震荡
>
>       欠拟合或过拟合：
>           k折交叉验证
>
>       正则化：
>           1、控制参数幅度，不让模型失控
>           2、限制参数搜索空间
>           L1正则化：|θ|
>           L2正则化：θ^2（可导）
>           λ∑θ^2 正则化项
>           λ：超参数，正则化程度
>           添加正则化的损失函数，不让参数θ的平方过大，限制参数的搜索空间
>
>

- **损失函数：**
>       损失函数要为非凸函数，这样就能找到全局最优解，否则可能只能找到局部最优解，达不到全局最优
>       cost function 和 loss function
>       损失函数和代价函数
>
>


- **逻辑回归：**
>       sigmoid函数 压缩函数，具有一个先验概率分布：伯努利分布！！
>       判定边界
>       损失函数：
>           用平方损失的话，是一个非凸函数
>           log损失函数（凸函数）
>           正则化项（L2正则化） 对复杂的逻辑回归中使用，限制θ的范围
>
>           通过极大似然估计，得到似然估计函数，然后通过梯度下降法可以求得最优模型参数。
>       优化方法：
>           梯度下降法
>           其他优化算法：
>               拟牛顿法
>               BFGS
>               L-BFGS
>               优点:
>                   与梯度下降法相比，拟牛顿法的不需要选择步长，通常比梯度下降法更快
>               缺点：
>                   更复杂
>
>
>       逻辑回归是一个分类模型，logistics方法主要应用于研究某些事件发生的概率。
>
>       模型优点：
>           1、速度快，适合二分类
>           2、易于理解，直接看到各个特征的权重
>           3、能容易更新模型吸收新的数据
>       模型缺点：
>           1、对数据和场景的适应能力有局限性，不如决策树适应性那么强
>
>       逻辑回归和多重现性回归区别：
>           逻辑回归和多重线性回归最大的区别在于因变量不同，其他的都差不多。
>           所以他们都归于一个家族，即广义线性模型。
>           - 如果是连续的，就是多重线性回归
>           - 如果是二项分布，就是logistics回归
>           - 如果是Poisson分布，就是Poisson回归
>           - 如果是负二项分布，就是负二项回归
>
>       逻辑回归应用：
>           1、寻找危险因素：如寻找某一疾病的危险因素
>           2、预测：如根据模型，预测在不同的自变量的情况下，发生某病或某种情况的概率是多大
>           3、判别：类似于预测，根据模型判断某人属于某种情况的概率大小
>
>       回归常规步骤：
>           1、寻找h预测函数
>           2、构造h损失函数
>           3、最大似然估计求得参数θ
>
>       过拟合问题：
>           过拟合主要原因：
>               过拟合产生的原因主要源自过多的特征
>           解决方法：
>               1、减少特征（减少特征会丢失一些信息，不可取）
>                   人工选择
>                   模型算法选择
>               2、正则化（特征较多时有效）
>                   保留所有的特征，但减少θ的大小
>       正则化方法：
>           正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或惩罚项。
>           正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化项越大。
>
>           正则项可以取不同的形式，在回归问题中取平方损失，就是参数的L2范数，也可以取L1范数，
>
>           正则项系数：
>               λ是正则项系数，
>               如果λ很大，说明对模型的复杂度惩罚大，对拟合数据的损失惩罚小，这样就不会过拟合数据，
>                   导致在训练数据上的偏差较大，在未知数据上的方差小，但是可能出现欠拟合现象
>               如果λ很小，说明比较注重对训练数据的拟合，在训练数据上的偏差会小，但是可能会导致过拟合
>
>
>

- **待续：**
>       参考：https://blog.csdn.net/chibangyuxun/article/details/53148005  机器学习算法--逻辑回归原理介绍
>           https://blog.csdn.net/pakko/article/details/37878837    逻辑回归 - 理论篇
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
