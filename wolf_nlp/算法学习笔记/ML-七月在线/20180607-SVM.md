### 七月在线机器学习 -- SVM
- **概述：**
>       NIPS机器学习的顶级会议
>       SVM：时间复杂度O(N^3)
>       适用中小数据集
>       在大数据情况下，跑不完数据
>
>

- **SVM:**
>       idea：
>           1、**最大间隔**
>           2、**决策公式**
>               通过最大间隔构造函数
>               yi(W*xi + b) - 1 = 0 在边界上的样本，决策边界
>
>           3、**目标函数**
>               通过公式简化成求 2/||W|| 最大值
>
>           4、**优化理论**
>               拉格朗日乘子法，分别对w和b求导，得到KKT的条件
>
>           5、**核方法**
>               两个点的内积转化为核方法，求核函数的结果
>
>           6、**SMO算法优化求解α**
>               固定两个变量(αi,αj)，然后求导求极值，然后迭代求解最优值
>
>           7、**Hinge loss**
>               损失函数，增加L正则化（L正则化还可以应用到特征选择，根据每个特征的权值计算每个特征项的重要程度）
>

- **核方法：**
>       kernel mathod：
>           线性方法 -> 核方法
>           拉格朗日乘子法：
>               求最小值： f1 = 1/2|W|^2
>               约束条件： f2 = yi(W*X + b) - 1，当f2=0的约束
>               W：支持向量的线性组合
>               将一个带条件的函数极值算法变换成一个不带条件的函数极值算法
>               L = f1 - ∑αi * f2
>                 = ∑αi - 1/2∑∑αi * αj * yi * yj * xi * xj
>               总结：L取决与xi * xj
>               核方法作用：
>                   kernel(xi,xj) = φ(xi) * φ(xj)
>                   比如有一个核函数为f(x1,x2) = (x1*x2+1)^n
>                   则L的最后优化中的xi * xj则变成优化(x1*x2+1)^n
>                   对于L的优化中，并不需要知道如何变换，只需要知道变换后，两个x变量的乘积
>                   是优化目标和决策函数具有非线性变换的能力
>                   核方法作用是计算出经过变换后的空间的xi和xj的乘积
>               优化：
>                   优化的目标就是计算最优的αi参数
>                   SMO算法：
>                       固定(αi,αj)，迭代计算最后收敛
>                       二元函数求极值的问题
>
>
>
>
>

- **待续：**
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
