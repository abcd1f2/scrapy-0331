### 七月在线机器学习 -- 决策树
- **概述：**
>       最大熵
>       信息增益：
>           用上一次的信息熵 减去 用某一个分割条件划分后的信息熵 的差值
>           差值越大，说明划分的越有效果，**消除的不确定性越大，越趋向于稳定**
>           Gain(S,A) = H(S) - ∑(Sv|S)*H(Sv)
>       ID3：
>           算法步骤：
>               1、先计算所有属性的信息增益
>               2、选择信息增益最大的属性作为测试属性，进行划分
>               3、如划分后为单属性，则为叶子节点，否则继续递归调用
>           过拟合：
>               **容易过拟合，比如选用日期作为划分属性，则非常纯洁，但是没有实际意义**
>           优化：
>               1、没有必要的分裂不要了
>                   significant test
>               2、剪枝
>                   用验证集合去验证剪枝效果，然后确定进行剪枝
>
>           信息增益的缺陷：
>               当用日期作为分裂属性时，信息增益可以达到最大，但是日期分裂没有实际意义
>
>           信息增益比：
>               优化信息增益的缺陷
>
>
>

- **其他常见算法：**
>
>       1、ID3
>           **信息增益作为属性的选择标准**
>           缺点：
>               **信息增益倾向于取值较多的属性进行分割，存在一定的偏好**
>           g(D,A) = H(D) = H(D|A)  表示分割前和分割后的信息熵的差值，就是信息增益，越大越好
>
>       2、C4.5
>           **信息增益率来选择节点属性**
>           可以克服ID3算法存在的不足：
>               1、ID3只适用于离散的描述属性
>               2、C4.5算法能够处理离散值，也可以处理连续值（对连续属性值需要扫描排序，会使C4.5性能下降）
>           公式：
>               GainRation(D,a) = Gain(D,a)/IV(a)       IV(a)即属性a本身的熵，是由属性A的特征值个数决定的，个数越多，IV值越大，信息增益率越小
>
>           **如果仅仅是用信息增益率来计算的话，模型又会偏向特征数少的特征。因此C4.5先从候选属性中找出信息增益高于平均水平的属性，在从中选择增益率最高的属性进行分裂。！！！**
>
>
>       3、CART算法
>           当节点的连续变量时，改树为回归树；当节点是分类变量时，改树为分类树
>           CART算法应用要多一些，既可以用于分类也可以用于回归。CART分类使用基尼指数来选择最好的数据分割的特征。
>
>           Gini(D)反映了数据集D的纯度，值越小，纯度越高，特征越好，这和信息增益是相反的。在候选集合中选择使得划分后基尼指数最小的属性作为最优划分属性。
>           基尼系数和熵之半的曲线非常接近，仅仅在45度角附近误差稍大。因此，**基尼系数可以作为熵模型的一个近似替代。** 如下图，
> ![avatar](https://github.com/nwaiting/wolf-ai/blob/master/wolf_others/pic/decision_tree_gini_vs_entropy.png)
>
>           CART分类树算法是使用的基尼系数来选择决策树的特征，同时，为了进一步简化，CART分类树算法每次仅仅对某个特征的值进行二分类，而不是多分，这样CART分类树就是二叉树，这样有两个优点：
>               a、可以进一步简化基尼系数的计算
>               b、可以建立一个更加优雅的二叉树模型
>
>           **注意：！！！**
>               与ID3和C4.5处理离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。
>
>           回归原理：
>               回归树总体流程也是类似，不过在每个节点（不一定是叶子节点）都会得一个预测值。
>               如年龄为例，该预测值属于这个节点的所有人年龄的平均值
>                   分支时，穷举每一个feature的每个阈值找最好的分割点，但衡量的标准不再是最大熵，而是最小化均方差（每个人的年龄-预测年）^2的总和/N，或者说每个人的预测误差平方和除以N。
>                   为什么是误差平方和？物理解释一下，被预测出错的人数越多，错的越离谱，均方差就越大，通过最小化均方差能够找到最靠谱的分支依据。
>               分支直到每个叶子节点上人的年龄都唯一（太困难）或者达到预设的终止条件（如叶子个数上限），若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄作为该叶子节点的预测年龄。
>
>
>
>       决策树的优点和缺点：
>           优点：
>               1、非黑盒，解释性强
>               2、**轻松去除无关的属性，即根据某个属性划分后的信息增益为0即为无关属性，清理数据**
>               3、效率高
>           缺点：
>               1、只能进行线性分割
>               2、贪婪算法，每一层的分裂属性的选择，可能找不到最好的树，而且做特征选择都是选择最优的一个特征来做分类决策，
>                   但是大多数，分类决策不应该是由某一特征决定的，而是应该由一组特征决定，这样决策得到的决策树更加准确，这种决策树叫多变量决策树（multi-variate decision tree），代表算法OC1
>               3、如果样本发送一点点改动，就会导致树结构的剧烈改变
>
>

- **决策树在不同的应用场景：**
>
>       连续值：
>           离散化
>       多分类：
>           计算熵的时候，需要计算所有属性的熵值
>       回归问题：
>           对最后预测的值进行LR回归处理或者或者对预测的值取平均值
>

- **进化的决策树：ensemble集成**
>
>       三大类ensemble方式：
>           1、bagging：
>               投票形式
>           2、Boosting
>       注：森林中的每棵树是通过ID3、ID4.5等方法构建的，然后通过森林的构建方法，构建一个森林
>
>       bootstrap：（自助法）
>           **bootstrap是一种抽样方法，通过方差的估计可以构造置信区间等，其运用范围得到进一步延伸。在小样本的时候，bootstrap效果比较好。**
>           选取东西的方式：
>               从N个集合中选取M个子集的方式
>           **bagging和boosting都是booststrap思想的一种应用。！！！**
>           **bagging和boosting都是集成学习（ensemble learning）领域的基本算法**
>           思想：
>               就是一个在自身样本重采样的方法来估计真实分布的情况
>               当不知道样本分布的时候，bootstrap方法最有用
>               整合多个弱分类器，称为一个强大的分类器，这时候，集合分类器（Boosting、Bagging等）出现了。
>
>       1、bagging：（bootstrap aggregation）
>           为什么叫bootstrap aggregation，因为它抽取样本的时候采用的就是bootstrap方法。
>           思想：
>               **每一次从原始数据中根据均匀概率分布有放回的抽取和原始数据大小相同的样本集合，样本点可能出现重复。然后对每一次产生的训练集构造一个分类器，再对分类器进行组合。**
>           a、有限属性，随机选取部分属性
>           构建多棵树，对样本的属性进行部分选择(每棵树仅含有部分样本进行训练，即每个样本被分配到多棵树中的部分树种进行训练??)
>           最后通过对多棵树的结果进行投票，得到最终结果
>           从m个数据属性中选出n个数据属性
>           **bagging的代表算法：RF(random forest)**
>
>       2、Boosting：
>           思想：对每一棵树，仅对前面树的分类结果继续进行分类，确定前面树分类结果的权重，得到最后的结果，最后的结果由加权投票决定
>           如下图，
> ![avatar](https://github.com/nwaiting/wolf-ai/blob/master/wolf_others/pic/decision_tree_boosting.jpg)
>           **思想：！！！**
>               每一次抽样的样本分布都是不一样的。每一次迭代都根据上一次迭代的结果，增加被错误分类的样本的权重，使得模型能在之后的迭代中更加注意到难以分类的样本，
>               这是一个不断学习的过程，也是一个不断提升的过程，这也就是boosting思想本质所在。
>               在迭代之后，将每次迭代的基分类器进行集成。那么如何进行样本权重的调整和分类器的集成是我们需要考虑的关键问题。！！！
>

- **集成学习：**
>
>       思想：
>           先产生一组个体学习器，然后再用一组策略将它们结合起来
>
>       个体学习器：
>           决策树、神经网络
>
>       个体学习器：
>           基分类器可以是同一类算法，也可以是不同类算法，如：可以全是决策树、也可以全是神经网络、也可以来自不同算法
>
>       什么时候集成效果好于单个学习器？
>           并不是任意选择学习器，任意组合都会获得更好的效果，最好的情况是，**每个学习器都不是特别差，并且要具有一定的多样性**，否则可能会起到负作用。
>
>       "错误率相互独立"的假设，这是集成学习的一个核心问题：
>       如何生成准确又不是很差，并且还能保证多样性的个体学习器呢？？
>           目前主要有两种生成方式：
>           Boosting：
>               个体学习器间存在强相互依赖，必须串行生成
>               思想：（和上面的思想层次不一样）
>                   **给定初始训练数据，由此训练第一个基学习器，基学习器通过训练对样本权重进行调整更新，对调整后的样本，训练下一个基学习器，重复上述过程T次，将T个学习器加权结合。**
>           Bagging：
>               个体之间不存在强依赖关系，可并行生成
>
>

- **具体算法：**
>
>       具体算法：
>           random forest（bagging算法类）
>               a、有限样本，随机有放回的抽取样本
>               b、有限属性，随机选取部分属性
>               特点：
>                   通过计算特征蕴含的信息量，特征中选取一个最具有分类能力的特征进行分裂
>                   **每棵树最大限度生长，不做任何剪枝**
>                   最后通过对多棵树的结果进行投票
>               **不是每棵树选相同的子特征，也不是每棵树随机选不同的子特征，而是每个节点都会随机选择一些特征。！！！！！**
>               **特征个数选择：选择子特征的个数的经验值一般是原来特征个数的平方根(分类问题)或原特征个数的三分之一(回归问题)**
>
>           AdaBoost：（Boosting算法类中的一个具体算法，boosting的代表算法，相对于大多数学习算法而言，不会很容易出现过拟合现象）
>               **基于样本权重和分类器权重学习**
>               算法步骤：
>                   1、初始化训练数据的权值分布，每一个训练样本最开始都被赋予相同的权值：1/N
>                   2、多伦迭代：
>                       a、计算分类误差率em
>                       b、计算树的分类权重am（根据em求出），更新树的分类权重
>                           即分类误差率越小的基本分类器在最终分类器中的作用越大
>                       c、更新训练数据集的权值分布，Re-weight Sample
>                           **每一个样本进行更新权重后，重新进行学习**
>                           **分类误差越小的基本分类器在最终分类器中的作用越大**
>                           这样，AdaBoost方法能聚焦于较难分的样本上
>                           对于下一棵树：1、有样本权重，还有哪些未被分类好
>                                        2、树的权重，树被信任的权重大小
>                   3、计算在最终分类器中的重要程度
>                           得到最终的分类器，各个树的权重加和
>
>           GBDT：（**Adaboost的Regression版本**）
>               **基于残差学习**
>               **工业效果比较好**
>               和AdaBoost的区别：
>                   1、分类误差方程：
>                       AdaBoost使用0/1分类准确率计算误差
>                       GBDT使用err方程进行计算
>                   2、**残差作为下一轮学习目标**
>                       比如：预测一个人年龄为18岁，第一棵树预测为12岁，残差为6岁，
>                             第二棵树把年龄设为6岁去学习，如果第二颗树真的学习到为6岁，则相加为最终年龄，
>                             如果第二颗树预测为5岁，则残差为1岁，第三棵树设置年龄为1岁，继续学习，
>                       最后的结果为加权和，不是简单的多数投票
>
>           XGBoost：(**本质还是个GBDT**)
>               **工业界效果很好**
>               华人团队开发的
>               **本质还是个GBDT，把效率和速度做到极致，所以叫X(extreme)GBoosted**
>               与GBDT区别：
>                   1、使用L1、L2防止过拟合
>                       误差函数中添加L1和L2，降低过拟合概率
>                   2、对代价函数一阶和二阶求导，更快的收敛Converge
>                       速度更快
>                   3、树长全后从底部向上剪枝，防止贪婪算法
>                       防止overfiting
>
>

- **待续：**
>       参考：https://zhuanlan.zhihu.com/p/34534004
>           https://zhuanlan.zhihu.com/p/29408863
>           http://www.cnblogs.com/pinard/p/6053344.html
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
