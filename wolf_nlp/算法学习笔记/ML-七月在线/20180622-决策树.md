### 七月在线机器学习 -- 决策树
- **概述：**
>       最大熵
>       信息增益：
>           用上一次的信息熵 减去 用某一个分割条件划分后的信息熵 的差值
>           差值越大，说明划分的越有效果，**消除的不确定性越大，越趋向于稳定**
>           Gain(S,A) = H(S) - ∑(Sv|S)*H(Sv)
>       ID3：
>           算法步骤：
>               1、先计算所有属性的信息增益
>               2、选择信息增益最大的属性作为测试属性，进行划分
>               3、如划分后为单属性，则为叶子节点，否则继续递归调用
>           过拟合：
>               **容易过拟合，比如选用日期作为划分属性，则非常纯洁，但是没有实际意义**
>           优化：
>               1、没有必要的分裂不要了
>                   significant test
>               2、剪枝
>                   用验证集合去验证剪枝效果，然后确定进行剪枝
>
>           信息增益的缺陷：
>               当用日期作为分裂属性时，信息增益可以达到最大，但是日期分裂没有实际意义
>
>           信息增益比：
>               优化信息增益的缺陷
>
>
>

- **其他常见算法：**
>       1、ID3
>           **信息增益作为属性的选择标准**
>       2、C4.5
>           **信息增益率来选择节点属性**
>           可以克服ID3算法存在的不足：
>               1、ID3只适用于离散的描述属性
>               2、C4.5算法能够处理离散值，也可以处理连续值
>       3、CART算法
>           基尼系数
>           当节点的连续变量时，改树为回归树；当节点是分类变量时，改树为分类树
>
>       决策树的优点和缺点：
>           优点：
>               1、非黑盒，解释性强
>               **2、轻松去除无关的属性，即根据某个属性划分后的信息增益为0即为无关属性，清理数据**
>               3、效率高
>           缺点：
>               1、只能进行线性分割
>               **2、贪婪算法，每一层的分裂属性的选择，可能找不到最好的树**
>

- **决策树在不同的应用场景：**
>       连续值：
>           离散化
>       多分类：
>           计算熵的时候，需要计算所有属性的熵值
>       回归问题：
>           对最后预测的值进行LR回归处理或者或者对预测的值取平均值
>

- **进化的决策树：ensemble集成**
>       三大类ensemble方式：
>           1、bagging：
>               投票形式
>           2、Random Forest
>           3、Boosting
>       注：森林中的每棵树是通过ID3、ID4.5等方法构建的，然后通过森林的构建方法，构建一个森林
>
>       bootstrap：
>           选取东西的方式
>           从N个集合中选取M个子集的方式
>       1、bagging：
>           a、有限属性，随机选取部分属性
>           构建多棵树，对样本的属性进行部分选择(每棵树仅含有部分样本进行训练，即每个样本被分配到多棵树中的部分树种进行训练??)
>           最后通过对多棵树的结果进行投票，得到最终结果
>           从m个数据属性中选出n个数据属性
>       2、random forest
>           a、有限样本，随机有放回的抽取样本
>           b、有限属性，随机选取部分属性
>           特点：
>               通过计算特征蕴含的信息量，特征中选取一个最具有分类能力的特征进行分裂
>               **每棵树最大限度生长，不做任何剪枝**
>               最后通过对多棵树的结果进行投票
>       3、Boosting：
>           思想：对每一棵树，仅对前面树的分类结果继续进行分类，确定前面树分类结果的权重，得到最后的结果
>               最后的结果由加权投票决定
>
>
>       具体算法：
>           AdaBoost：（Boosting算法类中的一个具体算法）
>               **基于样本权重和分类器权重学习**
>               算法步骤：
>                   1、初始化训练数据的权值分布，每一个训练样本最开始都被赋予相同的权值：1/N
>                   2、多伦迭代：
>                       a、计算分类误差率em
>                       b、计算树的分类权重am（根据em求出），更新树的分类权重
>                           即分类误差率越小的基本分类器在最终分类器中的作用越大
>                       c、更新训练数据集的权值分布，Re-weight Sample
>                           **每一个样本进行更新权重后，重新进行学习**
>                           **分类误差越小的基本分类器在最终分类器中的作用越大**
>                           这样，AdaBoost方法能聚焦于较难分的样本上
>                           对于下一棵树：1、有样本权重，还有哪些未被分类好
>                                        2、树的权重，树被信任的权重大小
>                   3、计算在最终分类器中的重要程度
>                           得到最终的分类器，各个树的权重加和
>
>           GBDT：（**Adaboost的Regression版本**）
>               **基于残差学习**
>               **工业效果比较好**
>               和AdaBoost的区别：
>                   1、分类误差方程：
>                       AdaBoost使用0/1分类准确率计算误差
>                       GBDT使用err方程进行计算
>                   2、**残差作为下一轮学习目标**
>                       比如：预测一个人年龄为18岁，第一棵树预测为12岁，残差为6岁，
>                             第二棵树把年龄设为6岁去学习，如果第二颗树真的学习到为6岁，则相加为最终年龄，
>                             如果第二颗树预测为5岁，则残差为1岁，第三棵树设置年龄为1岁，继续学习，
>                       最后的结果为加权和，不是简单的多数投票
>
>           XGBoost：(**本质还是个GBDT**)
>               **工业界效果很好**
>               华人团队开发的
>               **本质还是个GBDT，把效率和速度做到极致，所以叫X(extreme)GBoosted**
>               与GBDT区别：
>                   1、使用L1、L2防止过拟合
>                       误差函数中添加L1和L2，降低过拟合概率
>                   2、对代价函数一阶和二阶求导，更快的收敛Converge
>                       速度更快
>                   3、树长全后从底部向上剪枝，防止贪婪算法
>                       防止overfiting
>
>

- **待续：**
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
