### 七月在线机器学习 -- 决策树之算法
- **概述：**
>
>       本篇文章主要讲一下决策树相关的集成算法bagging和boosting两种
>
>       Bagging与boosting区别：
>           1、Bagging：树并行生成，如RF
>               boosting：树串行生成，如Adaboost
>           2、RF中的基树大多数情况下是强模型，boosting中的基模型为弱模型
>           3、boosting重采样的不是样本，而是样本的分布
>
>
>       Adaboost缺点：
>           1、Adaboost对于噪音数据和异常数据是十分敏感的。
>               boosting方法本身对噪声点异常点很敏感，因此在每次迭代时候会给噪声点较大的权重，这个不是我们系统期望的
>           2、运行速度慢
>               凡是涉及迭代的基本上都无法采用并行计算，Adaboost是一种串行算法，所以GBDT也非常慢
>
>
>

- **树形结构为什么不需要归一化？**
>
>       数值缩放，不影响分裂点位置。因为第一步都是按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分分裂点就不会有不同。
>       而对于线性模型，如LR，有两个特征，一个是(0,1)，一个是(0,100000)，这样运用梯度下降时，损失等高线是一个椭圆的形状，这样想迭代到最优点，就需要很多次迭代，
>           但是如果进行了归一化，那么等高线就是圆形的，那么SGD就会往原点迭代，需要迭代次数较少。
>       注意：
>           树模型是不能进行梯度下降的，因为树模型是阶跃的，阶跃点是不可导的，并且求导没意义，所以树模型（回归树）寻找最优点是通过寻找最优分裂点完成的。
>
>
>
>
>
>

- **boosting算法之Adaboost：**
>
>       boosting的代表算法是Adaboost，Adaboost相对于其他算法而言，不会很容易出现过拟合现象。
>       强学习器是基学习器的线性组合形式。
>
>       思想：
>           Adaboost算法让正确率高的分类器占整体的权值更高，基分类器的权值是关于误差的表达式，让错误率更高的分类器权值更低，从而提高最终分类器的正确率。
>       如下图，
> ![avatar](https://github.com/nwaiting/wolf-ai/blob/master/wolf_others/pic/decision_tree_adaboost.jpg)
>           第1行：初始化样本权重分布
>           第2行：for循环T次，训练T个基学习器
>           第3行：学习出基分类器
>           第4行：计算当前学习器的误差
>           第5行：如果误差大于0.5就停止
>           第6行：计算当前学习器的权重
>           第7行：得到下一时刻的样本权重分布
>           最后计算总体分类的结果
>
>       目标函数：
>           指数损失函数
>           l(H|D) = Ex~D[e^(f(x)H(x))]
>           为什么目标函数用指数损失函数？
>           答：若指数损失达到最小，则分类错误率也将最小化，也就是说指数损失可以作为分类任务0-1损失的替代函数，因为它连续可微，就用它来替代0-1损失函数作为优化目标。
>
>       基分类器和权重：
>           第一个基分类器由初始数据产生，之后的at(基分类器权重)，ht(基分类器)都是迭代产生
>           基分类器权重at：
>               想要求at使得at*ht最小化指数损失函数，则对at求导等于0，求出at的表达式
>           学习到h(t-1)后，希望下一个ht可以纠正h(t-1)的全部错误，即使下面的目标函数l(h(t+1)+ht|D)达到最小
>

- **Gradient Boosting**
>
>       Gradient Boosting是boosting的一种方法，主要思想：
>           每一次建立单个学习器时，是在之前建立的模型的损失函数的梯度下降方向。
>
>

- **boosting算法之GBDT：**
>
>       2001年提出Gradient Boosting框架，将损失函数扩展到更一般的情况，即通过梯度计算回归拟合的残差（准确应称之为伪残差），基于该残差生成新的基学习器，并计算其最优的叠加叠加权重值。
>       如果选择基分类器为决策树（如CART），则对应为GBDT算法。
>       CART树是分类树，并不满足GB回归损失函数的要求，因此需要将分类GINI系数指标替换为最小均方差，即当所有分支的预测结果唯一或是达到叶节点数量上限时，以该树所有节点的预测均值作为该分类器的预测结果。
>       GBDT还借鉴了Bagging集成学习的思想，**通过随机抽样提高模型泛化能力，通过交叉验证选择最优参数等**。
>
>       GBDT（Gradient Boosting Decision Tree）是以决策树(CART)为基学习器的GB算法，是迭代树，而不是分类树。
>       GBDT核心思想：
>           GBDT中每一棵树学的都是之前所有树结论和的残差。这个残差就是一个加预测值后能得真实值的累加量。
>           如：A的真实年龄为18岁，第一棵树预测年龄为12，则残差为6，第二棵树把A的年龄设置为6，如果第二棵树把A分到了6岁的叶子节点，那么累加两棵树的结论就是A的真实年龄。
>
>

- **GBDT与Adaboost的区别：**
>
>       Adaboost可以表示为boosting的前向分布算法的一个特例。
>       如何选择损失函数决定了算法的名字，如下图，
> ![avatar](https://github.com/nwaiting/wolf-ai/blob/master/wolf_others/pic/decision_tree_boosting_loss.jpg)
>       不同的损失函数和最小化损失函数方法决定了boosting的最终效果，上图为常见的boosting
>
>       区别：
>           Adaboost通过提升错分数据点的权重来定位模型的不足
>           而Gradient Boosting是通过计算梯度来定位模型的不足
>
>

- **GBDT与XGBoost区别：**
>
>       除了很多优化外，与GBDT的区别：
>       1、XGBoost是GB算法的高效实现，XGBoost中基学习器可以是CART，也可以是线性分类器
>       2、XGBoost在目标函数中显式地加上了正则化项
>       3、GB中使用Loss function对f(x)的一阶导数计算出伪残差用于学习生成fm，XGBoost不仅使用了一阶导数，还使用了二阶导数
>       4、CART回归树中寻找最佳分割点的衡量标准是最小化均方差，XGBoost寻找分割点的标准是最大化lamda、gama与正则化相关
>
>
>
>
>


- **boosting算法之xgboost：**
>
>       2014年，陈天奇博士提出XGBoost算法，XGBoost算法在基学习器损失函数中引入正则项，控制减少训练的过拟合。
>           XGBoost不仅使用一阶导数计算伪残差，还计算二阶导数可近似快速剪枝的构建新的基学习器
>           XGBoost还做了工程上的优化，如：并行计算、提高计算效率、处理系数训练数据等
>
>       **XGBoost相对于GBDT来说，更加有效应用数值优化，最重要是对损失函数（预测值和真实值的误差）变得更复杂**
>       目标函数：
>           目标函数依然是所有树的预测值相加等于预测值
>       损失函数：
>           损失函数中引入一阶导数，二阶导数
>
>       优秀的模型应具备的条件：
>           好的模型要具备两个基本要素，
>           a、要有好的精度（即好的拟合程度）
>           b、模型要尽可能简单（复杂模型容易出现过拟合，并且更加不稳定）
>           因此构建的目标函数右边第一项是模型的误差项，第二项是正则化项（就是模型复杂度的惩罚项）
>           常用的误差项有平方误差和逻辑斯蒂误差
>           常见的惩罚有L1和L2正则，L1是将模型各个元素进行求和，L2是对元素求平方
>       算法细节：
>           每一次迭代，都在现有树的基础上，增加一棵树去拟合前面树的预测结果与真实值之间的残差
>       如何得到优秀的组合树呢？？？
>           方法1：贪心算法
>               遍历节点内的所有特征，按照公式计算出每一个特征分割的信息增益，找到信息增益最大的点进行树的分割
>           方法2：
>               不会枚举所有的特征值，而对特征值进行聚合统计，按照特征值的密度分布，构造直方图计算特征值分布的面积
>

- **xgboost相比于GBDT创新之处：**
>
>       传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这样xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）
>       1、传统GBDT在优化时候只用到一阶导数信息，XGBoost对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数
>       2、XGBoost在代价函数里加入了正则项，用于控制模型的复杂度。使学习出来的模型更加简单，防止过拟合，优于GBDT
>       3、shrinkage（缩减），相当于学习速率（XGBoost的eta）。每次迭代，增加新的模型
>       4、列抽样。
>           XGBoost借鉴了RF的做法，支持列抽样（即每次输入的特征不是全部特征），不仅能降低过拟合，还能减少计算，优于GBDT
>       5、并行化处理
>           a、在训练之前，预先对每个特征内部进行了排序找出候选切割点，然后保存为block结构，后面的迭代中重复使用这个结构，大大减小计算量
>           b、在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行，即在不同的特征属性上采用多线程并行方式寻找最佳分割点。
>
>
>
>
>
>
>
>
>
>
>
>

- **待续：**
>       参考：https://zhuanlan.zhihu.com/p/34534004
>           https://www.jianshu.com/p/389d28f853c0      详细介绍 Adaboost 算法
>           https://www.jianshu.com/p/d55f7aaac4a7      浅谈 GBDT以及与Adaboost和XGBoost区别
>           https://www.jianshu.com/p/28f02bb59fe5
>           https://www.jianshu.com/p/708dff71df3a
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
