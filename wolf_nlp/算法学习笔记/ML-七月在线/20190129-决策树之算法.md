### 七月在线机器学习 -- 决策树之算法
- **概述：**
>
>       本篇文章主要讲一下决策树相关的集成算法bagging和boosting两种
>
>

- **boosting算法之Adaboost：**
>
>       boosting的代表算法是Adaboost，Adaboost相对于其他算法而言，不会很容易出现过拟合现象。
>       强学习器是基学习器的线性组合形式。
>
>       思想：
>           Adaboost算法让正确率高的分类器占整体的权值更高，基分类器的权值是关于误差的表达式，让错误率更高的分类器权值更低，从而提高最终分类器的正确率。
>       如下图，
> ![avatar](https://github.com/nwaiting/wolf-ai/blob/master/wolf_others/pic/decision_tree_adaboost.jpg)
>           第1行：初始化样本权重分布
>           第2行：for循环T次，训练T个基学习器
>           第3行：学习出基分类器
>           第4行：计算当前学习器的误差
>           第5行：如果误差大于0.5就停止
>           第6行：计算当前学习器的权重
>           第7行：得到下一时刻的样本权重分布
>           最后计算总体分类的结果
>
>       目标函数：
>           指数损失函数
>           l(H|D) = Ex~D[e^(f(x)H(x))]
>           为什么目标函数用指数损失函数？
>           答：若指数损失达到最小，则分类错误率也将最小化，也就是说指数损失可以作为分类任务0-1损失的替代函数，因为它连续可微，就用它来替代0-1损失函数作为优化目标。
>
>       基分类器和权重：
>           第一个基分类器由初始数据产生，之后的at(基分类器权重)，ht(基分类器)都是迭代产生
>           基分类器权重at：
>               想要求at使得at*ht最小化指数损失函数，则对at求导等于0，求出at的表达式
>           学习到h(t-1)后，希望下一个ht可以纠正h(t-1)的全部错误，即使下面的目标函数l(h(t+1)+ht|D)达到最小
>

- **Gradient Boosting**
>
>       Gradient Boosting是boosting的一种方法，主要思想：
>           每一次建立单个学习器时，是在之前建立的模型的损失函数的梯度下降方向。
>
>
>
>
>
>
>
>
>
>
>
>

- **boosting算法之GBDT：**
>
>       2001年提出Gradient Boosting框架，将损失函数扩展到更一般的情况，即通过梯度计算回归拟合的残差（准确应称之为伪残差），基于该残差生成新的基学习器，并计算其最优的叠加叠加权重值。
>       如果选择基分类器为决策树（如CART），则对应为GBDT算法。
>       CART树是分类树，并不满足GB回归损失函数的要求，因此需要将分类GINI系数指标替换为最小均方差，即当所有分支的预测结果唯一或是达到叶节点数量上限时，以该树所有节点的预测均值作为该分类器的预测结果。
>       GBDT还借鉴了Bagging集成学习的思想，**通过随机抽样提高模型泛化能力，通过交叉验证选择最优参数等**。
>
>       GBDT（Gradient Boosting Decision Tree）是以决策树(CART)为基学习器的GB算法，是迭代树，而不是分类树。
>       GBDT核心思想：
>           GBDT中每一棵树学的都是之前所有树结论和的残差。这个残差就是一个加预测值后能得真实值的累加量。
>           如：A的真实年龄为18岁，第一棵树预测年龄为12，则残差为6，第二棵树把A的年龄设置为6，如果第二棵树把A分到了6岁的叶子节点，那么累加两棵树的结论就是A的真实年龄。
>
>

- **boosting算法之xgboost：**
>
>       2014年，陈天奇博士提出XGBoost算法，XGBoost算法在基学习器损失函数中引入正则项，控制减少训练的过拟合。
>           XGBoost不仅使用一阶导数计算伪残差，还计算二阶导数可近似快速剪枝的构建新的基学习器
>           XGBoost还做了工程上的优化，如：并行计算、提高计算效率、处理系数训练数据等
>
>       XGBoost相对于GBDT来说，更加有效应用数值优化，最重要是对损失函数（预测值和真实值的误差）变得更复杂
>       目标函数：
>           目标函数依然是所有树的预测值相加等于预测值
>       损失函数：
>           损失函数中引入一阶导数，二阶导数
>
>
>
>
>
>
>
>
>
>
>
>

- **待续：**
>       参考：https://www.jianshu.com/p/389d28f853c0
>           https://www.jianshu.com/p/d55f7aaac4a7
>           https://www.jianshu.com/p/28f02bb59fe5
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
