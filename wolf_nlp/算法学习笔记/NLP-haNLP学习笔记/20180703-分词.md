### NLP-HanLP -- 分词
- **概述**：
>        目前基于统计的方法是中文分词、词性标注中的主流方法。问题统一抽象为序列标注问题，即给定序列X,求出最优标记序列
>        其中一类方法：
>           从概率角度来估计X和Y的概率分布，常用方法为HMM、ME、CRF
>        另外一种序列标注算法：
>           定义观测序列X和状态序列Y的分数为：
>           score(X,Y) = ∑Wk*fk(X,Y)
>           其中fk(X,Y)为特征函数，Wk为第k个特征的权重
>
>       词性标注集：
>           每个分词包含词性，HaNLP使用的HMM词性标注模型训练自2014年人民日报切分语料，兼容ICTPOS3.0汉语词性标注集，也兼容《现代汉语语料库加工规范》
>
>       词性标注：
>           汉语中词性标注比较简单，因为汉语中词性多变情况比较少见，大多数只有一个词性，或出现频次最高的词性远远高于第二位的词性。
>           利用HMM即可实现更高准确率的词性标注
>           步骤：
>               1、统计了2014年的人名日报切分语料，统计单词词性频次
>               2、统计词性之间的频次转移矩阵
>               3、利用1和2步的统计结果，可以得到HMM的初始概率、转移概率、发射概率，根据viterbi算出最大概率的标注序列
>
>
>
>

- **标准分词-词图：**
>       利用核心词典，就可以生成词图了。词图即句子中所有词可能构成的图。
>
>

- **HaNLP的中文分词流程：**
>       中文分词步骤：
>           1、句子输入，导入词典（中小等规模的数据一般用hash进行存储和查询效率高，当数据大于百万级别后，hash的空间利用率太低）
>           2、句子切分
>           3、词汇初分（会应用到核心词典、二元词典）
>               a、一元切分：
>                   首先对句子进行字符级切分，对输入的句子切分为单个中文字符、英文字符等
>               b、原子切分：
>                   **将字符切分的结果与词典词最大匹配**，匹配的结果包括词性、词形、词频等形成一元词网，之后对一元词网进行原子切分
>               c、二元切分：
>                   用一元分词的结果（二维数组）**查询二元词典**，**与二元词典进行最大匹配**，匹配的结果为一个graph，形成一个词图
>               d、N-最短路径切分：
>                   将查出的每个结果按照平滑算法计算二元分词的词频数得到词图中每个节点的权值（概率导数），应用NShort算法累加词图中每个节点构成的所有路径，概率最大的即为初分结果
>               e、应用规则
>                   对初分结果执行后处理应用规则，识别时间类专有名词
>           4、未登录词识别（应用到人名识别词典、地名识别词典、翻译人名识别词典）
>               a、人名识别：
>                   使用viterbi算法识别外国人名
>               b、翻译人名识别：
>                   使用viterbi识别地名
>               c、地名识别：
>                   Dijkstra算法识别组织机构名
>           5、优化细分逻辑同切分流程：
>               将命名实体识别后的分词结果加入词图中，对词图再次进行分词（Dijkstra最短路径法）
>           6、词性标注：
>               Viterbi算法，对分词结果进行词性标注
>           7、后处理
>           8、输出结果
>

- **ICTCLAS分词的流程：**
>       ICTCLAS分词的总体流程包括：
>           1、初步分词
>               (初步分词是一个比较复杂的过程，涉及的数据结构、数据表示以及相关算法都颇有难度)
>               a、原子切分，比如 “始##始/他/说/的/确/实/在/理/末##末”
>               b、找出原子之间所有可能的组词方案，比如 “始##始/他/说/的/的确/确/确实/实/实在/在/在理/末##末”
>               c、N-最短路径中文词语粗分
>                   (a)找出词之间的两两组合距离(查找BiGramDict词典)，构造词图，词图和有向图对应
>                   (b)最后如何组词需要在整句环境下考虑，达到整体最优。这就是N-最短路径需要完成的工作
>                       N-最短路径求解比较复杂，先从1-最短路径理清其流程，然后推广到N-最短路径
>           2、词性标注
>           3、人名、地名识别
>           4、重新分词
>           5、重新词性标注
>
>

- **常见的分词算法：**
>       **1、最短路径、N-最短路径分词：**
>           速度与精度平衡
>           N-最短路径：
>               《基于N-最短路径方法的中文词语粗分模型》（张华平、刘群，中文信息学报，16卷5期）
>               改算法是中科院分词工具NLPIR进行分词用到的一个重要算法
>               算法思想：
>                   就是给定以待处理字串，根据词典，找出词典中所有可能的词，够找出字串的一个有向无环图，算出从开始到结束所有路径中最短的前N条路径。
>                   NShortPath基本思想是Dijkstra算法的变种，比如1-最短路来说，先Dijkstra求一次最短路，然后沿着最短路的路径走下去，只不过在走到某个节点的时候，
>                   **检查到该结点在路径上的下一个节点是否还有别的路径到它（从PreNode查）**，如果有，就走这些别的鲁中的没走过的第一条（它们都是最短路径上的途径节点），
>                   推广到N-最短路径，N-最管路中PreNode有N个，分别对应n-最短路时候的PreNode
>               计算最短路径：
>                   N-最短路径是基于Dijkstra算法的一种简单扩展，在每个结点处记录了N个最短路径值与该结点的前驱，最后找出N个最短路径集合
>
>       **2、HMM的分词：**
>           jieba分词就是使用BEMS标签来分词
>           给定一个句子，找出最可能的词性序列（BEMS标签序列）
>
>
>       **3、基于结构化平均感知机的中文分词：**
>           侧重精度，可识别新词，适合NLP任务
>           利用感知机做序列标注任务，应用到中文分词、词性标注和命名实体识别
>           结构化：
>               AP分词器预测的是真个句子的BEMS标注序列，属于结构化预测问题
>           感知机：
>
>
>
>       **4、CRF分词：**
>           *[CRF相关](https://github.com/nwaiting/wolf-ai/blob/master/wolf_nlp/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/NLP%E6%B1%89%E8%AF%AD%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/20180322-crf.md)
>           侧重精度，可识别新词，适合NLP任务
>           特点：
>               1、对未登录词有更好的支持
>               2、比HMM能利用更多的特征，比MEMM更能抵抗标记偏执的问题
>           CRF解码为所有字打上BEMS标签：
>               使用viterbi算法实现，首先任何字的标签不仅取决于它自己的参数，还取决于前一个字的标签。
>               但是第一个字的标签如何计算？
>                   假设最开始的标签为X集合，遍历X计算第一个字的标签，去分数最大的那一个。
>               如何计算标签分数？
>                   某个字根据CRF模型提供的模板生成了一系列特征函数，这些函数的输出值乘以该函数的权值最后求和得出一个分数，改分数为"点函数"的得分，还需要加上"边函数"的得分
>                   边函数在本分词模型中简化为f(s',s)，s'为前一个字的标签，s为当前字的标签。于是该边函数就可以用一个4*4的矩阵表示，相当于HMM的转移概率矩阵
>               实现了评分函数后，从第二个字开始即可运用维特比后向解码，为所有字打上NEMS标签
>           **CRF分词步骤**：
>               先根据模型参数，对每个字代入到特征函数中，得到01010的函数返回值，然后在乘上函数的权值求和，得到各个标签的分数，然后输出标签的可能性
>
>
>       **5、极速词典分词：**
>           侧重速度，每秒数千万字符，省内存
>
>
>
>
>
>
>
>
>
>
>
>

- **待续：**
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
