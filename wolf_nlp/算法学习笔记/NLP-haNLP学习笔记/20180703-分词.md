### NLP-HanLP -- 分词
- **概述**：
>        目前基于统计的方法是中文分词、词性标注中的主流方法。问题统一抽象为序列标注问题，即给定序列X,求出最优标记序列
>        其中一类方法：
>           从概率角度来估计X和Y的概率分布，常用方法为HMM、ME、CRF
>        另外一种序列标注算法：
>           定义观测序列X和状态序列Y的分数为：
>           score(X,Y) = ∑Wk*fk(X,Y)
>           其中fk(X,Y)为特征函数，Wk为第k个特征的权重
>
>       两种分词标准：
>           粗粒度分词：
>               粗粒度分词主要应用在自然语言的引用中
>           细粒度分词：
>               细粒度分词主要应用在搜索引擎中
>
>       词性标注集：
>           每个分词包含词性，HaNLP使用的HMM词性标注模型训练自2014年人民日报切分语料，兼容ICTPOS3.0汉语词性标注集，也兼容《现代汉语语料库加工规范》
>
>       词性标注：
>           汉语中词性标注比较简单，因为汉语中词性多变情况比较少见，大多数只有一个词性，或出现频次最高的词性远远高于第二位的词性。
>           利用HMM即可实现更高准确率的词性标注
>           步骤：
>               1、统计了2014年的人名日报切分语料，统计单词词性频次
>               2、统计词性之间的频次转移矩阵
>               3、利用1和2步的统计结果，可以得到HMM的初始概率、转移概率、发射概率，根据viterbi算出最大概率的标注序列
>
>
>
>

- **分词要点：**
>       1、要点1
>           实际应用中，基于统计的分词系统都需要使用分词词典来进行字符串分词匹配，同时使用统计方法识别一些新词，即将字符串统计和匹配相结合
>
>       2、要点2
>           实际应用中，都是把机械分词作为一种初分手段，然后再通过利用各种其它的语言信息来进一步提高切分的准确率。
>           但是对于预处理过程来讲，**粗分结果的高召回率至关重要**，因为低召回率就意味着没有办法再作后续的补救措施。
>
>
>
>


- **标准分词-词图：**
>       利用核心词典，就可以生成词图了。词图即句子中所有词可能构成的图。
>
>

- **HaNLP的中文分词流程：**
>       中文分词步骤：
>           1、句子输入，导入词典（中小等规模的数据一般用hash进行存储和查询效率高，当数据大于百万级别后，hash的空间利用率太低）
>           2、句子切分
>           3、词汇初分（会应用到核心词典、二元词典）
>               a、一元切分：
>                   首先对句子进行字符级切分，对输入的句子切分为单个中文字符、英文字符等
>               b、原子切分：
>                   **将字符切分的结果与词典词最大匹配**，匹配的结果包括词性、词形、词频等形成一元词网，之后对一元词网进行原子切分
>               c、二元切分：
>                   用一元分词的结果（二维数组）**查询二元词典**，**与二元词典进行最大匹配**，匹配的结果为一个graph，形成一个词图
>               d、N-最短路径切分：
>                   将查出的每个结果按照平滑算法计算二元分词的词频数得到词图中每个节点的权值（概率导数），应用NShort算法累加词图中每个节点构成的所有路径，概率最大的即为初分结果
>               e、应用规则
>                   对初分结果执行后处理应用规则，识别时间类专有名词
>           4、未登录词识别（应用到人名识别词典、地名识别词典、翻译人名识别词典），使用隐马模型：
>               a、人名识别：
>                   使用viterbi算法识别外国人名
>               b、翻译人名识别：
>                   使用viterbi识别地名
>               c、地名识别：
>                   Dijkstra算法识别组织机构名
>           5、优化细分逻辑同切分流程：
>               将命名实体识别后的分词结果加入词图中，对词图再次进行分词（Dijkstra最短路径法）
>           6、词性标注，使用词性标注模型：
>               Viterbi算法，对分词结果进行词性标注
>           7、后处理
>           8、输出结果
>

- **ICTCLAS分词的流程：**
>       ICTCLAS分词的总体流程包括：
>           1、初步分词
>               (初步分词是一个比较复杂的过程，涉及的数据结构、数据表示以及相关算法都颇有难度)
>               a、原子切分，比如 “始##始/他/说/的/确/实/在/理/末##末”
>               b、找出原子之间所有可能的组词方案，比如 “始##始/他/说/的/的确/确/确实/实/实在/在/在理/末##末”
>               c、N-最短路径中文词语粗分
>                   (a)找出词之间的两两组合距离(查找BiGramDict词典)，构造词图，词图和有向图对应
>                   (b)最后如何组词需要在整句环境下考虑，达到整体最优。这就是N-最短路径需要完成的工作
>                       N-最短路径求解比较复杂，先从1-最短路径理清其流程，然后推广到N-最短路径
>           2、词性标注
>           3、人名、地名识别
>           4、重新分词
>           5、重新词性标注
>
>

- **常见的分词算法：**
>       **1、最短路径、N-最短路径分词：**
>           速度与精度平衡
>           N-最短路径：
>               《基于N-最短路径方法的中文词语粗分模型》（张华平、刘群，中文信息学报，16卷5期）
>               改算法是中科院分词工具NLPIR进行分词用到的一个重要算法
>               算法思想：
>                   就是给定以待处理字串，根据词典，找出词典中所有可能的词，够找出字串的一个有向无环图，算出从开始到结束所有路径中最短的前N条路径。
>                   NShortPath基本思想是Dijkstra算法的变种，比如1-最短路来说，先Dijkstra求一次最短路，然后沿着最短路的路径走下去，只不过在走到某个节点的时候，
>                   **检查到该结点在路径上的下一个节点是否还有别的路径到它（从PreNode查）**，如果有，就走这些别的鲁中的没走过的第一条（它们都是最短路径上的途径节点），
>                   推广到N-最短路径，N-最管路中PreNode有N个，分别对应n-最短路时候的PreNode
>               计算最短路径：
>                   N-最短路径是基于Dijkstra算法的一种简单扩展，在每个结点处记录了N个最短路径值与该结点的前驱，最后找出N个最短路径集合
>
>       **2、HMM的分词：**
>           jieba分词就是使用BEMS标签来分词
>           给定一个句子，找出最可能的词性序列（BEMS标签序列）
>
>
>       **3、基于结构化平均感知机的中文分词：**
>           侧重精度，可识别新词，适合NLP任务
>           利用感知机做序列标注任务，应用到中文分词、词性标注和命名实体识别
>           结构化：
>               AP分词器预测的是真个句子的BEMS标注序列，属于结构化预测问题
>           感知机：
>               使用感知机预测词的BEMS标注类别
>
>
>       **4、CRF分词：**
>           *[CRF相关](https://github.com/nwaiting/wolf-ai/blob/master/wolf_nlp/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/NLP%E6%B1%89%E8%AF%AD%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/20180322-crf.md)
>           侧重精度，可识别新词，适合NLP任务
>           特点：
>               1、对未登录词有更好的支持
>               2、比HMM能利用更多的特征，比MEMM更能抵抗标记偏执的问题
>           CRF解码为所有字打上BEMS标签：
>               使用viterbi算法实现，首先任何字的标签不仅取决于它自己的参数，还取决于前一个字的标签。
>               但是第一个字的标签如何计算？
>                   假设最开始的标签为X集合，遍历X计算第一个字的标签，去分数最大的那一个。
>               如何计算标签分数？
>                   某个字根据CRF模型提供的模板生成了一系列特征函数，这些函数的输出值乘以该函数的权值最后求和得出一个分数，改分数为"点函数"的得分，还需要加上"边函数"的得分
>                   边函数在本分词模型中简化为f(s',s)，s'为前一个字的标签，s为当前字的标签。于是该边函数就可以用一个4*4的矩阵表示，相当于HMM的转移概率矩阵
>               实现了评分函数后，从第二个字开始即可运用维特比后向解码，为所有字打上NEMS标签
>           **CRF分词步骤**：
>               先根据模型参数，对每个字代入到特征函数中，得到01010的函数返回值，然后在乘上函数的权值求和，得到各个标签的分数，然后输出标签的可能性
>
>
>       **5、极速词典分词：**
>           侧重速度，每秒数千万字符，省内存
>
>

- **分词方法总结：**
>       1、基于字符串匹配的方法，机械分词（速度快，对歧义和未登录词处理不佳）
>           a、正向最大匹配（邻近匹配算法，是正向最大匹配算法的改进）
>           b、逆向最大匹配（一般来说，逆向匹配的切分精度略高于正向匹配，遇到的歧义现象也比较少）
>           c、双向最大匹配
>           d、最少切分分词算法
>           e、N-最短路径分词算法
>               缺点是粗分结果不唯一，后续过程需要处理多个粗分结果。
>       2、基于统计的方法，无字典分词
>           a、N元文法模型
>           b、隐马模型分词（序列标注）
>           c、CRF模型分词（序列标注）
>           d、最大熵模型
>       3、基于语义（不成熟）
>           基本思想是在分词的同时进行句法、语义分析，利用句法信息和语义信息来处理歧义现象。
>       4、深度学习方法
>
>       各种分词方法总结：
>           机械分词：
>               扫描字符串，与词典中的词进行匹配。这类分词通常会加入一些启发式规则，如："正向/反向最大匹配"、长词优先等
>           基于统计和机器学习的分词：
>               基于人工标注的词性和统计特性，对标注特征进行建模，对模型参数进行训练，在分词阶段通过模型计算各种分词出现的概率，
>               将概率最大的分词结果作为最终结果。
>               常见的序列标注模型HMM、CRF等，这类分词算法能很好处理歧义和未登录词问题。
>               缺点：需要大量的人工标注数据
>           语义分词：
>
>       实际应用：
>           实际应用中，基于统计的分词系统都需要使用分词词典来进行字符串分词匹配，同时使用统计方法识别一些新词，即将字符串统计和匹配相结合，
>           既有匹配分词切分速度快、效率高，也结合了无词典分词统计结合上下文识别未登录词、自动消歧的优点
>           实际应用中，都是把机械分词作为一种初分手段，然后再通过利用**各种其它的语言信息**来进一步提高切分的准确率。！！！！
>
>
>       不同场景应用的分词：
>           搜索引擎：
>               在精确模式基础上，对长词再次进行切分，提高召回率，适用搜索引擎
>
>

- **机械分词的一些启发式规则：**
>       比如MMSEG机械分词应用规则，机械分词中经常会将多种分词进行组合，进行歧义消除，来得到最优的分词结果。
>       MMSEG消除歧义的规则有四个，一次使用四个规则进行过滤，直到只有一个结果或者四个规则用完。
>       1、规则1：
>           最大匹配。选择“词组长度最大的”那个词组，然后选择这个词组的第一个词作为切分出来的第一个词。
>           如"中国人民万岁"，匹配结果为：
>               中/国/人
>               中国/人/民
>               中国/人民/万岁
>               中国人/民/万岁
>           这个例子的“词组长度最长的”的词组为后两个，因此选择"中国"或"中国人"作为第一个切分出来的词
>       2、规则2：
>           最大平均词语长度。经过规则1过滤后，如果剩余的词组超过1，则选择平均词语长度最大的那个（平均词长=词组总词数/词语数量）
>           如"生活水平"，得到词组：
>               生/活水/平 (4/3=1.33)
>               生活/水/平 (4/3=1.33)
>               生活/水平 (4/2=2)
>           根据规则，可以确定选择"生活/水平 (4/2=2)"这个词组
>       3、规则3：
>           词语长度的最小变化率，这个变化率一般可以由标准差来决定。
>           如"中国人民万岁"这个短语,计算
>               中国/人民/万岁(标准差=sqrt(((2-2)^2+(2-2)^2+(2-2^2))/3)=0)
>               中国人/民/万岁(标准差=sqrt(((2-3)^2+(2-1)^2+(2-2)^2)/3)=0.8165)
>           于是选择"中国/人民/万岁"这个词组
>       4、规则4：
>           计算词组中所有单字词词频的自然对数，然后将所有值相加，取总和最大的词组。
>           如：
>               设施/和服/务
>               设施/和/服务
>           这两个词组中分别有单字"务"和"和"，分别对"务"和"和"取对数，对数值大的，则为切分结果。
>           如"和"词频为10，"务"词频为5，则对5和10取对数，最后取单字"和"的词组。
>
>
>       **基于字符串匹配的分词和其他方法进行结合或者算法改进：**
>           1、改进扫描方法
>               也称为特征扫描或者标志切分
>               优先在待分析字符串中识别和切分出一些带有明显特征的词，以这些词作为断点，可将原字符串分为较小的串再来进行机械分词，从而较少匹配错误率
>           2、将分词和词类标注结合起来
>               利用丰富的词类信息对分词决策提供帮助，并且在标注过程中，又反过来对分词结果进行检验、调整，从而提高切分效果。
>

- **N元文法模型：**
>       N元文法模型是一种生成式模型，
>       算法步骤：
>           1、先根据词典对句子进行简单匹配，找出所有可能的字典词
>           2、将字典词和所有单字作为节点，构造n元切分词图，节点表示候选词，边表示路径，边上的n元概率表示代价，最后用dp找出代价最小的分词结果
>       缺点：
>           分词是基于统计的分词算法，比机械分词精度更高，但是算法是基于现有词典，很难发现新词。
>
>

- **HMM模型：**
>       HMM是结构最简单的动态贝叶斯网络，这是一种尤其著名的有向图模型。
>       在分词算法中，HMM常用作能发现新词的算法，通过海量的数据学习，能够将人们、地名等新词识别出来。
>       **中文分词的问题就是通过观察序列来预测最优的状态序列。！！！！**
>       **HMM分词算法是基于字的状态（BMES）来进行分词的，因此很适合用于新词发现。如某一个新词只要标记为"BMME"，就可以识别出该词，不管是否在词典中出现**
>       HMM的三大问题对应了分词的几个步骤：
>           1、参数估计问题
>               即是分词的学习阶段，通过海量的预料数据来学习归纳出分词模型的各个参数。
>           2、状态序列问题
>               分词的执行阶段，通过观察集合（待分词句子）来预测最优的状态序列（分词结构）
>

- **CRF模型：**
>       CRF是一种判别式无向图模型，和HMM通过联合分布进行建模不同，CRF试图对多个变量在给定观测值后的条件概率建模。
>       如x={x1,x2...}为观测序列，y={y1,y2,...}表示对应的状态序列，CRF的目标是构建条件概率模型P(y|x)。
>       CRF使用特征函数和图结构上的团来定义条件概率P(y|x)
>       其中，已知训练数据集，由此可知经验概率分布，**可以通过极大化训练数据的对数似然函数来求模型参数。**可以加入惩罚项，然后进行训练。
>
>
>
>

- **待续：**
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
