### NLP-HanLP -- HMM分词
- **概述**：
>       参考：http://www.hankcs.com/nlp/segment/second-order-hidden-markov-model-trigram-chinese-participle.html
>       Stupid backoff：回退平滑算法
>           回退的含义是比如要计算P(wn|wn-2,wn-1)，但没有相关的三元统计，则可以使用二元统计P(wn|wn-2)来估计，如果没有相关的二元统计，我们就用一元模型P(wn)来估计
>       谈起基于Character-Based Generative Model（基于字符的生成模型）的中文分词方法，普遍的现象是BackOff上的成绩好，对集外词OOV（未登录词）的识别率高。
>           HaNLP中实现的CRF分词器就是这种原理的分词器，CRF的缺点也很明显：
>               1、模型体积大占内存
>                   一个可供生产使用的CRF模型至少使用前中后3个字符的组合做特征模板，在一两百兆的语料上训练，模型体积至少上百兆，加载后更好资源
>               2、速度慢
>                   相较与基于词语的Bigram分词器，速度慢的原因为：
>                       1、特征函数的查询次数多、
>                       2、概率图的节点更多（4倍文本长度个节点，4是BMES标签的个数），然后viterbi速度也慢了
>               3、无法修改
>                   如果用户对分词结果不满意，但是无法方便的修正。只有基于NGram词典和词频词典可以轻松修改，包括CRF在内的其他模型都需要重新训练
>
>       基于字符的HMM-BiGram分词器
>       基于HMM^2-TriGram的字符序列标注分词器
>

- **HMM：**
>       马尔科夫模型有两个假设：
>           1、时刻t的状态只与时刻t-1的状态相关（无后效性）
>           2、状态转移概率与时间无关（齐次性或时齐性）
>       隐马模型：
>           双重随机过程
>               1、状态转移之间是随机过程
>               2、状态到输出也是一个随机过程
>           五元组：
>               隐藏、观测集合、转移、发射、初始概率矩阵
>           前向算法、viterbi算法都是dp算法
>
>       应用：
>           拼音看做是观察状态，汉字是隐藏状态，那么输入法的整句解码就变成了维特比解码，其转移概率为二元语言模型，输出概率即是多音字对应不同拼音的概率
>           将拼音换成语音，则就变成了语音识别问题，转移概率仍然是二元语言模型，输出概率则是语音模型，即语音和汉字的对应模型
>
>

- **HMM的扩展优化：**
>       扩展：
>           1、对三大假设的时齐性进行扩展，假设状态转移概率与时间有关，这在输入法中也有实际意义的
>               参考 论文《一种非时齐性的隐马尔科夫模型在音字转换中的应用》
>
>           2、对无后效性假设进行扩展，原来只假设某状态只与前一状态有关，以至于只能使用二元语言模型，现假设某状态与前两个甚至更多个状态有关，这样就能使用多元语言模型。
>               如果我们假设与前两个状态有关，那么就可以使用三元语言模型，但是viterbi算法会出现问题，因为t时刻的状态概率不仅要考虑t-1时刻的状态，还要考虑t-2时刻的状态
>               解决viterbi算法在三元模型（二阶HMM问题）下的问题方法：
>                   合并前后两个状态将二阶HMM转化成一阶HMM问题
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>

- **二阶HMM：**
>       HMM本质上是一个Bigram的语法模型，未能深层次地考虑上下文（context）
>       二阶HMM对应三阶文法模型，连个称呼的阶数相差一个
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>

- **待续：**
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
