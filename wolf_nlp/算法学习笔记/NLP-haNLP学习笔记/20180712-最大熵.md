### NLP-HanLP -- EM
- **概述**：
>       最大熵模型其实就是在满足一直约束的条件下求得熵最大的过程，最终会转为一个解约束最优化的问题
>       最大熵模型就是根据已知的输入和输出集合去学习训练数据的条件概率分布，应用最大熵原理去学习分类能力最好的模型
>       约束条件：
>           对于给定的训练数据集，我们可以确定联合分布的经验分布以及边缘分布的经验分布
>           引入特征函数，特征函数为二值函数
>
>       求解最大熵过程：
>           1、利用拉格朗日乘子将带约束的最优化问题转为等价无约束优化，是一个极值问题
>           2、利用对偶的等价性，将极值问题转为对偶的极值问题
>
>       最大熵模型：
>           利用最大熵原理，利用特征函数建立起约束
>           求解模型时使用对偶的方式进行求解，先解最小化的负熵，再求极大化的对偶函数
>           最小化负熵可以得到最大熵模型最后的形式是指数形式，类似逻辑回归
>           在求极大化对偶函数时，发现其对偶函数与模型的极大似然法形式一致
>           最终可以按极大似然法方式去解决模型
>

- **EM：**
>       EM需要解决的问题：
>           在没有隐变量时，最常用的方法是极大化模型分布的对数似然函数。对于富含因变量的极值问题，无法直接计算似然函数，EM算法解决这个问题的思路就是启发式的迭代方法。
>       EM思想：
>           1、先假设隐变量（E）
>           2、然后基于观测数据和隐变量一起来极大化对数似然，然后更新隐变量和模型参数（M）
>           然后继续1、2步迭代，知道模型分布参数基本无变化，算法收敛
>       与EM算法类似的算法Kmeans：
>           1、假设k个初始化质心，即E步
>           2、根据假设的质心，计算得到每个样本的质心，并把样本聚类到最近的质心，然后更新新的质心，即M步
>           然后继续1、2步迭代，直到模型的质心基本无变化，算法收敛
>

- **EM思考：**
>       EM算法可以保证收敛到一个稳定点，但是却不能保证收敛到全局的极大值点，因此它是局部最优的算法。
>           当然，如果我们的优化目标L(θ,θj)是凸的，则EM算法可以保证收敛到全局最大值，这点和梯度下降法这样的迭代算法相同。
>
>       EM的idea：
>           在EM中，已知的是观测数据，未知的是隐变量和模型参数。
>               在E步，固定模型参数，优化隐变量的分布
>               在M步，固定隐变量分布，优化模型参数
>           在其他的优化算法中，很多都有类似的思想，如：SMO、坐标下降法等
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>

- **待续：**
>       参考：<<统计学习方法>> 李航
>           https://blog.csdn.net/itplus/article/details/26549871    最大熵学习笔记（一）预备知识
>           https://wanghuaishi.wordpress.com/2017/02/21/%E5%9B%BE%E8%A7%A3%E6%9C%80%E5%A4%A7%E7%86%B5%E5%8E%9F%E7%90%86%EF%BC%88the-maximum-entropy-principle%EF%BC%89/    图解最大熵原理
>           http://www.devtalking.com/articles/machine-learning-15/    机器学习笔记十五之决策树、信息熵（熵的计算）
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
