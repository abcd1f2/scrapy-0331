### NLP-HanLP -- 损失函数
- **概述**：
>       损失函数：
>           MSE(mean squared error)：
>               loss = 1/2m * ∑(yi-yi')^2
>               逻辑回归问题中，常常使用MSE作为loss函数
>
>           交叉熵：
>               评估label和predicts之间的差距
>               在ml中，p往往表示样本的真实分布，q表示模型所预测的分布，交叉熵越小，表示q分布和p分布越接近
>               单分类：
>                   loss = -∑yi*log(yi')
>                   在单类别（0-1分类）分类问题上，常常使用交叉熵作为loss函数
>               多分类：
>                   多类别预测问题中，预测值不再是通过softmax计算了，这里采用的是sigmoid。
>                   将每一个节点的输出归一化到[0,1]之间，所有的prediction的和不再为1。即每一个Label都是独立分布的，相互之间没有影响。
>                   所以交叉熵在这里是单独对每一个节点进行计算，每一个节点只有两种可能值，所以是一个二项分布。
>                   二项分布的熵的计算简化为：loss=−ylog(y^)−(1−y)log(1−y^)
>
>
>
>
>
>
>
>
>
>
>
>

- **待续：**
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
