### 哈工大NLP -- 分词和频度统计
- **概述：**：
>       规则与统计相结合
>
>
>
>
>
>

- **分词：**
>       语言的分类：
>           孤立语（汉语为代表）
>               (没有专门表示语义的附加成分)
>               语法关系主要靠词序和虚词来表示
>           黏着语（日语为代表）
>               有专门的表示语法意义的附加成分
>           曲折语（英语为代表）
>               用词的形态变化表示语法关系
>
>       汉语有五级语法单位：
>           语素、词、短语、句子、句群
>
>       形成词的要素：
>           结合紧密，使用频繁
>
>       分词：
>           把没有明显分词标志的字串切分成词串
>           1、根据分词规范建立机器词典
>           2、根据算法和机器词典将字串切成词典
>
>       难点：
>           歧义
>               交集型切分歧义（如：结合成分子）
>               覆盖型切分歧义（如：美/女/运动员）
>
>               真歧义：多种歧义分法都是正确的
>               伪歧义：只有一种正确的切分方法
>
>               蛋鸡问题（先有蛋还是鸡的问题）：
>                   根据前后的综合信息排除歧义
>
>               排除歧义规则：
>                   1、如果交段与其后继字串组成名词，则将该歧义词首字单切，否者歧义词为词
>                   2、如果歧义字段的直接前驱字串是数词，则歧义字段的首段单切
>                   3、使用句法、语义、语用相关信息等处理歧义
>
>               歧义切分字段在汉语书面文本中所占比例并不大，在实际书面文本中，特别是在新闻类文本中，未登录词是一个突出的难点
>
>           未登录词（分词的真正的挑战）：
>               最大熵和隐马模型是比较经典和常用的方法
>
>       主要分词方法：
>           正向最大匹配法：
>               （最早、效率比较高、精度不是很高(80~90)，类似于局部最优）
>               （类似于人工智能的爬山法）
>           逆向最大匹配法：
>               （处理精度(90)优于正向匹配）
>           双向匹配法：
>               （处理时间稍微长一点）
>           最少分词算法：
>               （哈工大的成果）
>               步骤：
>                   分段
>                   逐段计算最短路径
>                   统计排歧
>           词网格算法：
>               （精度较高）（组合爆炸问题）
>               全切分 --> 字节点 --> 词节点 --> 计算最优路径的组合
>
>

- **语料库：**
>       语料库：
>           平衡语料库：
>               （内容平衡）
>           生语料和熟语料：
>               熟语料：加入了语法、结构、语义等信息的语料
>               生语料：未进行处理过的语料
>           共时语料库和历时语料库：
>               共时：某一段时期的语料库
>               历时：很长历史发展的语料库
>           单语和双语语料库：
>               双语：如统计机器翻译
>
>           宾夕法尼亚大学的中文语料库是国际认可的中文词性标注和语义标注语料库
>
>

- **频度统计：**
>       词频统计：
>           ngram的n元词序列，ngram语言模型
>           收录词的重要依据
>           比较语言学的重要依据
>
>       体现一个作家的风格：虚词的使用
>       李贤平教授使用文本聚类分析法：将红楼梦120回分成前80回和后40回两类
>
>       统计分布规律：
>           高频词多为虚词，低频词多为实词
>           Zipf规律：频度正比于频序的倒数，即f正比于1/r
>
>       语料库达到多大规模，才能达到每个词在词表中至少出现一次（比如设计倒排索引的大小）
>
>       heap's law：
>           反应词表长度与语料库规模关系：
>               V=Kn^β，β∈(0,1)，K=(10~100),β=(0.4~0.6)
>
>       语言统计学的三大定律：
>           Zipf规律
>           heap's law
>
>

- **待续：**
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
