### NLP_总结 -- 激活函数和损失函数
- **概述**：
>       激活函数作用：
>           1、加入了非线性因素
>           2、充分组合特征
>
>       损失函数：
>           损失函数度量模型一次预测的好坏
>
>[优化算法详情](https://github.com/nwaiting/wolf-ai/blob/master/wolf_nlp/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/NLP-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/20180824-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95.md)
>
>

- **激活函数：**
>       1、sigmoid
>       2、softmax
>       3、tanh
>       4、rectified linear
>       5、ReLU
>       6、LeakyReLU
>       7、pReLU
>       8、ELU
>       9、maxout
>
>
>

- **损失函数：**
>       常见的损失函数：
>           softmax、logistic loss、hinge loss（用于最大化间隔分类器中）、
>       1、二次损失函数
>           如果神经元的房产是线性的，则用二次cost函数不会有学习慢的问题
>       2、交叉熵损失函数
>           cross-entropy cost几乎总是比二次cost好
>       3、对数似然函数
>
>       统计机器学习常见的损失函数：
>           1、0-1损失函数
>               L(y,f(x)) = 0, y != f(x)
>                           1, y = f(x)
>           2、平方损失函数
>               L(y,f(x)) = (y-f(x))^2
>           3、绝对值损失函数
>               L(y,f(x)) = |y-f(x)|
>           4、对数似然损失函数
>               L(y,p(y|x)) = -logP(y|x)
>
>

- **常见问题：**
>
>       Adam相对于SGD 增加了多少量级的参数？
>
>
>
>

- **待续：**
>       参考：https://nlpcs.com/category/NLP
>           https://www.cnblogs.com/makefile/p/loss-function.html   损失函数
>           https://zhuanlan.zhihu.com/p/32230623   一个框架看懂优化算法之异同 SGD/AdaGrad/Adam
>           https://zhuanlan.zhihu.com/p/46341693       深度学习常见优化算法总结
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
