### NLP_总结 -- 文本特征
- **概述**：
>       特征工程的核心就是尽量从多个角度和纬度来捕捉数据中的模式，并用数值特征来加以刻画。
>       对于高度抽象的文本数据，深度学习取得效果有限，实际的产品中，往往会加入一些传统的基于统计学习的自然语言技术。！！！
>
>       **工业界文本特征处理现在通常的做法：**
>           把深度学习模型作为系统的一个子模块（也是一维特征）
>           一些传统的基于统计的自然语言技术的特征
>           一些针对具体任务本身专门设计的特征
>           把上面所有的特征一起作为一个或多个模型(即模型集成)的输入，最终构成一个文本处理系统
>
>
>       **文本分类的核心就是如何从文本中抽取出能够体现文本特点的关键特征，抓取特征到类别之间的映射关系。**
>
>
>

- **文本分类工业界常用到的特征：**
>       大概可以分为4类特征：
>           1、Rule-based feature
>               针对任务本身设计的规则特征
>               如情感分析中positive/nagetive word出现次数
>
>           2、Deep Learning feature
>               基于NN model抽取的特征
>               CNN/RNN/GRU/LSTM （char/word base）
>
>           3、Semantic feature
>               基于embedding表示的特征
>               word2vec、Glove、NN last layer、LDA、twitterLDA
>
>           4、Lexical feature
>               基于BagOfWords抽取的特征
>               char/word/phrase level (ngram)
>               statistic level (word count)
>               TF-IDF
>               点间互信息PMI（可以应用在情感分析中）
>               BM25（搜索中有用到）
>
>
>
>
>
>
>
>
>

- **BOW特征：**
>       词袋模型
>           1、Naive版本，不考虑词频，出现即为1
>           2、考虑词频，词频越大越重要
>           3、考虑词的重要性ITIDF，TFIDF反映了一种折中的思想，既不是越多越重要，也不是越少越重要（**通常情况下，会采用TFIDF，使用的比较广泛。！！！**）
>           4、还有一些如：chi-square、互信息(MI)、熵等
>       优化：
>           1、Bag-of-N-gram，可以提供更多语言信息
>               词袋会更大，向量会更稀疏，n越大，信息越丰富，成本越高。
>
>       **应用总结：！！！**
>           1、通常考虑unigram和bigram来构建词袋模型
>           2、用TF-IDF时，注意对TF作归一化，通常用词频除以文本的长度
>           3、如果构建词袋维数太高，可以用TF（或者TF-IDF）来卡，将一些不常见的词（噪音）过滤掉
>           4、如果有一些先验的词袋，word count通常都是比较强的一维特征（如情感分类）可以考虑
>           5、基于词袋模型通常高维稀疏，通常使用非线性模型取得的效果比线性模型好，可以使用如：GBDT等
>               因为非线性模型能够学习更加复杂的规则，对于文本而言，体现在能够一定程度上考虑出现的语境情况
>
>       缺点：
>           1、丢失文本出现的先后顺序
>           2、没有考虑语义关系，仅使用了词语符号
>

- **word embedding方法：**
>       word2vec基本思想是用词出现的上下文来表示这个词，上下文越接近的词之间的语义相似性越高。
>           （语义接近并不表示含义接近，如"黑色"和"白色"两个词的上下文是相似的，但表达的含义却是相反的）
>       常用的方法：
>           word2vec
>               是predict-based，用一个3层的NN模型来预测词的上下文，词向量是训练过程的中间产物。
>           glove
>               glove是count-based的方法，通过对共现矩阵做降维来获取词的向量
>       两者效果相差不大，glove模型的优势在于矩阵运算可以并行化，能加快速度
>       两种方法思想类似，都是用词的上下文来表示这个词。
>

- **从word embedding到文本特征：**
>       基于embedding的特征刻画的是语义、主题层面上的特征，较词匹配而言，有一定的泛化能力。！！！！！！！
>       怎么从word embedding到文本的embedding呢？
>       对于短文本，比较好的方法有：
>           1、取各个词向量之和（或者取平均）作为文本的向量表示
>           2、用一个pre-train好的NN模型得到文本作为输入的最后一层向量表示
>
>       介绍一个短文本主题刻画的，TwitterLda（https://github.com/minghui/Twitter-LDA），主要针对短文本做主题刻画，实际效果还不错。！！！！
>       CNN和RNN在文本处理中的区别：
>           CNN善于捕捉文本中关键的局部信息
>           RNN善于捕捉文本的上下文信息（考虑语序信息），并且具有一定的记忆能力，两者都可以用在文本分类任务中，效果还不错。
>

- **基于NN Model提取的特征**
>       NN的好处是在于能end2end实现模型的训练和测试，利用模型的非线性和众多参数来学习特征，而不需要手工提取特征。
>       CNN和RNN都是NLP常用的模型，两个模型捕捉的角度不太一样：
>           CNN善于捕捉文本中关键的局部信息
>           RNN善于捕捉文本上下文信息，具有一定的记忆能力
>           总结：两者都可以用在文本分类任务中，而且效果还不错。
>       还有，可以将NN预测的分值作为分类系统的一个特征，来加强分类系统的性能。
>
>

- **基于任务本身抽取的特征：**
>       这种特征主要针对具体任务而设计的，通过对数据的观测，发现一些可能有用的特征。
>       如：正负面评论，包含负面词的数量就是一维很强的特征，有时候有些经验主义的东西很有用。！！！！！！
>
>       这类特征设计就是在拼脑力和拼经验，建议可以多看看各个类别数据找找数据，将那些你直观上感觉对分类有帮助的东西设计成特征，
>           有时候这些经验主义的东西很有用（可能是模型从数据中学不到的）
>

- **特征融合：**
>       高维特征分类：
>           对于特征维数较高、数据模式复杂的情况，建议用非线性模型（如GBDT、XGBoost）
>       低维特征分类：
>           对于特征维数较低、数据模式简单的情况，建议用简单的线性模型（如LR，LR本质上为在线性回归的基础上，套了一个逻辑函数）
>       注意：
>           训练子模型（GBDT/DNN）的训练数据和训练融合模型(LR)的训练数据需要不一样，为了防止子模型因为"见过"训练数据而产生偏向于子模型的情况。
>           实际中，可以用traing数据集作为子模型的训练数据，dev数据集作为最终融合模型的训练数据
>
>       总结：
>           1、融合模型能够从多个角度更加全面的学习出训练数据中的模式，往往比单个模型效果好2~3个点。！！！
>           2、通过观察LR模型给各个特征分配的权重大小和正负，可以看出对于训练数据而言，这些特征影响分类的重要程度，以及特征影响最终分类目标的极性。！！！
>           3、对于手工特征，可以观察手工特征的权重来验证这些特征的有效性和有效程度。
>
>       特征融合的模型框架如下图：
> ![avatar](https://github.com/nwaiting/wolf-ai/blob/master/wolf_others/pic/text_classification.png)
>
>       上图中是一个任务正负面评论分类（负面评论定义是不适合出现在网络上的评论，如政治敏感、人身攻击等），橙色框表示模型，蓝色框表示用到的特征，[]表示特征的维数。
>       注意：训练子模型(GBDT/DNN)的训练数据和训练融合(LR)的训练数据需要不一样。
>               为了防止子模型因为"见过"这些训练数据而产生偏向子模型的情况
>       实际的模型训练中：
>           可以用training数据集作为子模型的训练数据，dev数据集作为最终融合模型的训练数据
>
>

- **过滤清洗特征：**
>       分类和检索通常不需要对文本有深入的理解。
>       1、停用词
>           停用词表是一种去除空洞特征常用词的方法
>       2、基于频率过滤
>       3、基于频率的过滤和停用词列表结合起来
>       4、基于词长度过滤，单字词、数字等过滤
>           如一般单字词很多都没有明显特征
>

- **稀有词：**
>       重尾分布：160 万个Yelp评论包含 357481个独特单词,189915只出现在一次评论中,41162次出现在两次评论中。超过60%的词汇很少发生
>
>

- **文本特征总结：**
>       上面介绍了一个对于文本分类任务设计特征的思路，对于其他的NLP任务，总的思路都是相似的。
>       总的来说，特征工程的核心是尽量从多个角度和纬度来捕捉数据中的模式，并用数值特征来加以刻画。
>
>       中文文本分类中，有两个比较成熟的结论：
>           1、当特征数达到1万维的时候，采用分词后的词作为特征还是采用字的bigram作为特征，效果相近。
>               如果有比较靠谱的分词系统，就是分词作为特征，如果没有，就用bigram作为特征
>           2、特征选择的最好方法是Chi-Square，其基本思想是在标注了类别的文本数据上计算每个候选特征与类别之间的关系，选取每个类别最具有代表性的特征。
>

- **待续：**
>       参考：http://www.jeyzhang.com/text-classification-in-action.html
>       TwitterLda（https://github.com/minghui/Twitter-LDA），主要针对短文本做主题刻画，实际效果还不错。！！！！
>       https://www.zhihu.com/question/21374143     如何抽取文章特征？
>       https://www.quora.com/How-is-GloVe-different-from-word2vec  How is GloVe different from word2vec?
>
>
>
>
>
>
>
>
>
>
>
