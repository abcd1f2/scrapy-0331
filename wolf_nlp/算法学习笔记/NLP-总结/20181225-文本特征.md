### NLP_总结 -- 文本特征
- **概述**：
>       特征工程的核心就是尽量从多个角度和纬度来捕捉数据中的模式，并用数值特征来加以刻画。
>       对于高度抽象的文本数据，深度学习取得效果有限，实际的产品中，往往会加入一些传统的基于统计学习的自然语言技术。！！！
>
>
>

- **BOW特征：**
>       词袋模型
>           1、Naive版本，不考虑词频，出现即为1
>           2、考虑词频，词频越大越重要
>           3、考虑词的重要性ITIDF，TFIDF反映了一种折中的思想，既不是越多越重要，也不是越少越重要（通常情况下，会采用TFIDF，使用的比较广泛）
>           4、还有一些如：chi-square、互信息(MI)、熵等
>       优化：
>           1、Bag-of-N-gram，可以提供更多语言信息
>               词袋会更大，向量会更稀疏，n越大，信息越丰富，成本越高。
>
>       应用总结：
>           1、通常考虑unigram和bigram来构建词袋模型
>           2、用TF-IDF时，注意对TF作归一化，通常用词频除以文本的长度
>           3、如果构建词袋维数太高，可以用TF（或者TF-IDF）来卡，将一些不常见的词（噪音）过滤掉
>           4、如果有一些先验的词袋，word count通常都是比较强的一维特征（如情感分类）可以考虑
>           5、基于词袋模型通常高维稀疏，通常使用非线性模型取得的效果比线性模型好，可以使用如：GBDT等
>               因为非线性模型能够学习更加复杂的规则，对于文本而言，体现在能够一定程度上考虑出现的语境情况
>
>       缺点：
>           1、丢失文本出现的先后顺序
>           2、没有考虑语义关系，仅使用了词语符号
>

- **word embedding方法：**
>       常用的方法：
>           word2vec
>               是predict-based，用一个3层的NN模型来预测词的上下文，词向量是训练过程的中间产物。
>           glove
>               glove是count-based的方法，通过对共现矩阵做降维来获取词的向量
>       两者效果相差不大，glove模型的优势在于矩阵运算可以并行化，能加快速度
>       两种方法思想类似，都是用词的上下文来表示这个词。
>

- **从word embedding到文本特征：**
>       基于embedding的特征刻画的是语义、主题层面上的特征，较词匹配而言，有一定的泛化能力。！！！！！！！
>       怎么从word embedding到文本的embedding呢？
>       对于短文本，比较好的方法有：
>           1、取各个词向量之和（或者取平均）作为文本的向量表示
>           2、用一个pre-train好的NN模型得到文本作为输入的最后一层向量表示
>
>       介绍一个短文本主题刻画的，TwitterLda（https://github.com/minghui/Twitter-LDA），主要针对短文本做主题刻画，实际效果还不错。！！！！
>       CNN和RNN在文本处理中的区别：
>           CNN善于捕捉文本中关键的局部信息
>           RNN善于捕捉文本的上下文信息（考虑语序信息），并且具有一定的记忆能力，两者都可以用在文本分类任务中，效果还不错。
>

- **基于任务本身抽取的特征：**
>       这种特征主要针对具体任务而设计的，通过对数据的观测，发现一些可能有用的特征。
>       如：正负面评论，包含负面词的数量就是一维很强的特征，有时候有些经验主义的东西很有用。！！！！！！
>

- **特征融合：**
>       对于特征维数较高、数据模式复杂的情况，建议用非线性模型（如GBDT、XGBoost）
>       对于特征维数较低、数据模式简单的情况，建议用简单的线性模型（如LR）
>       注意：
>           训练子模型（GBDT/DNN）的训练数据和训练融合模型(LR)的训练数据需要不一样，为了防止子模型因为"见过"训练数据而产生偏向于子模型的情况。
>           实际中，可以用traing数据集作为子模型的训练数据，dev数据集作为最终融合模型的训练数据
>
>       总结：
>           融合模型能够从多个角度更加全面的学习出训练数据中的模式，往往比单个模型效果好2~3个点。！！！
>
>       通过观察LR模型给各个特征分配的权重大小和正负，可以看出对于训练数据而言，这些特征影响分类的重要程度，以及特征影响最终分类目标的极性。！！！
>       对于手工特征，可以观察手工特征的权重来验证这些特征的有效性和有效程度。
>
>

- **过滤清洗特征：**
>       分类和检索通常不需要对文本有深入的理解。
>       1、停用词
>           停用词表是一种去除空洞特征常用词的方法
>       2、基于频率过滤
>       3、基于频率的过滤和停用词列表结合起来
>

- **稀有词：**
>       重尾分布：160 万个Yelp评论包含 357481个独特单词,189915只出现在一次评论中,41162次出现在两次评论中。超过60%的词汇很少发生
>
>

- **待续：**
>       参考：http://www.jeyzhang.com/text-classification-in-action.html
>       TwitterLda（https://github.com/minghui/Twitter-LDA），主要针对短文本做主题刻画，实际效果还不错。！！！！
>
>
>
>
>
>
>
>
>
>
>
>
>
