### NLP_总结 -- 模型评估
- **概述**：
>       衡量一个语言模型的质量，最好的方法就是将其应用到具体的问题中，如：机器翻译、语音识别、拼写纠错等，观测模型在任务中的表现。！！！！
>       但是这种方法有两个缺点：
>           1、难以操作
>           2、验证耗时
>       因此，需要找到一种固有的评估方法，即根据语言模型自身的特性，设计的一种有效的评估指标，即Perplexity（困惑度）。
>       语言模型评估时，我们可以用Perplexity大致估计训练效果，作为判断和分析，但是它不是完全意义上的标准。！！！！！！
>
>       Perplexity（困惑度）是用来衡量一个概率分布或概率模型预测样本的好坏程度。
>           也可以用来比较两个概率分布或概率模型，低困惑度的概率分布模型能更好的预测样本
>
>       三种困惑度：
>           1、概率分布的困惑度
>           2、概率模型的困惑度
>           3、每个分词的困惑度
>

- **困惑度：**
>       Perplexity是一种衡量自然语言处理领域中，语言模型的指标。
>       Perplexity就是对于语言模型所估计的一句话出现的概率
>       Perplexity越小越好，Perplexity越小，表明句子出现的概率越大。
>       Perplexity的直观理解：
>           如在0-9这10个随机数组成的序列中，由于随机出现，所以每个位置出现的概率为1/10，
>           即在每个点都有10个等概率的候选答案供我们选择，于是我们的Perplexity为10（有10个合理的答案）
>           如果在一个语言模型报告其Perplexity为110，那么可以理解为平均情况下，这个模型预测下一词语时，有110个词等可能作为下一个词的合理选择
>
>       Perplexity有哪些特点：
>           1、训练集越大，Perplexity会下降的更低，10亿和10万数据集训练效果是不一样的
>           2、数据中的标点会对模型的Perplexity产生很大的影响，一个句号能让Perplexity波动很大，标点的预测总是不稳定
>           3、预测句子中的"的""了"等词对Perplexity有很大影响。
>
>
>
>
>
>
>
>

- **待续：**
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
