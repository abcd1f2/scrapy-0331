## NLP_总结 -- 相关知识点
>       LSTM
>       RNN
>       BP
>       牛顿法和梯度下降法区别
>       激活函数，损失函数
>       SGD、BGD、Adam区别
>       CNN（了解）
>       如何进行分词，算法原理，分词的几种方法，正向最大匹配法，逆向最大匹配法，分词的处理有没有用到跳词？
>       word2vector详细原理（输入是什么，输出是什么，每个门的输入输出是什么）
>       LR损失函数，如何分类，什么情况下用LR（梯度），如何多分类，数据成什么分布，
>               答：属于某一类事件的概率，
>           loss函数、似然函数、求对数怎么求最优，优化方法
>               答：cost函数：h(x) = 1/(1+e^(θ*X))，似然函数：L(θ)=-yilog(h(x))-(1-yi)log(1-h(x))
>           交叉熵物理含义，softmax物理含义
>           逻辑回归损失函数中为什么要除m样本数
>               答：（我们一般把求解极大值问题转换成求解极小值问题，所以这里我们给l(θ)乘以-1/m，转换成求最小值问题，为什么是1/m,方面后面求导后直接约掉）
>       新闻多分类系统中，如何预处理，如何多分类（去停用词，特征维数选择）
>           答：最后使用LR进行分类
>       GBDT和lightGBM、XGBoost区别，和LR区别  （列抽样、并行化、优化方法，XGBoost还支持线性分类器，用的不都是CART树）
>           XGBoost工程上的改进优化
>           bagging和boost
>       树：ID3和C4.5比较，信息熵、条件熵、信息增益、信息增益率  树模型不适合ont hot编码
>           树模型要不要做归一化
>       AUC是什么
>       L1、L2正则化，怎么挑选特征，L2对于那些特征作用小的系数就很小么？L1为何能选择稀疏矩阵？
>           什么时候用L1，什么时候用L2
>           用L2正则化训练起来很慢的、要实时预测的，L1正则化产出的模型，只需要存储不为0的特征，因此会节省很多空间，同时计算更高效
>               1、内置特征选择是L1范数被经常提及的有用的性质
>               2、稀疏性指的是一个矩阵（或向量）中只有少数的项是非零的。L1范数具备性质：产生许多0或非常小的系数和少量大的系数
>               3、L1范数没有一个解析解，但是L2范数有。这就允许L2范数在计算上能高效地计算。然而，L1范数的解具备稀疏性，这就允许它可以使用稀疏算法，以使得计算更加高效
>       PCA原理，与SVD的区别
>       两个正太分布相加或相乘还是正太分布吗？
            只有相互独立的正态分布加减之后，才是正态分布，相乘后应该就不再是正态分布了。
>       EM算法：E-step和M-step具体做什么
>
>
>
>       在无序数组中查找第K大的数，怎么最快
>       快排：递归和非递归实现，最坏时间复杂度，什么情况下产生，如何避免
>           当N很小时，快排速度慢，归并快；当N很大，无序时，堆排序快，有序时，快排排序快
>       动态规划：背包问题（价值最大），最小编辑距离
>       链表找公共节点，链表找环
>       topK要求空间复杂度为O(1)
>       如何查找一个文件夹下所有文件的大小和，用什么数据结构（树，后序遍历节约时间）
>       算法：如何找到二叉树每一层的最大值
>       Java中的ArrayList是链表还是数组，为什么插入和删除可以O(1)
>       升序数组，旋转一下，在旋转数组中查找有没有目标数字，二分法当时又写错了一点
>       给定一个字符串，给定一个很多子串的集合，求满足能完整无重叠的拼成字符串的所有集合
>       有序数组中查找重复的数（可以二分查找）
>
>

### 机器学习算法相关
- **EM算法：**
>       进行是进行参数估计，因为是无监督，一般应用在聚类上，也用在HMM参数估计上。凡是有EM算法的，一定是无监督学习，因为EM是对参数聚集。
>           无隐变量的最大似然，求导取极值
>           含隐变量的联合概率p(x,z)，其中z为隐变量
>               含有隐变量的最大似然估计，EM是一种存在隐含变量优化问题的有效方法。
>       算法一直在E步、M步迭代，更新隐变量数据和模型分布参数，直到收敛，即得到需要的模型参数。
>       E步（求期望Expectation和最大化Maximization）
>       1、E步骤（建立下边界，估计隐含变量）
>           计算联合分布的条件概率期望
>           无法直接求模型分布参数，先猜想隐含数据（隐变量），然后根据现在模型继续猜测隐含数据
>       2、M步骤（优化下边界，估计其他参数）
>           根据Jensen不等式，得出下边界Q函数，极大化Q函数
>           基于观察数据和猜测的隐含数据极大化对数似然，然后继续极大化对数似然
>       GMM和K-means比较：
>           都是用于聚类的算法，都需要指定K值
>           区别：对于GMM来说，引入了概率，GMM可以给出一个样本属于某类的概率是多少，所以GMM可以做聚类，也可以做概率密度估计
>

- **word2vec原理：**
>       参考：https://blog.csdn.net/mytestmy/article/details/26961315  深度学习word2vec笔记之基础篇
>            https://www.zhihu.com/question/21661274/answer/19331979
>
>
>
>
>
>
>
>

- **待续：**
>       参考：https://developer.aliyun.com/article/716005?utm_content=g_1000073790  150+面试题，十大必读书，数据挖掘offer轻松搞定 | 面试宝典系列
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
