### nlp-常用算法和模型
- **概述：**
>       常用模型算法：
>           HMM
>           CRF
>           EM
>           MaxEnt
>           MEMM（最大熵马尔科夫模型）
>           LDA
>
>       深度学习方法：
>           RNN
>           GRU
>           LSTM
>           attention
>           seq2seq
>
>
> ![avatar](https://github.com/nwaiting/wolf-ai/blob/master/wolf_others/pic/nb_hmm_crf.png)
>

- **EM：**
>       EM算法研究起源于统计学的误差分析问题
>       EM算法那是基于极大似然估计理论的优化算法。是一种通过迭代进行极大似然估计的优化算法，
>           通常作为牛顿迭代法的替代用于对包含隐变量或缺失数据的概率模型进行参数估计。
>       一般在做nlp的序列标注等任务，在训练阶段肯定是有隐状态序列。所以极大似然估计法是非常常用的学习方法。
>

- **HMM：**
>       学习方法为：极大似然估计
>       优化方法为：Baum-Welch算法，就是一个EM过程，反复迭代
>

- **CRF：**
>       CRF实际上是定义在时序数据上的对数线性模型，
>           学习方法为：极大似然估计和正则化的极大似然估计
>           优化方法为：梯度下降、拟牛顿法、改进尺度方法IIS
>

- **MaxEnt：**
>       MaxEnt
>           学习方法为：
>               分类模型为条件概率分布p(y|x)，在满足约束条件的前提下使得模型的熵最大，即max H(p(y|x))
>           优化方法为：由于是一个光滑的凸函数，因此可以用GIS、IIS、梯度下降法、拟牛顿法
>               （前两种方法是专门为最大熵模型设计，后两种是通用的优化算法）
>           缺点：由于约束函数数量和样本数有关系，导致迭代过程计算量巨大，实际应用比较困难
>

- **MEMM：**
>       最大熵马尔科夫模型：
>           学习方法：
>           优化方法：
>       与HMM、CRF的比较：
>           注：O观测集合、S状态集合、M模型
>           HMM：给定O，找到M，使得P(O|M)最大（由于S未知，所以使用EM算法）
>               缺点：输出独立性假设，不能考虑上下文，限制了特征的选择
>           MEMM：给定O和S，找到M，使得P(O|S,M)最大（可以通过极大释然估计）
>               缺点：可以任意选择特征，由于其在每一节点都要进行归一化，所以只能找到局部最优值
>           CRF：所有特征进行全局归一化，可以找到全局最优值
>
>
>
>
>
>
>
>
>
>
>

- **待续：**
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
