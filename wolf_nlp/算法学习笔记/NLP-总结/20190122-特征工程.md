### NLP_总结 -- 特征工程
- **概述**：
>
>       特征工程大体有下面几步：
>           1、数据的准备过程
>           2、构造特征
>           3、特征选择
>           4、特征降维
>
>       特征工程的两个重要问题：
>           1、特征选择
>           2、特征提取
>
>
>       数据特征处理，见下图，
> ![avatar](https://github.com/nwaiting/wolf-ai/blob/master/wolf_others/ml_pic/ml_feature_selection.jpg)
>
>
>

- **数据归一化和标准化的区别：**
>       归一化：
>           对不同特征维度的伸缩变换的目的是使各个特征维度对目标函数的影响权重是一致的，即使得那些扁平分布的数据伸缩变换成类圆形，这也就改变了原始数据的一个分布。
>           好处：
>               1、提高迭代收敛速度
>               2、提高迭代求解精度
>           虽然这样样本会失去原始的信息，但这防止了归一化前直接对原始数据进行梯度下降类似的优化算法时最终解被数值大的特征所主导。
>               归一化之后，各个特征对目标函数的影响权重是一致的。这样的好处是在提高迭代求解的精度。
>
>           x = (x - min) / (max - min)
>               这种归一化方法比较适用在数值比较集中的情况。这种方法有个缺陷，如果max和min不稳定，很容易使得归一化结果不稳定，使得后续使用效果也不稳定。
>                   实际使用中可以用经验常量值来替代max和min
>
>       标准化：
>           对不同特征维度的伸缩变换的目的是使得不同度量之间的特征具有可比性，同时不改变原始数据的分布。
>           好处：
>               1、使得不同度量之间的特征具有可比性，对目标函数的影响体现在几何分布上，而不是数值上
>               2、不改变原始数据的分布
>
>           做标准化有什么注意事项吗?
>               最大的注意事项就是先拆分出test集，不要在整个数据集上做标准化，因为那样会将test集的信息引入到训练集中，这是一个非常容易犯的错误!
>

- **数据归一化：**
>       数据归一化：
>           一般来自各个维度的数据，就是说的统计口径不一样，造成的结果是得到的数据大小范围变换非常大，并且可能数据类型也不一样，
>           统计学里把数据分为数值型数据、分类型数据、顺序型数据，对这些数据处理成统一口径的问题，就是机器学习中数据归一化的问题。
>
>       哪些模型需要归一化？？？
>           需要归一化：
>               有一个准则，就是需要归一化的模型，说明该模型关心变量的值
>           不需要归一化：
>               相对于概率模型来说，关心的是变量的分布和变量之间的条件概率，所以大部分概率模型不需要归一化。
>               如果模型使用梯度下降法求最优解时，归一化往往是非常有必要的，否则很难收敛甚至不能收敛
>

- **哪些模型需要做特征的归一化：**
>       概率模型(树形模型)不需要归一化：
>           因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、RF、XGboost
>       模型特征需要归一化：
>           如果模型使用梯度下降法求最优解时，归一化往往是非常有必要的，否则很难收敛甚至不能收敛
>           Adaboost、SVM、LR、KNN、KMeans等最优化问题需要归一化
>

- **模型类型：**
>       模型类型：
>           广义线性模型
>               线性回归
>               岭回归
>               Lasso回归
>               最小角回归
>               逻辑回归
>               贝叶斯回归
>               多项式回归
>               Elastic Net
>           集成模型
>               RF
>               Adaboost
>               梯度提升树
>           线性判别分析
>           SVM
>           K近邻
>           朴素贝叶斯
>           决策树
>           感知机
>           神经网络
>
>

- **数据的准备：**
>
>       数据准备过程包括：
>           1、数据采集
>               略
>           2、数据清洗
>               去除脏数据，对有问题的数据进行预处理
>               常用的异常点检测算法包括：
>                   (1)、偏差检测：
>                       聚类
>                       最近邻
>                   (2)、基于统计的异常点检测：
>                       极差
>                       四分位数间距
>                       均差
>                       标准差
>                       这种方法适合于挖掘单变量的数值型数据
>                   (3)、基于距离的异常点检测：
>                       主要通过距离方法来检测异常点
>                       曼哈顿距离
>                       欧氏距离
>                       冯氏距离
>                   (4)、基于密度的异常点检测：
>                       考察当前点周围密度，可以发现局部异常点
>                       LOF算法
>           3、数据采样
>               如正负样本不均衡，需要进行数据采样
>               采样方法有：
>                   随机采样
>                   分层抽样（更多的是根据特征采用分层抽样）
>
>
>

- **特征构造：**
>
>       从原始数据中构造特征集，主要的构造特征方法如下：
>           1、对数值型特征
>               (1)、无量纲化
>                   对于非tree base的模型，对数据的规格要求是很高的，这就需要无量纲化各个维度的数据
>                   常用方法：
>                       归一化
>                       最大最小区间缩放
>                       正则归一化
>               (2)、统计值
>                   包括max、min、mean、std等
>                   对数据计算的统计值也可以作为特征
>               (3)、离散化
>                   就是分箱/分区操作，将连续值转成非连续数据。常用对定理特征进行二值化。（Binarizer类工具）
>           2、对类别型特征
>               常用的方法是one-hot编码（OneHotEncoder类工具）
>           3、对时间戳特征
>               (1)、day of weak（一周的星期几）、
>                   day of month、
>                   day of year、
>                   week of year、
>                   month of year、
>                   hour of day、
>                   minute of day、
>                   哪个季度
>               (2)、t_m24前一天的数值、
>                   t_m48前两天的数值
>               (3)、tdif（与前一天的数值的差值等）
>           4、对文本数据特征
>               (1)、bag of word
>               (2)、TF-IDF
>           5、计算统计特征
>               统计内容包括：加减平均、分位线、次序型、比例类等
>           6、组合特征
>               (1)、拼接型
>                   简单的组合特征，就是把多个特征组合成一个
>               (2)、模型特征组合
>                   用GBDT产出特征组合路径。组合特征和原始特征一起放到LR中进行训练
>           7、缺失特征处理
>               对缺失的特征可以修改成新的特征，也可以删除。
>               (1)、删除
>                   如果大部分样本该属性都缺失，那么这个属性提供的信息有限，可以选择删除
>                   如果一个样本大部分属性，可以选择删除该样本
>                   这种删除只适用于数据集中缺失较少的情况
>               (2)、统计填充
>                   可以使用平均数、中位数、众数、最大值、最小值等
>               (3)、统一填充
>                   对于含缺失值的属性，把所有缺失值统一填充为自定义值。
>                   如果有可用类别信息，也可以为不同类别分别进行统一填充。常用的统一填充值有："空"、"0"、"正或负无穷"
>               (4)、预测填充
>                   可以通过预测模型利用不存在缺失值的属性来预测缺失值，先用预测模型把数据填充后再做进一步的分析，如统计、学习等。
>                   **虽然这种方法比较复杂，但是最后得到的结果比较好**。！！！
>           8、特征转换
>               对已有的特征做变换，产生新的特征。
>               常见的数据变换有：
>                   (1)、基于多项式的
>                   (2)、基于指数函数的
>                   (3)、基于对数函数的
>
>

- **特征选择：**
>
>       当数据预处理完成后，需要选择有意义的特征进行训练。原始的特征可能有冗余（两个特征是表示一个问题，相关性太强）、噪声（影响效果）
>       通常从两个方面考虑选择特征：
>           1、特征是否发散
>               如果一个特征不发散，方差接近于0，就是说样本在这个特征上基本没差异，这个特征对于样本的区分没有什么用
>           2、特征与目标的相关性
>               这个比较明显，除了方差方法外，还有以下方法：
>                   a、过滤方法
>                       评估单个特征与结果之间的相关程度，留下topk相关的特征。
>                       评价方式：Pearson相关系数、互信息、距离相关度
>                       缺点：只评估了单个特征对结果的影响，没有考虑特征之间的关联作用，有可能将有用的关联特征踢掉。**工业界使用较少**。！！！
>                       (1)、方差选择方法
>                           计算各个特征的方差，选择方差大于阈值的特征
>                       (2)、相关系数方法
>                           计算各个特征对目标值的相关系数以及相关系数的P值。可以用sklearn的feature_selection的SelectBest类结合相关系数来选择
>                           卡方检验是检验定性自变量对定性因变量的相关性；互信息是评价定性自变量对定性因变量的相关性，这两个都可以用sklearn处理
>                   b、包裹方法
>                       把特征选择看做一个特征子集搜索问题，筛选各种特征子集，用模型评估子集特征的效果。
>                       典型算法："递归特征删除算法"，应用在逻辑回归的过程：
>                           (1)、用全量跑一个模型
>                           (2)、根据线性模型的系数（体现相关性），删除弱特征，观察准确率的变化
>                           (3)、逐步进行，直到准确率出现大的下滑停止
>                   c、嵌入方法
>                       根据模型来分析特征的重要性，最常用的方法为用正则化方式来做特征选择。（**这种方法在工业界很常用**）！！！
>                       (1)、基于惩罚项的方法
>                           就是用L1、L2来正则化做特征选择
>                           L1有截断效应：不重要的参数权重为0
>                           L2有缩放效应：拿到手的特征都比较小。可以用SelectFromModel工具
>                       (2)、基于树模型的特征选择方法
>                           树模型中的GBDT也可用来作为基模型进行特征选择。feature_selection库中的SelectFromModel类结合了GBDT模型
>

- **特征降维：**
>
>       常见的特征降维方法如下：
>           1、L1惩罚项
>           2、PCA
>               PCA是为了让映射后的样本具有更大的发散性
>               是一种无监督学习
>           3、线性判别分析LDA
>               LDA是为了让映射后的样本有最好的分类性能
>               是一种有监督学习
>

- **为什么需要特征数据归一化：**
>
>       为什么要对数据进行归一化，维基百科给出解释：
>           1、数据归一化后加快了梯度下降求最优解的速度
>               数据归一化后，特征的等高线显得很圆，不是很尖锐，在梯度下降时候能较快收敛。
>           2、归一化有可能提高精度
>               有些模型对数据数值很敏感。比如KNN，这种需要归一化
>

- **数据标准化与归一化类型：**
>
>       数据归一化类型常见的有：
>           1、线性归一化
>               公式为：x' = (x - min(x))/(max(x) - min(x))
>               这种归一化比较适用于数值比较集中的情况。这种方法有个缺陷，如果min和max不稳定，很容易使得归一化结果不稳定，使得后续效果也不稳定。
>                   实际使用中可以用经验常量值来替代max和min
>
>           3、非线性归一化
>               经常用在数据分化比较大的场景，有些数值很大，有些数值很小。
>               可以通过一些数学函数，将原始值进行映射。方法包括：log、指数、正切等，如：x' = log(x) / log(max(x))
>               注意：需要根据数据分布的情况，决定非线性函数的曲线，比如log(V,2)还是log(V,10)等
>
>           2、标准差标准化
>               经过处理的数据符合标准正太分布，即均值μ为0，标准差σ为1，其转化函数为：
>                   x' = (x - μ) / σ
>               μ为所有样本数据的均值，σ为所有样本数据的标准差
>
>
>       标准化与归一化的区别：
>           归一化是为了消除不同数据之间的量纲，方便数据比较和共同处理，如在神经网络中，归一化可以加快神经网络的收敛性；
>           标准化是为了方便数据的下一步处理，而进行的数据缩放等变换，并不是为了方便与其他数据一同处理或比较，
>               如将数据经过零-均值标准化后，更利于使用标准正太分布的性质进行处理。
>               不改变原始数据的分布。
>
>           归一化：
>               1、把数据变成[0,1]或[-1,1]之间
>               2、把有量纲表达式变成无量纲表达式
>               最常用的归一化就是将特征向量调整为L1范数（就是绝对值相加），使特征向量的数值之和为1。L2范数就是欧几里得之和。
>
>           数据标准化：
>               1、把数据按照比例缩放，使之落入一个小的空间里
>               常用方法有：
>                   a、最小-最大规范化（线性变换）
>                   b、z-zero规范化
>                       当X的最大值和最小值未知，或孤立点左右了最大最小规范化时，该方法有效
>                   c、小数定标规范化
>                       通过移动x的小数位置来进行规范化
>                       y=x/(10^j)   (j是使得Max(|y|)<1的最小整数)
>                   d、对数logistic模式
>                       x' = 1/(1 + e^(-x))
>                   e、模糊量化模式
>
>       标准化和归一化如何选择？
>           归一化算法是通过特征的最大最小值将特征缩放到[0,1]区间范围内
>           而多于许多机器学习算法，标准化也许会更好，标准化是通过特征的平均值和标准差将特征缩放成一个标准的正态分布，均值为0，方差为1
>
>

- **哪些模型需要特征归一化？**
>       不需要归一化：
>           当模型不关心变量的值，只关心变量的分布和变量之间的条件概率，有以下模型：
>           决策树（比如ID3、C4.5、CART的熵和基尼系数，研究对象只是概率分布）
>           RF
>
>       需要归一化：
>           下面的模型最优化问题需要特征归一化，如：
>           Adaboost
>           SVM
>           LR
>           KNN
>           KMeans
>
>
>
>


- **待续：**
>       参考：https://aiputing.com/topic/642   机器学习之特征工程
>           https://feisky.xyz/machine-learning/basic/feature-engineering.html  特征工程
>           https://www.zhihu.com/question/29316149     特征工程到底是什么？
>           https://juejin.im/post/5b569edff265da0f7b2f6c65     机器学习之 特征工程
>           https://www.jianshu.com/p/5621a98d598f      机器学习之特征工程
>           https://github.com/apachecn/feature-engineering-for-ml-zh   [译] 面向机器学习的特征工程
>           https://cloud.tencent.com/developer/article/1079043     达观数据分享文本大数据的机器学习自动分类方法
>           https://www.zhihu.com/question/28641663/answer/110165221    机器学习中，有哪些特征选择的工程方法？
>           http://www.raincent.com/content-10-12066-1.html     数据特征的标准化和归一化你了解多少？
>
>
>
>
>
