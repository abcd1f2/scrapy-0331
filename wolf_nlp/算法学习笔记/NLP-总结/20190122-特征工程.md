### NLP_总结 -- 特征工程
- **概述**：
>       特征工程大体有下面几步：
>           1、数据的准备过程
>           2、构造特征
>           3、特征选择
>           4、特征降维
>
>
>

- **数据的准备：**
>       数据准备过程包括：
>           1、数据采集
>               略
>           2、数据清洗
>               去除脏数据，对有问题的数据进行预处理
>               常用的异常点检测算法包括：
>                   (1)、偏差检测：
>                       聚类
>                       最近邻
>                   (2)、基于统计的异常点检测：
>                       极差
>                       四分位数间距
>                       均差
>                       标准差
>                       这种方法适合于挖掘单变量的数值型数据
>                   (3)、基于距离的异常点检测：
>                       主要通过距离方法来检测异常点
>                       曼哈顿距离
>                       欧氏距离
>                       冯氏距离
>                   (4)、基于密度的异常点检测：
>                       考察当前点周围密度，可以发现局部异常点
>                       LOF算法
>           3、数据采样
>               如正负样本不均衡，需要进行数据采样
>               采样方法有：
>                   随机采样
>                   分层抽样（更多的是根据特征采用分层抽样）
>
>
>

- **特征构造：**
>       从原始数据中构造特征集，主要的构造特征方法如下：
>           1、对数值型特征
>               (1)、无量纲化
>                   对于非tree base的模型，对数据的规格要求是很高的，这就需要无量纲化各个维度的数据
>                   常用方法：
>                       归一化
>                       最大最小区间缩放
>                       正则归一化
>               (2)、统计值
>                   包括max、min、mean、std等
>                   对数据计算的统计值也可以作为特征
>               (3)、离散化
>                   就是分箱/分区操作，将连续值转成非连续数据。常用对定理特征进行二值化。（Binarizer类工具）
>           2、对类别型特征
>               常用的方法是one-hot编码（OneHotEncoder类工具）
>           3、对时间戳特征
>               (1)、day of weak（一周的星期几）、
>                   day of month、
>                   day of year、
>                   week of year、
>                   month of year、
>                   hour of day、
>                   minute of day、
>                   哪个季度
>               (2)、t_m24前一天的数值、
>                   t_m48前两天的数值
>               (3)、tdif（与前一天的数值的差值等）
>           4、对文本数据特征
>               (1)、bag of word
>               (2)、TF-IDF
>           5、计算统计特征
>               统计内容包括：加减平均、分位线、次序型、比例类等
>           6、组合特征
>               (1)、拼接型
>                   简单的组合特征，就是把多个特征组合成一个
>               (2)、模型特征组合
>                   用GBDT产出特征组合路径。组合特征和原始特征一起放到LR中进行训练
>           7、缺失特征处理
>               对缺失的特征可以修改成新的特征，也可以删除。
>               (1)、删除
>                   如果大部分样本该属性都缺失，那么这个属性提供的信息有限，可以选择删除
>                   如果一个样本大部分属性，可以选择删除该样本
>                   这种删除只适用于数据集中缺失较少的情况
>               (2)、统计填充
>                   可以使用平均数、中位数、众数、最大值、最小值等
>               (3)、统一填充
>                   对于含缺失值的属性，把所有缺失值统一填充为自定义值。
>                   如果有可用类别信息，也可以为不同类别分别进行统一填充。常用的统一填充值有："空"、"0"、"正或负无穷"
>               (4)、预测填充
>                   可以通过预测模型利用不存在缺失值的属性来预测缺失值，先用预测模型把数据填充后再做进一步的分析，如统计、学习等。
>                   **虽然这种方法比较复杂，但是最后得到的结果比较好**。！！！
>           8、特征转换
>               对已有的特征做变换，产生新的特征。
>               常见的数据变换有：
>                   (1)、基于多项式的
>                   (2)、基于指数函数的
>                   (3)、基于对数函数的
>
>

- **特征选择：**
>       当数据预处理完成后，需要选择有意义的特征进行训练。原始的特征可能有冗余（两个特征是表示一个问题，相关性太强）、噪声（影响效果）
>       通常从两个方面考虑选择特征：
>           1、特征是否发散
>               如果一个特征不发散，方差接近于0，就是说样本在这个特征上基本没差异，这个特征对于样本的区分没有什么用
>           2、特征与目标的相关性
>               这个比较明显，除了方差方法外，还有以下方法：
>                   a、过滤方法
>                       评估单个特征与结果之间的相关程度，留下topk相关的特征。
>                       评价方式：Pearson相关系数、互信息、距离相关度
>                       缺点：只评估了单个特征对结果的影响，没有考虑特征之间的关联作用，有可能将有用的关联特征踢掉。**工业界使用较少**。！！！
>                       (1)、方差选择方法
>                           计算各个特征的方差，选择方差大于阈值的特征
>                       (2)、相关系数方法
>                           计算各个特征对目标值的相关系数以及相关系数的P值。可以用sklearn的feature_selection的SelectBest类结合相关系数来选择
>                           卡方检验是检验定性自变量对定性因变量的相关性；互信息是评价定性自变量对定性因变量的相关性，这两个都可以用sklearn处理
>                   b、包裹方法
>                       把特征选择看做一个特征子集搜索问题，筛选各种特征子集，用模型评估子集特征的效果。
>                       典型算法："递归特征删除算法"，应用在逻辑回归的过程：
>                           (1)、用全量跑一个模型
>                           (2)、根据线性模型的系数（体现相关性），删除弱特征，观察准确率的变化
>                           (3)、逐步进行，直到准确率出现大的下滑停止
>                   c、嵌入方法
>                       根据模型来分析特征的重要性，最常用的方法为用正则化方式来做特征选择。（**这种方法在工业界很常用**）！！！
>                       (1)、基于惩罚项的方法
>                           就是用L1、L2来正则化做特征选择
>                           L1有截断效应：不重要的参数权重为0
>                           L2有缩放效应：拿到手的特征都比较小。可以用SelectFromModel工具
>                       (2)、基于树模型的特征选择方法
>                           树模型中的GBDT也可用来作为基模型进行特征选择。feature_selection库中的SelectFromModel类结合了GBDT模型
>

- **特征降维：**
>       常见的特征降维方法如下：
>           1、L1惩罚项
>           2、PCA
>               PCA是为了让映射后的样本具有更大的发散性
>               是一种无监督学习
>           3、线性判别分析LDA
>               LDA是为了让映射后的样本有最好的分类性能
>               是一种有监督学习
>
>
>
>
>
>
>
>
>
>
>
>
>
>


- **待续：**
>       参考：https://aiputing.com/topic/642   机器学习之特征工程
>           https://feisky.xyz/machine-learning/basic/feature-engineering.html  特征工程
>           https://www.zhihu.com/question/29316149     特征工程到底是什么？
>           https://juejin.im/post/5b569edff265da0f7b2f6c65     机器学习之 特征工程
>           https://www.jianshu.com/p/5621a98d598f      机器学习之特征工程
>           https://github.com/apachecn/feature-engineering-for-ml-zh   [译] 面向机器学习的特征工程
>
>
>
>
>
>
>
>
