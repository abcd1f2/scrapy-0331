## NLP_总结 -- 概率图模型之-hmm.md
- **概述**：
>
>
>
>
>
>
>
>
>
>
>
>

- **hmm学习：**
>
>       1、cost函数
>           a、如下图，Baum-Welch非监督学习
> ![avatar](https://github.com/nwaiting/wolf-ai/blob/master/wolf_others/pic/pgm_hmm_cost_function.png)
>
>           b、监督学习方法：
>               监督学习方法求转移概率和发射概率，直接计算统计概率值即可
>
>
>       2、优化方法
>           Baum-Welch是一种EM迭代算法
>
>       EM算法的基本思路：
>           随机初始化一组参数0(o),然后根据后验概率模型P(Y | X,0(0) )来更新隐含变量Y的期望E(Y),然后用E(Y)代替Y求出新的模型参数0(1),就这样迭代直到0趋于稳定就可以.
>           对于HMM的第三个问题(学习问题),隐含变量自然就是状态的变量,要求状态变量的期望值实际上就是求在t时刻随机变量X所处状态qt = i的概率,为了求这个概率,我们引入了向前变量和向后变量。
>
>
>
>
>
>
>
>
>
>
>
>

- **待续：**
>       参考：https://www.jianshu.com/p/9047f6e6ca92   EM算法学习(番外篇):HMM的参数估计
>           https://blog.csdn.net/fenghuibian/article/details/91126329  隐马尔科夫模型HMM（四） -- 参数估计问题
>           https://www.cnblogs.com/pinard/p/6972299.html   隐马尔科夫模型HMM（三）鲍姆-韦尔奇算法求解HMM参数
>
>
>
>
>
>
>
>
>
>
>
>
