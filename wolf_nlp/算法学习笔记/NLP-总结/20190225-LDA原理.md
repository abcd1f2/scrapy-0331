## NLP_总结 -- LDA原理
- **概述**：
>       MCMC方法：
>           1、Gibbs
>           2、Metropolis Hastings
>
>
>
>
>
>
>

- **采样：Metropolis–Hastings**
>       Metropolis–Hastings是统计学与统计物理中的一个钟马尔科夫蒙特卡罗(MCMC)方法，用于在难以直接采样时从某一概率分布中抽取随机样本序列，
>           得到的样本序列可用于估计该概率分布或计算积分（如期望）等。
>       Hastings或其他的MCMC算法一般用于从多变量（尤其是高斯）分布中采样；对于单变量，常使用自适应判别采样等其他能抽取独立样本的方法，而不会出现MCMC中样本自相关的问题。
>

- **采样：Gibbs**
>       Gibbs采样是统计学中用于马尔科夫蒙特卡罗（MCMC）的一种算法，用于在难以直接采样时从某一多变量概率分布中近似抽取样本序列，
>           该序列可用于近似联合分布、部分变量的边缘分布或计算积分（如期望）。
>           某些变量可能为已知变量，故对这些变量并不需要采样。
>       Gibbs采样常用于统计推断（尤其是贝叶斯推断）之中。是一种随机化算法，与最大期望算法等统计推断中的确定性算法相区别。与其他的MCMC算法一样，Gibbs重马尔科夫链中抽取样本，
>           可以看做是Hastings（黑斯廷斯）的特例。
>       Gibbs适用于条件分布比边缘分布更容易采样的多变量分布。
>

- **MCMC方法**
>       马氏链及其平稳分布：
>           马氏链定理：马氏链的最终收敛与初始状态无关，由概率转移矩阵决定
>
>
>
>
>
>

- **待续：**
>       参考：https://cosx.org/2013/01/lda-math-mcmc-and-gibbs-sampling/   LDA-math-MCMC 和 Gibbs Sampling
>            https://zhuanlan.zhihu.com/p/21112618      MCMC: Metropolis-Hastings algorithm
>           https://zh.wikipedia.org/wiki/%E6%A2%85%E7%89%B9%E7%BD%97%E6%B3%A2%E5%88%A9%E6%96%AF%EF%BC%8D%E9%BB%91%E6%96%AF%E5%BB%B7%E6%96%AF%E7%AE%97%E6%B3%95     梅特罗波利斯－黑斯廷斯算法
>           https://zh.wikipedia.org/wiki/%E5%90%89%E5%B8%83%E6%96%AF%E9%87%87%E6%A0%B7     吉布斯采样
>           https://zhuanlan.zhihu.com/p/23114198   玩点高级的--带你入门Topic模型LDA（小改进+附源码）
>           https://youzipi.blog.csdn.net/column/info/topic-model   主题模型系列详解（多篇文章）
>
>
>
>
>
>
>
>
>
>
>
