## NLP_总结 -- 优化算法
- **概述**：
>       常见的优化算法：
>           1、梯度下降法（Gradient Descent）
>           2、共轭梯度法（Conjugate Gradient）
>           3、Momentum算法及其变体
>           4、牛顿法和拟牛顿法（包括L-BFGS）
>           5、AdaGrad
>           6、Adadelta
>           7、RMSprop
>           8、Adam及其变体
>           9、Nadam
>

- **梯度：**
>       1、全量梯度下降
>           计算所有样本的损失
>       2、批量梯度下降
>           每次计算一个batch样本的损失
>       3、随机梯度下降
>           每次随机选取一个样本计算损失
>           对于非凸的优化问题，我们可以将其转化为对偶问题，对偶函数一定是凹函数，但是这样求出来的解并不等价于原函数的解，只是原函数的一个确下界
>

- **Momentum：**
>       SGD中，每次的步长一致，并且方向都是当前梯度的方向，这会收敛的不稳定性：无论在什么位置，总是以相同的“步子”向前迈
>       Momentum的思想就是模拟物体运动的惯性：当我们跑步时转弯，我们最终的前进方向是由我们之前的方向和转弯的方向共同决定的。
>       Momentum在每次更新时，保留一部分上次的更新方向
>       优点：
>           一定程度上缓解了SGD收敛不稳定的问题，并且有一定的摆脱局部最优的能力（当前梯度为0时，仍可能按照上次迭代的方向冲出局部最优点），直观上理解，它可以让每次迭代的“掉头方向不是那个大“
>       缺点：
>           这里又多了另外一个超参数ρ需要我们设置，它的选取同样会影响到结果
>

- **Adagrad：**
>       即adaptive gradient，自适应梯度法
>       它通过记录每次迭代过程中的前进方向和距离，从而使得针对不同问题，有一套自适应调整学习率的方法
>       优点：
>           解决了SGD中学习率不能自适应调整的问题
>       缺点：
>           学习率单调递减，在迭代后期可能导致学习率变得特别小而导致收敛及其缓慢
>           同样的，我们还需要手动设置初始α
>

- **RMSprop：**
>       其实它就是Adadelta，这里的RMS就是Adadelta中定义的RMS，也有人说它是一个特例，ρ=0.5的Adadelta，且分子α，即仍然依赖于全局学习率
>
>

- **Adam：**
>       Adam 算法将动量和 RMSProp 的概念结合成一种算法，以获得两全其美的效果
>       利用误差函数的一阶矩估计和二阶矩估计来约束全局学习率
>       adam是世界上最好的优化算法，不知道用啥时，用它就对了（仅供参考）
>       优点：
>           结合Momentum和Adaprop，稳定性好，同时相比于Adagrad，不用存储全局所有的梯度，适合处理大规模数据
>
>
>
>
>

- **待续：**
>       参考：https://blog.csdn.net/qsczse943062710/article/details/76763739   最全的机器学习中的优化算法介绍
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
