### NLP_数学 -- 最大熵与EM
- **概述**：
>       **信息量与概率成反比，熵是信息量的期望，熵反应的是不确定性，概率反应的是确定性**
>
>       1、信息量与概率：
>           信息量与概率成反比，概率越大，信息量越小
>           h(xi) = log(1/p(xi))
>       2、熵：
>           是信息量的期望，**反应的是不确定性,概率反应的是确定性**,熵的大小可以大于1
>           H(x)=∑P(xi)log(1/P(xi))
>
>       3、条件熵：
>           H(x|y)
>           条件熵表示在已知随机变量X的条件下随机变量Y的不确定性。
>
>       4、联合熵：
>           并集
>           H(x,y)
>
>       5、互信息：
>           I(x,y)
>           交集
>           变量间相互依赖性的度量
>           应用在特征选择、降维、特征的相关性
>           反应相关性时与相关性系数ρ的区别：
>               相关性系数ρ只能刻画线性相关性系数
>               互信息主要刻画非线性相关性
>
>       6、相对熵(KL散度)：(重要)
>           D(p||q)=∑p*log(p/q)，即p,q两个概率分布之间的相对熵
>           **衡量两个分布之间的差异，即两个样本集之间的差异性，近似度量**
>           **某种度量的方式去度量概率上的差异，以此分出两堆样本的差异性**
>           特点1：不对称KL(p||q)不等于KL(q||p)
>           特点2：D(p||q) >= 0（应用到琴生不等式进行证明）
>           当散度D(p||q)很大时，表示p和q分布差异非常明显
>
>       7、交叉熵：(重要)
>           刻画两个分布的
>           交叉熵可以用来计算学习模型分布于训练分布之间的差异
>           交叉熵广泛用于逻辑回归的Sigmoid和Softmax函数中作为损失函数使用
>           CH(p,q) = H(p) + D(p||q)
>
>       8、信息增益：
>           决策树
>
>

- **最大熵模型：**
>       最大熵的原则：
>           承认已知事物，对未知事物没有任何偏见
>       例子：
>           对于一个简单的变量，则求最大熵即对熵求概率p的偏导等于0，求得最大熵时的概率
>       最大熵：
>           训练一个分类器时使用，这时条件熵起作用，一般求条件熵的最大值
>           因为在分类器模型中，当给定特征X时，需要求出最大的P(Y|X)，即最大的条件概率
>           P(y|x) = argmax H(y|x)
>           这个式子表示：
>               **给定一个x，达到最大熵的条件概率P(y|x)判断出y的归属分类，是通过最大熵模型最后判断类别归属**（这句话真特么拗口啊）
>               满足最大熵模型下的最优的条件概率
>
>           **在满足先验概率以及约束条件下，求相应的参数，满足条件熵最大，则为求最大熵的目的!!!!**
>
>       算法：
>           1、定义条件熵
>           2、构造特征函数
>           3、约束条件
>               a、全概率公式得到加和为1
>               b、求期望值,Ef(xi)即有多个特征函数
>           4、目标函数就是max H(y|x)
>               有目标函数，有约束条件，用拉格朗日乘子方法求取结果。
>               构造拉格朗日乘子法，然后求偏导，令偏导为0，求到条件熵的最值，即条件概率的最优解的表达式（含有参数λ）
>               **迭代求解参数λ，求取最大熵时的概率最优解**
>               IIS(improved iterated scaling)迭代求解：
>                   主要思想：通过迭代的思想，求解参数λ使得拉格朗日乘子法结果越大
>
>
>

- **凹函数和凸函数：**
>       凹函数：
>           曲线头凸在直线之上
>           判断方法二：求函数的二阶导数，f''(x)<=0
>       凸函数：
>           曲线头凸在直线之下
>
>       琴生不等式性质：
>           Ef(x)与f(Ex)关系
>
>
>


- **熵在工程中的应用:**
>       熵在深度学习领域，如能量函数，指引参数调整
>       熵在决策树、随机森林的生长过程、参数调整过程，利用信息增益，利用互信息的方式
>       互信息：变量间相互依赖性的度量，常用在特征选择、特征的关联性，主要刻画非线性的相关性，降维时候应用到
>       相关系数：也是一种刻画相关性的方法，只能刻画线性相关性
>
>       相对熵即KL散度，在求解和贝叶斯相关的度量当中应用广泛
>
>       交叉熵：深度学习中最常用的目标函数能量函数
>
>

- **EM算法：**
>       迭代优化
>       两种理解方法：
>           方法1、最大释然估计 -> 求解下边界函数极值 -> 最后到EM算法
>           方法2、先通过k均值聚类k-means -> 高斯混合模型GMMS -> 最后到EM算法
>
>       方法1：
>           对于没有隐变量的最大似然：
>               服从独立同分布的情况下，对于似然函数：likelihood = ∏P(yi)，通过求导可以求解最大似然
>           附有隐变量的似然函数：
>               基本思想：
>                   l(θ) = ∑ln∑Pθ(yi|z)Pθ(z) 含有隐变量的对数似然函数
>                   **对数中有加和项时、指数中有加和项等，难以求到解析解，只能迭代求解近似最优解**
>                   **最大似然 -> EM  -> 下边界函数 -> (求导求极值)求解一个最优的下边界函数 -> 最后求得一个相对优的参数θ**
>
>               算法：
>                   输入：显变量Y(统计得到),隐变量Z(随机赋值)
>                   输出：模型参数θ
>                   过程：
>                       1、给参数θ随机赋初值
>                       2、E(Expectation)估计：令θn为第n次求得的参数
>                           Q(θ|θn) = ∑Pθn(Z|y)lnPθ(Z|y)，即对lnPθ(Z|y)求期望，概率为Pθn(Z|y)
>                       3、M求最大值：Q函数求偏导∂Q/∂θ=0 => θ(n+1)=argmax(Q(θ|θn))
>                       4、迭代过程：重复2、3步直到找到符合条件退出
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>

- **EM(Expectation Maximization)算法在工程中应用：**
>       主要用做参数的估计
>           迭代的优化方式
>
>
>
>
>
>
>
>
>

- **待续：**
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
