### NLP_数学 -- 奇异值分解SVD、LSI(LSA)
- **概述**：
>
>
>
>
>
>
>

- **特征向量和特征值：**
>       矩阵乘法对应了一个变换，是把任意一个向量变成另一个方向或长度的新向量。在变化过程中，原向量主要发生旋转、伸缩的变化。
>       如果矩阵对某些向量只发生伸缩变换，不产生旋转效果，那么这些向量就称为这个矩阵的特征向量，伸缩的比例就是特征值。
>
>       特征值分解是将一个矩阵分解为如下形式：
>           A = QΣQ^(-1)，其中Q^(-1)表示Q的逆
>           其中，Q是矩阵A的特征向量组成的矩阵，Σ是一个对角矩阵，每一个对角线元素就是一个特征值，里面的特征值由大到小排列的，
>               这些特征值所对应的特征向量就是描述这个矩阵变化仿效（从主要的变化到次要的变化排序）。
>           即矩阵A的信息可以由特征值和特征向量表示。
>
>       总结：
>           特征值分解可以得到特征值和特征向量，特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么。
>           但是特征值分解也有很多的局限，比如变换的矩阵必须是方阵。
>

- **奇异值分解SVD：**
>       特征值分解是一个提取特征很不错的方法，但是它只是针对方阵而言。！！！
>       在实际应用中，大部分矩阵都不是方阵，那么怎么才能描述这种非方阵矩阵的重要特征呢？奇异值分解就是一个能适用于任意的矩阵的一种分解方法。
>       分解，如下图，
> ![avatar](https://github.com/nwaiting/wolf-ai/blob/master/wolf_others/pic/nlp_math_matrix_svd.jpg)
>       A是一个m*n的矩阵，得到的U是一个m*m的方阵（称为左奇异向量），
>       Σ是一个m*n的矩阵(除了对角线的元素都是0，对角线上的元素称为奇异值),
>       V^T是一个n*n的矩阵（称为右奇异向量）
>
>

- **协方差：**
>       协方差的意义：
>           度量各个维度偏离其均值的程度。
>           分为方差和协方差两部分，方差构成了对角线上的元素，协方差构成了非对角线上的元素。
>           方差：σx = (1/(n-1)) * ∑(x-x')^2
>           协方差：σ(x,y) = (1/(n-1)) * ∑(x-x')(y-y')
>           方差可视作随机变量x关于其自身的协方差σ(x,x)
>
>           协方差的值意义：
>               正值，则说明两者是正相关的（从协方差可以引出"相关系数"的定义）
>               负值，两者为负相关
>               零值，相互独立
>           协方差绝对值越大，两者对彼此的影响越大，反之越小。
>
>       实例：
>           比如样本对(xi,yi),对x,y做二维图，则分为4个象限，
>           正相关：
>               情况1：X越大，Y也越大
>               情况2：X越小，Y也越小
>           负相关:
>               情况1：X越大，Y越小
>               情况2：X越小，Y越大
>

- **协方差矩阵：**
>       对角线上的元素为各个随机变量的方差，非对角线上的元素为两两随机变量之间的协方差。
>       根据协方差的定义，可以认为协方差矩阵是对称矩阵。
>

- **对称矩阵：**
>       指元素以主对角线为对称轴对应相等的矩阵。
>

- **正交矩阵：**
>       正交矩阵是指其转置等于其逆的矩阵，若n阶方阵A满足A*A^T=E（即A^-1=A^T），那么A称为正交矩阵。
>
>
>
>
>
>

- **待续**
>       参考：
>           https://www.zhihu.com/question/22237507     奇异值的物理意义是什么？ 非常经典的一篇介绍奇异值的文章
>           https://www.zhihu.com/question/22237507/answer/53804902
>           https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html    数学(5)-强大的矩阵奇异值分解(SVD)及其应用
>           https://www.zhihu.com/question/21874816     如何理解矩阵特征值？
>
>
>
>
>
>
>
>
>
>
>
>
>
