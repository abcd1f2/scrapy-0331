### NLP_数学 -- sigmoid vs softmax
- **概述**：
>       sigmoid：（也叫逻辑斯蒂函数）
>           sigmoid是将一个值隐射到(0,1)区间，常用来做二分类问题或神经网络的激活函数
>       softmax：
>           softmax将一个k维的值向量(a1,a2,...)隐射成一个(b1,b2,...)，其中bi是一个(0,1)的常熟，可以根据bi的大小进行多分类。
>           softmax是logistic二分类的推广，当k=2时，softmax退化为logistic回归。表明softmax回归是logistic回归的一般形式。
>           softmax一般用于神经网络的输出层进行归一化，softmax具有和cross-entropy相同的优点，即不会出现学习慢的问题。
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>

- **待续：** 
>
>
>
>
>
>
>
>
>
>
>
>
