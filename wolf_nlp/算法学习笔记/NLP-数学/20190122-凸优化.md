### NLP_数学 -- 凸优化
- **概述**：
>       如果一个最优化问题的可行域是凸集，并且目标函数是凸函数，则该问题为凸优化问题。
>
>

- **牛顿法：**
>       和梯度下降法一样，牛顿法也是寻找导数为0的点，同样是一种迭代法。
>       核心思想：
>           在某点处用二次函数来近似目标函数，得到导数为0的方程，求解该方程，得到下一个迭代点。
>           因为是用二次函数近似，因此可能会有误差，需要反复迭代，直到到达导数为0的点。
>
>       算法过程：
>           参考：https://zhuanlan.zhihu.com/p/37588590
>
>       缺点：
>           1、局部极小值
>               牛顿法找到的导数为0的点，不一定是极值点，因此会面临局部极小值和鞍点问题。
>           2、Hessian矩阵不可逆
>               Hessian矩阵可能不可逆，导致这种方法失效
>               求解Hessian矩阵的逆矩阵或者求解线性方程组计算量大，需要耗费大量时间
>       改进方法：
>           拟牛顿法
>

- **拟牛顿法：**
>       基本思想：
>           拟牛顿法的思想是不计算目标函数的Hessian矩阵然后求逆矩阵，而是通过其他手段得到Hessian矩阵或其逆矩阵。
>           具体做法是构造一个近似Hessian矩阵或其逆矩阵的正定对称矩阵，用改矩阵进行牛顿法的迭代。
>
>       具体做法：
>           (方程略)这个条件称为拟牛顿条件，用来近似代替Hessian矩阵的矩阵需要满足此条件。
>           根据此条件，**构造了多种拟牛顿法**，典型的有：
>               DFP算法
>               BFGS算法
>               L-BFGS算法
>

- **BFGS算法：**
>       BFGS算法的思想就是构造Hessian矩阵的近似矩阵 Bk ≈ Hk，并迭代更新这个矩阵 Bk+1 = Bk + ∆Bk，初始值B0为单位矩阵I，
>           然后，要解决的问题就是每次修正矩阵∆Bk的构造。
>       缺点：
>           每一步迭代都要计算n*n的矩阵Dk，当n很大时，存储改矩阵非常耗费内存。
>       算法优化：
>           改进方法L-BFGS，思想是不存储完整的矩阵Dk，值存储向量Sk和Yk。               
>
>
>
>
>
>

- **常见的机器学习算法凸优化问题：**
>       线性回归：
>           损失函数为误差平方和均值，优化问题是一个不带约束条件的凸优化问题，
>           可以用梯度下降法或牛顿法
>       岭回归：
>           岭回归是加上正则化项之后的线性回归。
>           加上L2正则化，如果正则化项系数大于0，则Hessian矩阵是严格正定的,否则为半正定的。
>       SVM：
>           通过拉格朗日对偶，转换为对偶问题，加上核函数后的对偶问题，目标函数是凸函数
>       logistics回归：
>           加上L2正则化后，求解的是一个不带约束的优化问题，这个函数的Hessian矩阵是半正定的。
>       softmax回归：
>           softmax是logistic回归在多分类问题的推广
>           这是一个不带约束的优化问题，目标函数是凸函数
>       神经网络训练时优化目标函数不是凸函数，因此有可能陷入局部极小值和鞍点的风险，这也是神经网络一个长期的问题。
>
>
>
>


- **待续：**
>       参考：https://zhuanlan.zhihu.com/p/37588590    理解牛顿法
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
