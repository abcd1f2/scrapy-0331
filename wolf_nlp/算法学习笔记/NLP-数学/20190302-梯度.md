## NLP_数学 -- 梯度
- **概述**：
>
>
>       梯度下降算法家族：
>           BGD (Batch Gradient Descent) 批量梯度下降
>           SGD (Stochastic Gradient Descent) 随机梯度下降
>           MBGD (Mini-batch Gradient Descent) 小批量梯度下降法
>
>

- **梯度下降算法之BGD：**
>       是GD最常用的形式
>       具体思想：
>           在更新参数时使用所有的样本来进行更新
>

- **梯度下降算法之SGD：**
>       和BGD原理类似
>       具体思想：
>           求梯度时没有用所有的m个样本的数据，而是仅仅选取一个样本j来求梯度
>

- **BGD VS SGD：**
>       两个算法是两个极端，BGD使用所有样本，SGD使用一个样本
>       训练速度：
>           SGD训练速度快，BGD训练速度较慢
>       准确度：
>           SGD仅仅用一个样本来决定梯度方向，导致解很可能不是最优
>       收敛速度：
>           SGD一次迭代一个样本，导致迭代方向变化很大，不能很快收敛到局部最优解
>

- **梯度下降算法之MGD：**
>       MGD是BGD和SGD的折衷
>       具体思想：
>           对于m个样本，采用x个子集来迭代，其中1<x<m
>           一般可以取x=10，可以根据样本调整x的值
>
>
>


- **梯度下降算法调优：**
>       1、算法步长
>           需要多次运算后得到一个最优解
>       2、算法参数的初始值
>           如果损失函数是凸函数则一定是最优解，当然由于有局部最优解的风险，所以需要多次用不同初始值运行算法
>       3、归一化
>           为了减少特征取值的影响，可以对特征数据归一化
>

- **待续：**
>
>       参考：https://zhuanlan.zhihu.com/p/24913912    为什么梯度反方向是函数值局部下降最快的方向？
>           https://www.cnblogs.com/pinard/p/5970503.html   梯度下降（Gradient Descent）小结
>
>
>
>
>
>
>
>
>
>
>
>
>
