## NLP_数学 -- 牛顿法
- **概述**：
>       本文主要内容：
>           牛顿法
>           拟牛顿法
>
>       机器学习中经常有非线性的优化问题，主要工作在于求解非线性极小化问题。
>
>       通过函数在x0处的二阶泰勒展开：
>           仅仅使用梯度信息的优化算法被称为一阶优化算法，如梯度下降。
>           使用Hessian矩阵的优化算法被称为二阶最优化算法，如牛顿法
>
>

- **newton方法：**
>       算法：
>           通过函数f(x)在x0处二阶泰勒展开近似得φ(x)，求φ'(x)=0，则可得newton法。
>
>

- **newton应用：**
>       注意：
>           当f(x)是一个正定二次函数时，牛顿法只要应用一次迭代就能直接跳到函数的最小点。
>           如果f(x)不是一个真正二次但能在局部近似为正定二次，牛顿法则需要多次迭代。
>           迭代地更新近似函数和跳到近似函数的最小点可以比梯度下降更快地达到临界点。
>           这在接近局部最小值时候是一个特别有用的性质，但是在鞍点附近是有害的。！！！
>           当附近的临界点是最小点（Hessian矩阵的所有特征值都是正的）时，牛顿法才适用，而梯度下降不会被吸引到鞍点（除非梯度指向鞍点）。
>
>       缺点：
>           1、原始牛顿法由于迭代公式中没有步长因子，而是定步长迭代，对于非二次型目标函数，有时会使函数值上升，
>               即出现f(k+1)>f(k)的情况，表明原始牛顿法不能保证函数值稳定地下降
>           2、对目标函数有较严格要求，函数必须具有连续的一、二阶偏导数，Hessian矩阵必须正定
>           3、计算复杂，除需要计算梯度外，还需要计算二阶偏导矩阵和逆矩阵，计算和存储量很大，且均以维数N的平方比增加
>
>

- **阻尼牛顿法**
>       优化步长因子：
>           阻尼牛顿法每次迭代方向仍采用dk，但是每次迭代需沿此方向作一维搜索，寻求最优的步长因子λk，即λk=arg minf(Xk+λ*dk)
>


- **拟牛顿法思想：**
>       DFP、BFGS、L-BFGS算法都是重要的拟牛顿法
>       拟牛顿法基本思想：
>           不用二阶偏导数而够找出可以近似Hessian矩阵的正定对称阵，在拟牛顿的条件下优化目标函数。
>           只是对算法中用来计算搜索方向的Hessian矩阵作了近似计算
>

- **拟牛顿法之DFP算法：**
>       DFP是最早的拟牛顿法。
>       算法思想，相对于newton方法的优势：
>           通过迭代的方法，**对Hessian矩阵的逆做近似**。
>
>

- **拟牛顿法之BFGS算法：**
>       与DFP相比，BFGS算法性能更佳。目前成为求解无约束非线性优化问题最常用的方法之一。
>       通过迭代算法，对Hessian矩阵做逼近
>       相对于DFP算法优势：
>           整个算法中不再需要求线性代数方程组，由矩阵-向量运算就可以完成
>
>       BFGS和DFP算法相比，唯一不同仅在于Dk+1的迭代公式不同。！！！
>
>       BFGS算法缺点：
>           在BFGS中，需要用到一个N*N的矩阵Dk，当N很大时，存储这个矩阵将变得很耗计算机资源。
>

- **拟牛顿法之L-BFGS算法：**
>       L-BFGS即 Limited-memory BFGS或Limited storage BFGS
>
>       L-BFGS思想：
>           L-BFGS就是这样一种算法，对BFGS算法进行了近似
>           不再存储完整的矩阵Dk，而是存储计算过程中的向量序列{si}、{yi}，需要矩阵Dk时，利用向量序列{si}、{yi}的计算来代替。
>           而且{si}、{yi}并不是所有的都存，而是固定存最新的m个(参数m可以根据用户机器内存自行指定)，每次计算Dk时，只利用最新的m个{si}和m个{yi}。
>           则将存储由原来的O(N^2)降到了O(mN)
>
>
>       相对于BFGS的优势：
>           L-BFGS将存储由原来的O(N^2)降到了O(mN)
>
>

- **迭代步长的计算：**
>       1、精确搜索
>           寻找最优的步长因子（阻尼牛顿法）
>           但是每次迭代需沿此方向作一维搜索（line search），寻找最优的步长因子λk
>       2、非精确搜索
>           Wolfe搜索
>           Armijo搜索
>           满足Goldstein条件的搜索
>
>
>
>
>


### 知识点介绍
- **鞍点：**
>       定义：
>           一个不是局部最小值的驻点。
>       数学含义为：
>           函数在此点一阶导数为零，但该点是某一方向上的函数极大值点，在另一方向上是函数极小值点。
>           在矩阵中，若一个元素是所在行中的最大值，所在列中的最小值，称之为鞍点。
>       判断鞍点的充分条件：
>           函数在驻点的Hessian矩阵为不定矩阵
>
>
>       鞍点，如下图，
> ![avatar](https://github.com/nwaiting/wolf-ai/blob/master/wolf_others/pic/nlp_math_newton_Saddle_point.gif)
>       如f(x)=x^3上x=0的点就是鞍点
>

- **驻点：**
>       函数的一阶导数等于零的点
>       对于可微函数，极值点一定是驻点
>

- **拐点：**
>       函数的凸凹分界点
>       若拐点二阶可导，则二阶导数为0
>
>
>


- **待续：**
>
>       参考：https://blog.csdn.net/itplus/article/details/21896453    详细讲解牛顿法以及各种拟牛顿法
>           相关书籍参考：numerical optimization  数值优化
>
>
>
>
>
>
>
>
>
>
>
>
