## NLP_数学 -- 牛顿法
- **概述**：
>       通过函数在x0处的二阶泰勒展开：
>           仅仅使用梯度信息的优化算法被称为一阶优化算法，如梯度下降。
>           使用Hessian矩阵的优化算法被称为二阶最优化算法，如牛顿法
>
>

- **newton方法：**
>
>
>
>
>
>

- **newton应用：**
>       注意：
>           当f(x)是一个正定二次函数时，牛顿法只要应用一次迭代就能直接跳到函数的最小点。
>           如果f(x)不是一个真正二次但能在局部近似为正定二次，牛顿法则需要多次迭代。
>           迭代地更新近似函数和跳到近似函数的最小点可以比梯度下降更快地达到临界点。
>           这在接近局部最小值时候是一个特别有用的性质，但是在鞍点附近是有害的。！！！
>           当附近的临界点是最小点（Hessian矩阵的所有特征值都是正的）时，牛顿法才适用，而梯度下降不会被吸引到鞍点（除非梯度指向鞍点）。
>
>       鞍点，如下图，
> ![avatar](https://github.com/nwaiting/wolf-ai/blob/master/wolf_others/pic/nlp_math_newton_Saddle_point.gif)
>
> <iframe height=500 width=500 src="https://github.com/nwaiting/wolf-ai/blob/master/wolf_others/pic/nlp_math_newton_Saddle_point.gif">
>
>
>
>
>
>
>


- **待续：**
>
>       参考：
>
>
>
>
>
>
>
>
>
>
>
>
>
