## NLP_数学 -- PCA和SVD
- **概述**：
>       特征值分解和奇异值分解的目的是一样的，就是提取出一个矩阵中最重要的特征。
>
>
>

- **奇异值分解的物理意义：**
>       参考：数学之美 系列十八 － 矩阵运算和文本处理中的分类问题
>       奇异值分解在文本分类中的应用：
>           （下面即为LSI的精髓内容）
>           比如用一个大矩阵A来描述着一百万篇文章和五十万词的关联性，即A1000000*500000，
>           奇异值分解就是把A分解成三个小矩阵相乘，X*B*Y，则X1000000*100,B100*100,Y100*500000，则三个矩阵总元素加起来为原来的1/3000。
>           且三个小矩阵有非常清楚的物理含义：
>               X的每一行表示意思相关的一类词，数值大小表示相关性
>               B表示类词与文章类之间的相关性
>               Y的每一列表示同一类文章的相关性
>

- **特征值：**
>       特征值分解可以得到特征值与特征向量，特征值表示这个特征的重要性，特征向量表示什么，可以将每一个特征向量理解为一个线性的子空间。
>       特征值分解有很多局限性，比如变换的矩阵必须是方阵。！！！
>

- **奇异值：**
>       由于特征值分解只针对方阵，奇异值分解是针对任意的矩阵进行分解。
>
>
>
>
>
>
>
>
>
>
>
>
>


- **待续：**
>       参考：http://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html 机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用（以及svd在nlp中的应用）
>           https://mp.weixin.qq.com/s/Dv51K8JETakIKe5dPBAPVg   机器学习中SVD总结（矩阵分解的常用方法）
>           https://zhuanlan.zhihu.com/p/37777074   主成分分析（PCA）原理详解
>           https://zhuanlan.zhihu.com/p/30482640   关于奇异值分解SVD的总结（PCA、LDI）
>           https://www.zhihu.com/question/263722514/answer/272977924   人们是如何想到奇异值分解的？
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
