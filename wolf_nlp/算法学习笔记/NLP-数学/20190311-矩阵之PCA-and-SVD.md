## NLP_数学 -- PCA和SVD
- **概述**：
>       **特征值分解和奇异值分解的目的是一样的，就是提取出一个矩阵中最重要的特征。**
>
>       PCA的实现一般有两种：
>           一种是特征值分解实现的（变换的矩阵必须是方阵）
>               最大方差法
>               最小损失法
>           一种是奇异值分解实现的（适用于任意的矩阵分解）
>
>
>       奇异值分解意义：
>           可以将一个比较复杂的矩阵用更小更简单的几个子矩阵的相乘来表示，这些小矩阵描述的是矩阵的重要的特性
>           应用：
>               PCA
>               数据压缩（图像压缩）
>               搜索引擎的语义检索的LSI
>
>
>

- **奇异值分解的物理意义：**
>       参考：数学之美 系列十八 － 矩阵运算和文本处理中的分类问题
>       奇异值分解在文本分类中的应用：
>           （下面即为LSI的精髓内容）
>           比如用一个大矩阵A来描述着一百万篇文章和五十万词的关联性，即A1000000*500000，
>           奇异值分解就是把A分解成三个小矩阵相乘，X*B*Y，则X1000000*100,B100*100,Y100*500000，则三个矩阵总元素加起来为原来的1/3000。
>           且三个小矩阵有非常清楚的物理含义：
>               X的每一行表示意思相关的一类词，数值大小表示相关性
>               B表示类词与文章类之间的相关性
>               Y的每一列表示同一类文章的相关性
>

- **特征值和特征向量：**
>       特征值分解可以得到特征值与特征向量，特征值表示这个特征的重要性，特征向量表示什么，可以将每一个特征向量理解为一个线性的子空间。
>       一个矩阵的一组特征向量是一组正交向量
>       特征值分解：
>           特征值分解可以得到特征值与特征向量，特征值表示的是这个特征到底有多重要，特征向量表示这个特征是什么，可以将每一个特征向量理解为一个线性的子空间。
>       特征值分解有很多局限性，比如变换的矩阵必须是方阵。！！！
>

- **奇异值分解详解：**
>       由于特征值分解只针对方阵，奇异值分解是针对任意的矩阵进行分解。
>       A = u * ∑ * v^T
>       A为m*n矩阵，u为m*m的方阵(里面的向量都是正交的，左奇异向量)，∑为m*n的矩阵(奇异值)，v是n*n的矩阵(里面的向量都是正交的，右歧义向量)
>       由于A是非方阵，那么A*A^T将会得到一个方阵，用这个方阵求特征值可以得到右奇异向量，
>           (A^T * A)v = λv
>           即可以得到右奇异向量，但是这样只能得到一个方向的PCA，根据公式推导，PCA集合可以说是对SVD的一个包装
>           (Lanczos迭代是一种解对称方阵部分特征值的方法，是将一个对称的方程化为一个三对角矩阵再进行求解)
>
>       SVD不仅可以对列进行压缩（对特征进行压缩，去掉没有太大价值的特征），还可以对行进行压缩（去掉没有太大价值的sample）
>
>       奇异值的计算是一个O(N^3)的算法，所以当数据量太大时，需要并行计算
>
>
>
>
>
>
>

- **PCA：**
>       PCA的思想是将n维特征映射到k维上（k<n），这k维是全新的正交特征。这k维特征称为主元，是重新构造出来的k维特征，而不是简单地从n维特征中去除其余n-k维特征。
>       PCA可以被定义为数据在低维线性空间上的正交投影，这个线性空间被称为主子空间，使得投影数据的方差被最大化。
>           等价地，它也可以被定义为使得平均投影代价最小的线性投影。平均投影代价是指数据点和它们投影之间的平均平方距离。
>
>       使用SVD求解PCA：
>
>
>
>
>
>
>
>
>
>


- **待续：**
>       参考：http://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html 机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用（以及svd在nlp中的应用）
>           https://mp.weixin.qq.com/s/Dv51K8JETakIKe5dPBAPVg   机器学习中SVD总结（矩阵分解的常用方法）
>           https://zhuanlan.zhihu.com/p/37777074   主成分分析（PCA）原理详解
>           https://zhuanlan.zhihu.com/p/30482640   关于奇异值分解SVD的总结（PCA、LDI）
>           https://www.zhihu.com/question/263722514/answer/272977924   人们是如何想到奇异值分解的？
>           https://www.cnblogs.com/jerrylead/archive/2011/04/18/2020216.html   主成分分析（Principal components analysis）-最小平方误差解释
>           http://pelhans.com/2019/01/03/prml_note12/  连续潜在变量(PRML)
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
