### NLP汉语自然语言处理原理与实践（三）
- **词汇与分词技术**
> #### 概述：
>
>> 前期主要是人工完成，标准不统一，语料精度不高，因此迫切需要统一的分词规范及高精度的中文分词算法
>
>> <b>中文分词经历了20年，基本分为3个流派：</b>
> - ##### 机械分词法(基于词典)
>
>>  与词典中的词逐一匹配，如果找到则匹配成功
>
>>  特点：实现简单、实用性强，但是对未登录词识别效果差
>
>> - ##### 基于语法和规则的分词法
>
>>  思想就是在分词的同时进行句法、语义分析，利用句法信息和语义信息来进行词性标注，以解决分词歧义现象。但是现在的语法、句法规则十分复杂，所以分词精确度不能令人满意。
>
>> - ##### 基于统计的分词法
>   思想就是根据字符串在语料库中出现的统计频率来决定是否构成词。词是字的组合，相邻的字同时出现的次数越多，越有可能成为一个词，因此字与字相邻共现的频率或概率能反应它们成为词的可信度
>
>>  HShort最短路径算法、一元词网与原子切分、生成二元词图、命名实体识别、细分阶段等
>
>> <b>分词规范：</b>
> - ##### 北大规范
>   将汉语的词类分为13种：名词、动词、代词、形容词、数词、量词、副词、介词、连词、助词、语气词、叹词和象声词
>> - ##### 宾州树库中文分词规范
>
>> <b>两种分词标准</b>
> - ##### 粗粒度
>   将词作为语言处理的最小单位进行切分
> - ##### 细粒度
>   不仅对词进行切分，也对词汇内部的语素进行切分。
>   一般细粒度切分的对象都为专有名词，因为专有名词常表现为几个一般名词的组合
> - ###### 实际应用
>   粗粒度和细粒度使用范围不同，粗粒度切分主要用于自然语言处理的应用中；细粒度分词最常用的就是搜索引擎。搜索引擎中，常用的方案是在索引的时候使用细粒度分词保证召回，在查询的时候使用粗粒度分词保证精度。
>
>> <b>歧义、机械分词、语言模型</b>
> - ###### 机械分词
> 基于最大匹配方法作为最基本的分词算法，正向最大匹配算法，简称MM算法
>
>>  使用MM算法精度不高，随着语料的增大，误差也逐渐变大，于是提出了双向匹配法。双向匹配法是从最大匹配方法发展而来的，分为正向最佳匹配法和逆向最佳匹配法
>
>>  **双向匹配法：**
>
>>  由于两个算法搜索方向相反，待处理的字符串如果存在交叉歧义，那么两种方法得到的结果必然不同，**缺点**是只能找出最长的词，但是不能找出搜优的候选词，**应用**使用双向扫描法来快速检测出歧义的位置
>
>>  早起的分词器没有考虑词汇上下文关系，分词准确度都不高，正向最大匹配算法的分词器准确度为78%，召回率为75%，F1值为76%，后来的双向匹配算法在80%左右，这种准确率不能满足高精度文本处理的需求
>
>>  **语言模型:**
>
>>  机械分词虽然准确度不高，但是却总结了一个**规律**：一个词汇的出现与其上下文环境中出现的词汇序列存在着紧密的关系
>
>>  如果一个词出现的概率只与它相邻的一个词有关，这时的语言模型叫做**二元模型** ，也叫**一阶Markov链**
>
>>  **总结：**将语言模型应用到分词算法中，中文分词水平得到显著的提高，ICTCLAS使用了模型的中文分词准确率为98%，召回率为98.5%，F1值为98%
>
>> <b>未登录词</b>
>
>> - 未登录词
>
>>  未登录词识别就是包括人名、译名、地理位置名、组织机构名等**专有名词**的识别
>
>>  通常将专有名词和数字、日期等词称为**命名实体**
>
>>  **影响：**命名实体识别效果直接影响到信息抽取、信息检索、机器翻译、文摘自动生成等应用系统的性能
>
>>  **处理方法：**
>
>>  **1、构词学角度研究算法**
>
>>  以往人名处理方法常从构词学角度来研究算法，因此研究了很复杂的基于构词编码的方法，HanLP中文分词系统就是一种基于构词角度来进行命名实体识别的算法。
>
>>  **效果：** 这种算法对于狭窄的专门领域(人名中的中国人名、译名、日本人名等)等未登录词有较好的效果，但是处理大规模不同领域的未登录词存在着很大的问题，至少是不现实的
>
>>  **2、语义类角度研究算法**
>
>>  不同语义类下的未登录词，在统计学规律上具有相似性，利用这个规律，基于**半监督的条件随机场CRF算法**，对于处理不同领域的专有名词识别具有较低的成本和较好的效果

- **系统总结流程和分词结构**
> #### 概述：
>
>>  ICTCLAS汉语分词系统是张华平博士于2002年设计开发的一套中文分词系统，精度最高、速度最快的中文分词系统
>
>>  **特点：**原理简单、易于实现、便于使用、效率高、精度准等优势
>
>>  **功能：**中文分词、词性标注、命名实体识别、用户词典功能以及支持GBK、UTF8等编码
>
>>  **发展：**很多重写了ICTCLAS，有java版的ICTCLAS4J、孙健的Ansj(新版本算法为CRF)、C#版本的SharpIctclas、比较全面的HanLP，这个库对ICTCLAS算法还原的比较好
>
> #### 中文分词流程：
>
>>  ICTCLAS分词算法思想来源于HMM，HanLP的实现做了一些修改：
>
>>> 1、使用双数组树读取和检索词典，提高性能
>
>>> 2、提供了更丰富的命名实体识别库：人名、译名、地名等
>
>>> 3、搜索引擎系统的支持
>
>>> 4、细分阶段使用了最短路径法
>
>>  **分词流程：**
>
>>  句子输入 -> 导入词典 -> 句子切分 -> 词汇初分阶段 -> 一元切分 -> 原子切分 -> 二元切分 -> N-最短路径 -> 应用规则 -> 未登录词识别 -> (人名、译名、地名等识别) -> 优化细分逻辑 -> 词性标注 -> 后处理 -> 输出结果
>
>> **详细流程：**
>
>>  1、读取字符串
>
>>  2、导入模型词典
>
>>  3、粗分阶段
>>> - 字符级切分 切分成单个字符（函数ToCharArray()）
>>> - 一元切分
>
>>>     查询词典，将切分的结果与词典词最大匹配，匹配的结果包括词性、词形、词频等形成一元词网，然后对词网进行原子切分，按模式合并英文和数字的字符构成原子词
>
>>>- 二元切分
>
>>>     用一元分词结果(二维数组)查询二元词典，与二元词典进行最大匹配，匹配的结果为一个Graph词图
>
>>>- NShort算法计算
>
>>>     将查出的结果按平滑算法计算二元分词的词频数得到词图中每个节点的权值(概率的倒数)，应用NShort算法累加词图中每个节点构成所有路径，权值最小(概率最大)对应的路径就是初分结果
>
>>> - 对粗分结果执行后处理应用规则，识别时间类专有名词
>
>>4、未登录词识别
>
>>> - 根据人名识别词典，使用Viterbi算法识别外国人名
>
>>> - 根据地名识别词典，使用Viterbi算法识别地名
>
>>> - 根据组织机构名词典，使用Dijkstra算法识别组织结构名
>
>>5、细分阶段
>
>>>将命名实体识别后的分词结果加入词图中，对词图进行分词(Dijkstra最短路径法)
>
>>6、词性标注
>
>>>使用词性标注模型，使用Viterbi算法最分词结果进行词性标注
>
>>7、转换路径为分词结果，输出结果
>
> #### 分词词典结构：
>
>>词典是中文分词的外部资源，中文分词和命名实体识别都离不开词典和语言模型资源
>
> #### 分词词典结构：
>
>>**HanLP中包括：**
>
>>**一元模型词典(CoreNatureDictionary)**，也成为核心词典，其他词典都是从此词典衍生而来，这个词典越大，包含的词汇越多，一般可用的、最小规模的中文分词器，核心词典规模不能少于30万
>
>>**二元语言模型词典(CoreNatreDictionary.ngram)**，本质上构建了一个二维矩阵，如果把二元和一元模型词典结合在一起，可以看做一个图(Graph)，顶点为一元语言模型中的词，二元语言模型为连接两个顶点的一条边，整个NShort算法就是对这个图计算最大概率的过程
>
>>**人名识别(preson/nr.tr,nrf,nrj)**，结构和一元模型词典类似，trie.dat为一元词典，value.dat为二元词典，当使用原子字符串查询trie.dat时，就返回了相应的元模式及对应的概率，如"王","菲"这两个连续的原子词，可以得到相应的一个元模式:"B"、"E"，这样就构成了隐马尔科夫链的转移概率矩阵
>
>>**地名识别(place/ns.tr)**
>
>>**组织结构名识别(organization/nt.tr)**
>
>##### 词典的存储结构
>>hash数据结构，由于对键的约束不高，一直都是中等规模查询和存储的重要数据结构，但是大规模的数据存储，空间利用率很低，因此引入了一种词典树，其本质是一个确定有限状态机(CFA)，每个节点代表自动机的一个状态，根据变量不同，进行状态转移，当达到介绍状态或无法转移时，完成一次查询操作，而**双数组Trie**机构就是将Trie树用两个整型数组类表示，目的是为了设计一种Trie结构的压缩形式，该结构结合了数字搜索树检索时间高效和链式表示空间紧凑的特点
>
>##### 算法源码解析
>
