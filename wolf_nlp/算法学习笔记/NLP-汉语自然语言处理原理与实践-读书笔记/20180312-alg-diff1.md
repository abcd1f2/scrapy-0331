### NLP汉语自然语言处理实践-随笔
- **分词准备**
> 分词一般需要两种数据文件
>
> <b>1、词典文件：</b>
> 常用词典文件：
>
> **核心词典**
> 用于初始切词，二元词典记录了两个词联合出现的频数
>
> **二元词典**
>
> **人名词典**
>
> **翻译人名词典**
>
> **地名词典**
>
> **词性标注文件**
>
> <b>2、模型文件：</b>
>
>
- **分词流程**
> 待续
>
>
>
>
>

- **ansj词典以及简要分词过程：**
> 1、加载公司名、人名、新词、词性表、词性关联表、bigram模型、英文词典、数字词典、crf模型
>
> 2、在第一步中加载过程中会加载core.dic核心词典，构造双数组trie树、森林
>
> 3、加载用户词典、歧义词典
>

- **ansj分词过程：**
> 参考：http://goofyan.iteye.com/blog/2223945
>
> 构造最短路径图，判断是否启用歧义词典，若存在，优先歧义词，根据歧义分词数组中的词及词性逐个添加到图中
>
>

- **常用分词算法：**
> **1、字典匹配：**
> 启发式的匹配方案，比如：最大匹配(就是尽可能匹配最长的词)、mmseg算法(分词后出现的词尽可能少)
>
> **2、统计模型：**
> 基于字典的分词会有很多问题，比如歧义问题，一元模型(n元模型)可以解决歧义问题
>
> **3、隐马模型HMM：**
> n元模型能够解决歧义问题，但是不能很好的解决未登录词，提出了基于标注(SBME)的分词算法，在分词上取得了比较高的准确率，HMM就是一个字标注的分词算法
>
> **4、最大熵模型ME：**
> 最大熵模型就是在已知条件下，求熵最大的情况，给定的条件是样本期望等于模型期望。最大熵模型的一个好处是我们可以引入各种各样的特性，而不仅仅是从词频去分词，比如我们加入已知的字典信息等，
> 最大熵模型的问题就是把每个字的标注问题分裂来看了
>
> **5、最大熵马尔科夫模型MEMM：**
> 结合了最大熵和马尔科夫，不仅可以利用最大熵的各种特性，而且加入了序列化的信息，使得能够从整个序列最大化的角度来处理，而不是处理每个字
>
> **6、条件随机场CRF：**
> MEMM的缺点就是马尔科夫链的假设，假设每个状态只与他前面的状态有关，这样的假设的有偏差的，于是有了CRF，使得每个状态不止与他前面的状态有关，还与他后面的状态有关，HMM是基于贝叶斯网络的有向图，CRF是无向图。因为条件随机场可以像最大熵那样加入各种特性，又没有马尔科夫链那样的偏执假设，近年来CRF被认为是最好的分词算法。
>

- **判别式和生成式模型区别：**
> **生成式模型：**
>       无穷样本-->概率密度模型 = 产生模型-->预测
>       如果我根据训练集，统计出一个概率密度模型，**运用统计概率**，然后通过模型进行预测，那么就是一个生成式模型
>       相关算法：朴素贝叶斯、马尔科夫随机场
>
>**判别式模型：**
>       有限样本-->判别函数 = 预测模型-->预测
>       如果我写一个判别函数，通过训练集，通过**迭代求参**，求出模型的各个参数的最优解，从而得到一个模型，在通过模型预测，那么就是一个判别式模型
>       相关算法：SVM、逻辑回归、条件随机场
>
