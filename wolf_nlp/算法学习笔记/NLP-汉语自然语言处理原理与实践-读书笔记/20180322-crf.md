### NLP相关算法-CRF
- **概述：**
>       条件随机场最早用于NLP领域，主要用于文本标注，并可以在分词(标注字的词位信息，由字构词)、词性标注(标注分词的词性，如：名词、动词等)、命名实体识别(识别人名、地名、机构名等实体名词)
>

- **应用场景：**
>       句法分析、序列标注、数据分割、组块分析等自然语言处理中，在中文分词、人名识别、歧义消除等任务表现良好，在小规模的中文分词、词性标注、未登录词识别、命名实体识别、语义组块、语义角色标注
>

- **CRF流程：**
>       1、将分词语料的标注符号转化为用于命名实体序列标注
>       2、确定特征模板，一般采用当前位置前后n个位置的字及其标记表示，即以当前位置的前后n个位置范围内的子串及其标记作为观察窗口(一般n为2~3)
>       3、得到相应的特征函数，剩下的就是训练CRF模型的参数λ和u
>
>
>

- **模板：**
>       CRF一共有两种模板u-gram和b-gram，第一个字符为U表示unigram template，第二个字符B表示bigram template
>       **u-gram模板：**
>           u-gram就是unigram template，描述了unigram feature
>           一元模板，表示只与当前位置对应的标签相关的特征
>       **b-gram模板：**
>           b-gram：就是bigram template
>           二元模板，表示前一个位置和当前位置对应的标签相关的特征
>           注：当类别数很大的时候，这种类型会产生许多可区分的特征，这将会导致训练和测试的效率都很不好
>
>       **实例讲解：**
>           使用crf++中文分词时，使用的参数：
>               crf_learn -f 3 -c 4.0 template 6.train.data 6.model > 6.train.rst
>               crf_test -m 6.model 6.test.data > 6.test.rst
>
>           使用crf进行命名实体识别：
>               crf_learn -f 4 -p 4 -c 3 template train.data model > train.rst
>               crf_test -m model test.data > test.rst
>
>           使用crf进行词性标注：
>               crf_learn -f 3 -p 4 -c 4.0 template train.data model > train.rst
>               crf_test -m model test.data > test.rst
>
>       **模板如下：**
>            # Unigram
>            U00:%x[-2,0]
>            U01:%x[-1,0]
>            U02:%x[0,0]
>            U03:%x[1,0]
>            U04:%x[2,0]
>            U05:%x[-2,0]/%x[-1,0]/%x[0,0]
>            U06:%x[-1,0]/%x[0,0]/%x[1,0]
>            U07:%x[0,0]/%x[1,0]/%x[2,0]
>            U08:%x[-1,0]/%x[0,0]
>            U09:%x[0,0]/%x[1,0]
>            # Bigram
>            B
>           说明：
>               每个特征后面的数字用于区分特征，这些数字不是一定要连续
>               假设我们的训练语料句子是：我是中国人(下表：-2,-1,0,1,2)，我们考虑的当前位置为：中
>               U00-U04特征模板：表示某个位置与当前位置的信息之间的关系，比如U00，指的是"我"和"中"之间的联系
>               U05-U07特征模板：表示某三个位置与当前位置的信息之间的关系，比如U05，指的是"我"、"是"、"中"和"中之间的联系"
>               U08-U09特征模板：表示某两个位置与当前位置的信息之间的关系，比如U08，指的是"是"、"中"和"中"之间的联系
>               注：一般使用unigram就足够了，若使用bigram，也使用最简单的模板，它会带来效率低下的问题
>
>               T**:%x[#,#]中的T表示模板类型，两个"#"分别表示相对的行偏移与列偏移。
>               **第一种函数**：每一行%x[#,#]生成一个CRFs中的**点函数**：f(s,o)，其中s为t时刻的标签，o为t时刻的上下文
>               如句子中：
>                       那 S
>                       音 B
>                       韵 E
>                       如 S
>                       轻 B
>                       柔 E
>                       的 S
>                       夜 B
>                       风 E
>                       ， S
>               根据上面的模板可以得到下面的函数：
>               func1 = if (output = B and feature="U02:那") return 1 else return 0
>               这个是根据模板 U02:%x[0,0] ，在输入文件的第一行生成的点函数，将输入文件的第一行带入到函数中，返回1,
>               同时，如果输入文件的某一行在第一列也是"那"，并且它的output(第二列)也是B,那么函数也返回1
>
>               **第二种函数**：第一个字符是B，每一行%x[#,#]生成一个CRFs中的边(Edge)函数：f(s',s,o)，其中s'为t-1时刻的标签，即Bigram类型和Unigram大致相同，
>                   只是还要考虑到t-1时刻的标签，如果致谢一个B的话，默认生成f(s',s)，这意味着前一个output token和current token将组合成bigram features
>
>           注：比如做词性标注工作的时候，我们知道"动词后边很容易跟名词，所以某个位置的词性与其附近的词性有很大的关系"，所以说这种情况下，动词后边名词的概率会变高，指引我们"动词后边名词的概率很大"
>
>


- **特点：**
>       <b>CRF VS 词典分词：</b>
>           优点：对汉字进行标注即由字构词(组词)，不仅考虑了文字词语出现的频率信息，同时考虑上下文信息，具备较好的学习能力，对歧义词和未登录词识别有良好的效果
>           缺点：训练周期长，计算量大，模型较大，性能不够好
>
>       <b>CRF VS HMM、MEMM：</b>
>           CRF、HMM、MEMM(最大熵隐马模型)常用来做序列标注的建模，如分词、词性标注、命名实体
>
>       <b>各自特点：</b>
>
>       **隐马模型：**
>           缺点是其独立性假设，导致其不能考虑上下文的特征，限制了特征的选择。
>           HMM模型是对转移概率和表现概率直接建模，统计共现概率
>
>       **最大熵隐马模型：**
>           可以任意选择特征，解决了隐马模型的问题，但是要在每一个节点都要进行归一化，所以只能找到局部最优值，同时也存在标记偏见的问题，即凡是在语料中未出现的情况全部忽略。
>           MEMM模型是对转移概率和表现概率建立联合概率，统计条件概率。容易陷入局部最优解，因为只在局部做归一化
>
>       **条件随机场模型：**
>           不在每一个节点进行归一化，而是搜优特征进行全局归一化，可以求得全局最优解。CRF模型中统计全局概率，做归一化时，考虑了数据在全局的分布，而不仅是局部归一化，这样就解决了MEMM中标记偏执问题。
>           训练时：CRF是在给定需要标记的观察序列的条件下，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布
>
>       <b>实例讲解：</b>
>           对于一个标注任务，"我爱北京天安门"，标注为" s s b e b m e"
>           **HMM：**判断标注成立的概率为：P=P(s转移到s)*P('我'转移到s)*P(s转移带b)*P('爱'转移到b)*...*P()。训练时，要统计状态转移概率矩阵和表现矩阵
>           **MEMM：**判断标注成立的概率为：P=P(s转移到s|'我'表现为s)*P('我'表现为s)*P(s转移到b|'爱'表现为s)*P('爱'表现为s)*...*P()。训练时，要统计条件状态转移概率矩阵和表现矩阵
>           **CRF：**判断标注成立的概率为：P=F(s转移到s，'我'表现为s)...，F是一个函数，是在全局范围统计归一化的概率而不是想MEMM在局部统计归一化概率
>
>

- **模型训练结果文件说明：**
>       对于最后训练出来的model.txt文件：
>           文件头：
>               模型版本、特征函数最大id、特征维数等
>           标签：
>               BEMS即为最终输出
>           模板：
>               训练时候用到的模板
>           特征函数：
>               按照[id] [参数o]的格式排列，f(s,o)应该接受两个参数，但是s隐藏起来了，注意到id不是连续的，而是间隔四个，这表示四个标签(s=BEMS)和公共的参数o组合成了四个特征函数，
>               特别的，0-15为BEMS转移到BEMS的转移函数，也就是f(s',s,o=null)
>           特征函数权值：
>               按照顺序列出了每个特征函数的权值
>               CRF的解码：
>                   特征函数的权值和解码的关系
>                   比如：有一个句子"商品和服务"，对于每个字都按照上述模板生成一系列U特征函数的参数带入，得到一些类似010101的函数返回值，乘上这些函数的权值求和，就得到了各个标签的分数，从达到小输出这些标签的可能性
>                   至于B特征函数（特征简单的f(s',s)），在viterbi向后解码的时候，前一个标签确定了后就可以代入当前的B特征函数，计算出每个输出标签的分数，再次求和排序即可
>
>

- **分词原理：**
>       **概述：**
>           先将语料进行分词，然后对分词的结果进行词位标注，然后通过训练**得到CRF模型**，最后通过CRF模型对新的文本进行词位标注，进行分词
>           1、CRF把分词当做字的词位分类问题，通常定义词位信息为EBMS，
>           2、分词的过程就是对词位标注，然后进行分词
>
>           3、分词实例：
>               原句：原始例句：我爱北京天安门
>               CRF词位标注：我/S 爱/S 北/B 京/E 天/B 安/M 门/E
>               分词结果：我/爱/北京/天安门
>

- **各种RF：**
>       马尔科夫随机场(MRF)：
>           典型的马尔科夫网，即使用无向边表示变量间的依赖关系
>           马尔科夫随机场通过分离集来实现条件独立
>
>       条件随机场(CRF)：
>           隐马模型和马尔科夫随机场都属于生成式模型，即对联合概率进行建模，条件随机场是对条件分布进行建模
>           CRF和HMM的解码都是在给定观测值序列后，研究状态序列可能的取值，CRF可以有多种结构，只需保证状态序列满足马尔科夫性即可，常用的是链式条件随机场
>
>       linear-CRF：线性链条件随机场
>           条件随机场是给定一组输入序列条件下另一组输出序列的条件概率分布模型，在NLP中应用广泛
>           三个基本问题：
>               1、评估问题：
>                   给定linear-CRF的条件概率分布P(y|X)，在给定输入序列X和输出序列y时，计算条件概率P(yi|x)和P(yi-1,yi|x)以及对应的期望
>               2、学习问题
>                   给定训练集合X和Y，学习模型参数wk和条件概率Pw(y|x)，一般的梯度下降法、牛顿法等都可以解决
>               3、解码问题
>                   给定条件概率分布P(y|x)和输入序列x，计算使条件概率最大的输出序列y，使用viterbi算法可以解决
>
>
>


- **待续：**
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
