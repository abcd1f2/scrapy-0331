### NLP汉语自然语言处理原理与实践-情感分析
- **概述：**
>       目前工业应用现状：
>           目前主要的情感分析方法是基于情感词典的，以及基于监督或半监督学习的。
>
>       1、确定一个词是积极还是消极，是主观还是客观，主要依赖词典
>           英文中有资源较全的词典资源，SentiWordNet，含有积极消极、主观客观和情感强度值
>           中文领域有一些资源，但是做的不是很完善，质量不高、不细致，缺乏主客观词典
>
>       2、识别一个句子是积极还是消极，主观还是客观，
>           词典方法：当有情感词典时，直接匹配进行计算
>           机器学习方法：将分好的积极消极数据使用算法进行训练，训练得到的分类器就可以进行分类是积极还是消极
>                       但是判断主客观比较麻烦，一般需要人工标注
>
>       3、情感挖掘升级到意见挖掘
>           这一步需要从评论中找出产品的属性，比如手机的屏幕、电池等都是它的属性，然后需要分析评论是如何评价这些属性的。
>
>       情感分析的主要方法：
>           1、基于词典的
>               短文本中，基于词典的情感分析效果更好
>               通过对文本进行段落拆分、句法分析、计算情感值
>               由于不同词汇组合起来能达到不同的情感程度或者相反的情感极性，所以以句子为最基本的情感分析粒度较为合理
>           2、基于机器学习的
>               适合长文本的情感分析
>               类似于分类问题，进行有监督学习
>

- **基于情感词典的情感分析：**
>       目前工业应用现状：
>           目前主要的情感分析方法是基于情感词典的，以及基于监督或半监督学习的。
>           基于情感词典：
>               主要是利用事先准备好的情感词库，给每个词以相应的情感倾向权重，然后从文本中提取出所有的情感词并根据句子特点（反问句、疑问句等）计算最后的情感得分，判断极性
>
>           1、如何构建情感词典，如何判断一个词是积极还是消极？
>               a、一种是基于语义计算，
>                   一般可根据《知网》情感词计算语义相似度，计算目标词语跟基准词语之间的紧密程度，然后判断情感极性。
>                   原理：
>                       基于语义计算的情感词构建，核心是如何构建基础情感词，然后由目标词与这些基础情感词做语义相似度计算。
>                       比如有人工标记的《知网(HowNet)》、台湾大学NTUSD、《学生褒贬义词典》、《褒义词词典》、《贬义词词典》 情感词典
>                       我们在构建自己的情感词典时，可以综合几种不同的情感词典，我们可以对情感极性根据几个词典进行投票，得到这个词的最后极性
>                   计算文档的极性：
>                       先对预测的文档进行分词，然后扫描，与情感词典中的词进行HowNet语义相似度计算，当达到一定阈值可以判断改词的情感倾向和权重
>
>               b、基于统计分析，
>                   计算目标词语与基准词语的点间互信息值，确定两个词之间的紧密程度，从而获取目标词的情感倾向
>                   原理：
>                       基于统计的情感词典构造也是需要事先准备一套情感极性明显的基础情感词典，然后由目标词与该基础情感词典中的词计算点间互信息，一般使用SO-PMI算法，
>                       然后由点间互信息值来判断目标词的极性
>                   点间互信息：
>                       互信息是非常重要的信息度量，在实际应用中最广泛的通常是点间互信息（PMI），用于计算词语间的语义相似度。！！！
>                       基本思想：
>                           统计两个词语在文本同时出现的概率，如果概率越大，相关性越久越紧密，关联度越高。
>                           PMI = log(P(w1 & w2)/(P(w1)*P(w2)))
>                               P(w1 & w2) 表示两个词共同出现的概率
>                               P(w1),P(w2) 分别表示两个词单独出现的概率
>
>           基于监督或半监督：
>               主要是基于神经网络、深度学习的一些方法将文档转换成向量后，利用SVM、Bayes等分类器去分类，从而判断文档的情感倾向。
>


- **互信息、点互信息、点间互信息：**
>       互信息：
>       点互信息：
>       点间互信息：
>
>
>
>
>
>
>
>
>
>
>
>
>
>


- **实践：**
>       使用贝叶斯训练数据，进行分类
>       可能会出现共线性问题，自变量之间出现共线性，说明自变量所提供的信息是重叠的，处理的方法：
>       1、删除不重要的自变量减少重复信息，但从模型中删除自变量时应该注意为相对不重要并从偏相关系数检验证实为共线性原因的那些变量中删除。
>           如果删除不当，会产生模型设定误差，造成参数估计严重有偏的后果。
>       2、多重共线性问题的实质是样本信息的不充分而导致模型参数的不能精确估计，因此追加样本信息是解决问题的一条有效途径。但是实际工程中，这个方法效率不高（样本数据不好获取）。
>       3、利用非样本先验信息
>       4、改变解释变量的形式
>       5、逐步回归法（最常用也最有效）
>           逐步回归是一种常用的消除多重共线性、选取最优回归方程的方法
>           当数据的特征数高于样本数，或者特征之间高度相关时，会导致X^TX奇异，从而限制了LR和LWLR的应用。这时需要考虑使用缩减法。
>           缩减法：
>               可以理解为对回归系数的大小施加约束后的LR，也可以看作是对一个模型增加偏差（模型预测值与数据之间的差异）的同时减小方差（模型之间的差异）。
>               一种是岭回归（L2），一种是lasso法（L1），由于计算复杂，一般用效果差不多但是实现容易的前向逐步回归法
>           前向逐步回归：
>               前向逐步回归算法是一种贪心算法，每一步都尽可能减小误差，一开始，所有权重都设置为1，然后每一步所做的决策是对某个权重增加或减少一个很小的值
>           主要基于假设：
>               在线性条件下，哪些变量组合能够解释更多的因变量变异，则将其保留
>               首先模型中只有一个单独解释因变量变异最大的自变量，之后尝试将加入另一自变量，看加入后整个模型所能解释的因变量变异是否显著增加；这一过程反复迭代，直到没有自变量再符合加入模型的条件。
>
>       6、主成分回归
>           主成分分析作为多元统计分析的一种常用方法在处理多变量问题时具有一定的优越性，对于一般的多重共线性问题适用，尤其是对共线性较强的变量之间。当采取主成分提取了新的变量后，
>           往往这些变量的组内差异小而组间差距大，起到了消除共线性问题
>
>

- **总结：**
>       （从Twitter的一次情感分析中得出的一些总结）
>       朴素贝叶斯：是最简单且速度最快的分类器，许多研究表明这个分类器取得了最好的效果，使用高阶的n元词组提高准确率
>               一元词串+二元词串+三元词串为特征并使用朴素贝叶斯分类器进行训练，准确率最高
>       最大熵模型：基于一元、二元和三元词的分类器准确率最高
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>

- **待续：**
>       参考：https://www.zybuluo.com/evilking/note/1012623
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
