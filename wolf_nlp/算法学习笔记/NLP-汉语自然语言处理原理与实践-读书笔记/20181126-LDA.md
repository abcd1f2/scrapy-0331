### NLP - LDA算法
- **概述：**
>
>       当我们看到一篇文章后，往往喜欢推测这篇文章是如何生成的，我们可能认为作者先确定这篇文章的几个主题，然后围绕这几个主题遣词造句，形成文章
>       LDA：根据给定的一篇文档，反推其主题分布 ！！！！
>           利用看到的文档推断其隐藏的主题（分布）的过程（就是产生文档的逆过程），便是主题建模的目的：自动的发现文档集合中的主题。！！！！
>       人类是根据文档生成过程写成了各种各样的文章，计算机利用LDA推测文章分布都写了什么主题，而且各篇文章各个主题出现的概率大小（主题分布）是什么。！！！！
>

- **LDA文本分类：**
>
>       将文档集中每篇文档的主题以概率分布的形式给出，从而通过分析一些文档抽取它们的主题(分布)出来后，便可以根据主题(分布)进行主题聚类或文本分类。
>       LDA是一种典型的词袋模型，即每一篇文档是由一组词构成，词与词之间没有先后顺序关系。
>       LDA的作用：
>           LDA的作用就是根据给定一篇文档，反推其主题分布。
>
>
>
>

- **LDA原理：**
>       LDA中的一篇文档生成过程如下：（先从狄利克雷先验中"随机"抽取出主题分布，然后从主题分布中随机抽取主题，最后从确定后的主题对应的词分布中随机抽取出词）
>           （随机选主题也是根据主题分布来随机选取，这里的随机不是完全随机的意思！！！！）
>           1、从狄利克雷分布α中取样生成文档i的主题分布θi
>           2、从主题的多项式分布θi中取样生成文档i第j个词的主题Zi,j
>           3、从狄利克雷分布β中取样生成主题Zi,j对应的词分布Φzi,j
>           4、从词语的多现实分布Φzi,j中取样最终生成词Wi,j
>
>       Beta分布是二项式分布的共轭先验概率分布，
>       而狄利克雷分布是多项式分布的共轭先验概率分布。
>
>       样本固定，参数未知但不固定，是个随机变量，服从一定的分布，所以LDA属于贝叶斯派思想。！！！！！！
>
>       贝叶斯思考问题的固定模式：
>           先验分布π(θ) + 样本信息X => 后验分布π(θ|X)
>           新观测到的样本信息将修正人们以前对事物的认知。
>
>       频率学派和贝叶斯学派不同的思考方式：
>           频率学派：频率学派把推断的参数θ看做是固定的未知常数，即概率θ虽然是未知的，但起码是确定的一个值，同时样本X是随机的，所以频率学派重点研究样本空间，大部分的概率计算都是针对样本X的分布
>           贝叶斯学派：观点和频率学派截然相反，他们认为带估计的参数θ是随机变量，服从一定的分布，而样本X是固定的，由于样本是固定的，所以他们研究的重点是参数θ的分布
>
>       文档-词语概率P(wj|di)：
>           P(wj|di) = ∑P(wj|zk)P(zk|di)
>       则文档中每个词的生成概率为：
>           P(di,wj)= P(di)*∑P(wj|zk)P(zk|di)
>           由于P(di)可以计算出来，而P(wj|zk)和P(zk|di)未知，所以θ=(P(wj|zk),P(zk|di))就是我们要估计的参数值，即就是要最大化θ。
>           用什么方法估计呢？
>               常用的参数估计方法有极大似然估计MLE、最大后验估计MAP、贝叶斯估计等等。
>               因为待估计的参数中含有隐变量z，所以我们可以考虑EM算法。
>

- **Gibbs采样：**
>
>       LDA的原始论文中使用变分法和EM算法进行贝叶斯估计的近似推断，但不太好理解，而且EM算法可能推导出局部最优解。Heinrich使用了Gibbs抽样法，这也是目前LDA的主流算法。
>       通常在均匀分布中，即我们熟悉的rand()函数，可以由线性同余发生器生成。而其他的随机分布都可以在均匀分布的基础上，通过某种函数变换得到。比如，正太分布可以通过Box-Nuller变换得到。
>           然后变换依赖于计算机目标分布的积分的反函数，当目标分布的形式很复杂，或者是高维分布时，很难简单变换得到。
>       当一个问题无法用分析的方法来求精确值，此时通常只能去推断该问题的近似解，而随机模拟(MCMC)就是求解近似解的一种强有力的方法。随机模拟的核心就是对一个分布进行抽样。！！！！
>       MCMC基础：Markov链通过转移概率矩阵可以收敛到稳定的概率分布。这意味着MCMC可以借助Markov链的平稳分布特性模拟高维概率分布p(x)。当Markov链经过burn-in阶段，消除初始参数的影响，到达平稳状态后，
>           每一次转移都可以生成待模拟分布的一个样本。Gibbs抽样是MCMC的一个特例，它交替的固定某一维度，然后通过其他维度的值来抽样该维度的值。
>       基本算法：
>           1、选择一个维度i，可以随机选择
>           2、根据分布p抽样xi
>
>       算法：三个阶段（初始化、burn-in、sampling）
>           1、设置全局变量
>           2、对所有文档m∈[1,M]：
>               对文档m中的所有单词n∈[1,Nm]：
>                   初始化每个单词对应的主题Zm,n=k∼Mult(1/K)
>                   增加文档-主题计数：nmk += 1
>                   增加文档-主题总数：nm += 1
>                   增加主题-词项计数：nkt += 1
>                   增加主题-词项总数：nk += 1
>           3、迭代上面的步骤，直到Markov链收敛
>               对所有文档m∈[1,M]：
>                   对文档m中的所有单词n∈[1,Nm]：
>                       删除该单词的主题计数：n(k)m−=1; nm−=1; n(t)k−=1; nk−=1;
>                       根据公式采样出该单词的新主题：k~p()
>                       增加该单词的新主题计数：n(k~)m+=1; nm+=1; n(t)k~+=1; nk~+=1;
>
>

- **LDA和pLSA比较：**
>       pLSA中，主题分布和词分布是唯一确定的，能明确的指出主题分布可能就是{教育：0.5，经济：0.3，交通：0.2}，
>           词分布可能就是{大学：0.5，老师：0.3，课程：0.2}
>       LDA中，主题分布和词分布不再唯一确定不变了，即无法确切给出。例如主题分布可能是{教育：0.5，经济：0.3，交通：0.2}，也可能是{教育：0.6，经济：0.2，交通：0.2}，到底是哪个我们不再确定（即不知道）
>           因为它是随机的可变的。但是不管怎么变化，也依然服从一定的分布，即主题分布和词分布由狄利克雷先验随机确定。
>
>       从上面可以看出，LDA在pLSA的基础上，为主题分布和词分布分别加了两个狄利克雷先验。
>       LDA只是pLSA的贝叶斯版本（加了两个先验分布的参数，即贝叶斯化），一个主题分布的先验分布是狄利克雷的α分布，一个词语的先验分布是狄利克雷的β分布。
>       两个都要根据文档去推断其主题分布和词语分布（两个本质都是为了估计给定文档生成主题，给定主题生成词语的概率），
>       只是用的推断方法不同：
>           pLSA用极大似然估计的思想去推断两未知的固定参数，频率学派思想
>           LDA则把这两参数弄成随机变量，且加入狄利克雷先验，贝叶斯学派思想
>
>       参数估计：
>           pLSA使用EM算法估计主题-词项矩阵Φ和文档-主题矩阵Θ参数，使用的思想就是极大似然估计MLE
>           LDA中，估计Φ、Θ这两个未知参数可以使用变分-EM算法，也可以使用gibbs采样。
>               变分-EM算法的思想是最大后验估计MAP（MAP与MLE类似，都把未知参数当做固定的值）
>               gibbs采样的思想是贝叶斯估计。（贝叶斯估计是对MAP的扩展，与MAP有着本质的不同，即贝叶斯估计把待估计参数看作是服从某种先验分布的随机变量）
>               （LDA的原始论文中是用变分-EM算法估计未知参数，后来发现另一种估计LDA未知参数的方法更好，就是Gibbs采样。）
>               （Gibbs采样是马尔科夫链蒙特卡尔理论中用来获取一系列近似等于指定多维概率分布（比如2个或者多个随机变量的联合概率分布）观测样本的算法）
>
>
>
>
>

- **EM算法基本思想：**
>       基本思想：
>           首先随机选取一个值去初始化待估计的参数值θ(0)，然后不断的迭代寻找更优的参数θ(n+1)，使得似然函数likelihoodL(θ(n+1))比原来的L(θ(n))要大。
>           EM的关键就是要找到L(θ)的一个下边界Q(θ;θ(n))，然后不断极大化下边界函数Q，从而逼近要求解的似然函数L(θ)
>
>       E-step：
>           假定参数已知，计算此时隐变量的后验概率
>       M-step：
>           带入隐变量的后验概率，最大化样本分布的对数似然函数，求解相应的参数。
>
>       在凸优化问题中，有目标函数，有约束条件的极值问题，常用的方法就是拉格朗日乘子法，引入拉格朗日乘子将约束条件和目标函数融合在一起，转化为无约束条件的极值问题。
>       然后分别对参数求偏导，令偏导为0，即可得到极值。
>
>       算法原理：
>           1、随机选择或根据先验知识初始化0(0)
>           2、不断迭代：
>               a、给出当前的参数估计0(n)，计算似然函数L(0)的下边界函数Q(0;0(n))
>               b、重新估计参数0，即求0(n+1)，使得0(n+1)=argmaxQ(0;0(n))
>           3、重复第二步骤，直到L(0)收敛
>
>
>

- **pLSA原理：**
>       pLSA模型下生产文档：
>           1、k面的文档-主题骰子和V面的主题-词语骰子
>           2、先动文档-主题骰子中选择主题，然后从主题-词语筛子中选择词语
>               选择主题和选择词语的时候，都是随机选择，但是这个随机选择遵循一定的概率分布。
>           3、循环的用文档-主题骰子和主题-词语骰，重复N次（产生N个词），完成一篇文档，重复这一方法M次，完成M篇文档
>           上述过程抽象出来即是pLSA的文档生成模型。在这个过程中，我们并未关注词与词的出现顺序，所以pLSA是一种词袋模型。
>
>       样本随机，参数虽未知但固定，所以pLSA属于频率派思想。！！！
>
>       pLSA算法原理：
>           1、由于P(wj|zk)和P(zk|di)未知，然后使用EM算法去估计参数θ=(P(wj|zk),P(zk|di))
>           2、将P(wj|zk)转换成主题-词语矩阵Φ，将P(zk|di)转成文档-主题矩阵Θ
>           3、最终求出参数
>
>
>
>
>

- **待续：**
>       参考：https://blog.csdn.net/v_july_v/article/details/41209515  通俗理解LDA主题模型
>           http://blog.jqian.net/post/lda.html
>           https://zhuanlan.zhihu.com/p/31470216   一文详解LDA主题模型
>           https://www.cnblogs.com/pinard/p/6831308.html
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
