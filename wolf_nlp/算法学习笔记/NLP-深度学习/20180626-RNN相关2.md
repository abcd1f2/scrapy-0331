### 深度学习 - 神经网络基础2
- **线性单元：**
>       感知器有一个问题，当面对的数据集不是线性可分的时候，感知器规则可能无法收敛，这意味着我们永远无法完成一个感知器的训练
>           解决方法：使用一个可导的线性函数来替代感知器的阶跃函数，这种感知器就叫做线性单元
>           替换激活函数后，线性单元将返回一个实数值而不是0,1分类。因此线性单元用来解决回归问题，而不是分类问题。
>
>       线性模型：
>           y = W*X
>           输出y就是输入特征x1,x2,x3...xn的线性组合
>
>       无监督和有监督：
>           ticks：很多工程中，既有X也有y的训练样本是很少的，大部分只有X，为了弥补标注样本的不足，可以用无监督学习方法先做一些聚类，然后用少量的带标注的训练样本，去训练模型
>
>       目标函数：
>           就是模型总的误差函数，每个样本的误差之和
>
>       优化问题：
>           模型的训练，就是求到合适的权重W，是的目标函数最小，数学上称为优化问题
>
>

- **梯度下降优化算法：**
>       概述：
>           数学中使用求导计算函数的极值，但是计算机不能求导，只能通过迭代一步一步逼近极值点
>           为什么每次都是像函数的梯度的相反方向修改x？
>               梯度是一个向量，指向函数值上升最快的方向
>               W = W - η*▽E(W)
>               经过推导得到 ▽E(W) = -∑(y-y')x
>               ▽E(W)就是指E(W)的梯度
>               η是步长，即学习率
>               求最小值就是网梯度下降的方向移动，求最大值就是网梯度上升的方向移动
>
>

- **随机梯度下降算法：（SGD）**
>       批梯度下降：（BGD）
>           每次更新W的时候，需要遍历训练数据中所有的样本进行计算
>       随机梯度下降：（SGD）
>           在SGD算法中，每次更新W的迭代，只计算一个样本。由于样本的噪声和随机性，每次更新W并不一定按照减少E的方向。然而，虽然存在一定随机性，大量的更新总体上沿着减少E的方向前进，因此最后也能收敛到最小值附近。
>
>

- **总结：**
>       一个机器学习算法其实只有两部分：
>           1、模型从输入特征X预测y的函数f(x)
>           2、目标函数取极值所对应的参数值，就是模型的参数的最优值。很多时候我们只能获得目标函数的局部极值，只能得到模型参数的局部最优解
>
>       优化算法求目标函数的极值：
>           针对同一个目标函数，不同的优化算法会推导出不同的训练规则。
>           1、批梯度下降
>           2、随机梯度下降
>
>       特点：
>           机器学习中，关键在于特征提取
>           神经网络算法的一个优势：能够自动学习到应该提取什么特征，从而使算法不再那么依赖人工
>
>
>
>
>
>
>
>
>

- **待续：**
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
