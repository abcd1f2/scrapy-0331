### 深度学习 - 神经网络基础4
- **概述：**
>       语言模型是对一种语言的特征进行建模，给定一句话的前面，预测后面的词是什么，用处广泛（语音转文本、声学模型、图像到文本等）
>       N-gram：
>           在使用RNN之前，语言模型主要采用N-gram，但是当需要处理任意长度的句子时，比如我 昨天 上学 迟到 了 ，老师 批评 了 ____，N设置多少都不合适
>       RNN：
>           U:输入层到隐藏层的权重矩阵
>           V:隐层到输出层的权重矩阵
>           W:隐层上一次的结果作为这次输入的权重矩阵
>           xi：输入
>           si：隐层状态值
>           oi：输出
>           公式：
>               1、o = g(Vst)  输出
>               2、st = f(Uxt + Ws(t-1))  状态值
>               将2带入到1中，最后可以得到 o 与前面的xt,xt-1,xt-2....都有关系，这就是为什么RNN可以往前看多个输入值的原因
>       双向循环网络：
>           语言模型中，很多时候只看前面的词是不够的，比如：我的手机坏了，我打算____一部新手机。
>           三个公式：
>               o = g(Vst + V's't)
>               st = f(Uxt + Ws(t-1))
>               s't = f(U'xt + W's'(t+1))
>           可以看出：正向计算时，隐藏层的值st与st-1有关；
>                   反向计算时，隐藏层的值s't与s't+1有关
>                   最终的输出取决于正向和反向计算的加和
>           注意：正向计算和反向计算不共享权重，即正向的三个权值矩阵和反向的权值矩阵不同
>
>       深度循环神经网络：
>           当隐层大于1个时，就得到了深度循环神经网络
>
>       RNN的训练：
>           BPTT算法是针对循环层的训练算法，基本原理和BP算法一样
>           训练算法BPTT：
>               1、前向计算每个神经元的输出
>               2、反向计算每个神经元的误差项δi值，它是误差函数E对神经元j的加权输入netj的偏导数
>               3、计算每个权重的梯度
>               最后用随机梯度下降算法更新权重
>
>       梯度消失和梯度弥散：
>           根据公式推导可以看出，最后的误差是一个指数函数，当指数很大时，会导致误差项的值增长或缩小的非常快，这样会导致爆炸或消失的问题(取决于底数是大于1还是小于1)
>           处理办法：
>               梯度爆炸：
>                   梯度爆炸相对比较好处理，可以设置一个梯度阈值，当梯度超过这个阈值直接截取
>               梯度消失：
>                   **标准的RNN能够存取的上下文信息范围很有限。这个问题就使得隐含层的输入对于网络输出的影响随着网络环路的不断递归而衰减**
>                   1、合理初始化权重值，使每个神经元尽可能不要极大或极小值
>                   2、使用ReLU替代sigmoid或tanh
>                   3、使用其他结构的RNN，比如LSTM或GRU
>           softmax层：
>               多分类问题
>               softmax计算出每个词是当前预测的概率
>
>       RNN语言模型的训练：
>           输入-输出标签对
>           使用one-hot对x和标签y进行向量化
>           最后使用交叉熵误差函数作为优化目标（也可以使用其他的误差函数，一般对概率进行建模时，选择交叉熵误差函数更好），对模型进行优化
>
>
>
>
>

- **待续：**
>
>
>
>
>
>
>
>
>
>
>
>
>
>
