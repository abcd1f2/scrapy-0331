### 深度学习 - 神经网络基础-LSTM
- **概述：**
>       由于梯度爆炸或消失的原因导致RNN很难训练，导致了它在实际应用中，很难处理长距离的依赖
>       LSTM：
>           1、改进的RNN，成功解决了原始RNN的缺陷，
>           2、但是LSTM结构复杂
>       GRU：
>           1、结构比LSTM好
>           2、效果和LSTM一样好
>
>       递归神经网络：
>           有时候仅仅拥有处理序列的能力还不够，还需要处理比序列更为复杂的结构（树结构）-递归神经网络（RNN）
>

- **LSTM：**
>       在RNN中，误差项δ从t时刻传递到k时刻，当t-k很大时，前面提到的优化技巧效果都不够好
>       从前面公司中，权重数组W最终的梯度是各个时刻梯度之和，当从某时刻开始，得到的梯度几乎为0时，后面的梯度就不会对最终的梯度有比较大的贡献，这就是原始RNN无法处理长距离依赖的原因
>       LSTM：
>           原始RNN的隐层只有一个状态s，对于短期输入非常敏感
>           LSTM增加了一个状态c，来保存长期的状态
>           **三个输入两个输出：**
>               X：当前时刻的输入
>               ht-1：上一时刻的输出
>               ct-1：上一时刻的输出
>               ht：当前时刻的输出值
>               ct：当前时刻的单元状态
>
>           **三个开关：**
>               开关1（遗忘门）：负责控制继续有多少保存上一时刻的长期状态c到当前状态
>               开关2（输入门）：负责把当前网络的输入有多少保存到长期状态c
>               开关3（输出门）：负责把长期状态c有多少作为当前的输出
>
>               开关：
>                   开关在算法中通过门来实现，门实际上就是一层全连接层
>
>               输出公式：
>                   ot = δ(W*(ht-1, xt) + b)
>                   ht = ot · tanh(ct)
>
>       训练：
>           训练算法框架：
>               LSTM的训练算法仍然是BP算法
>               **训练算法步骤：**
>                   1、前向计算每个神经元的输出值，
>                   2、反向计算每个神经元的误差项δ值，与RNN一样，反向传播包括两个方向：
>                       一个是时间的反向传播，即从当前时刻开始，计算每个时刻的误差项
>                       一个是将误差项向上一层传播
>                   3、根据相应的误差，计算每个权重的梯度
>           LSTM需要学习的参数共8组：
>               遗忘门的权重矩阵Wf和偏置bf
>               输入门的Wi和bi
>               输出门的Wo和bo
>               计算单元状态的Wc和bc
>
![avatar](https://github.com/nwaiting/wolf-ai/blob/master/wolf_others/pic/lstm.png)
>       针对图进行说明：
>           1、BasicLSTMCell(num_units, forget_bias=0.0, state_is_tuple=True)中的num_units就是这个层的隐藏神经元个数
>           2、cell的状态是一个向量，是有多个值的
>           3、上一次的状态h(t-1)和下一次的输入x(t)进行结合，就是简单的直接拼接，如x(t)为28维，h(t-1)为128维，则拼接结果就是156维
>           4、在单向的lstm cell的权重是共享的，网上一般的图形中展示了多个cell的级联，其实只是代表了一个cell在不同时序时候的状态，所有的数据只会通过一个cell，然后不断更新他的权重
>           5、一层的lstm参数计算：参数的数量是由cell的数量决定的，如何计算一个cell里面的参数个数
>               假设num_units是128位，输入是28位的，那么根据第3点的拼接，四个小黄框(遗忘门、输入门、输出门、计算单元状态)一共有(128+28)*(128*4)，即156*512，
>               即TensorFlow中最简答的BasicLSTMCell的中间层参数就是这样，还要加上输出时候的激活函数的参数，假设10个分类的话，就是128*10的W参数和10个bias参数
>           6、cell最上面的一条线的状态s(t)代表了长时记忆，下面的h(t)代表了工作记忆或短时记忆
>
>
>
>

- **网络结构：**
>       整体网络结构
>                         W1(100X300)                         W2(300X10)
>       输入(X_100)     --------------->   隐层(hidden_300)  -------------> 输出(Y_10)
>       将输入的X_100与W1进行计算，隐射到size为300的隐含层，隐含层与W2进行计算，最后得到输出Y_10
>       比如输入为x1,x2,x3,输入权重为w1,隐层为a4,a5,a6,a7，输出权重为w2，输出为y1,y2
>       a4 = sigmoid(w41*x1 + w42*x2 + w43*x3)
>       a5 = sigmoid(w51*x1 + w52*x2 + w53*x3)
>       a6 = sigmoid(w61*x1 + w62*x2 + w63*x3)
>
>       y1 = sigmoid(w84*a4 + w85*a5 + w86*a6)
>       y2 = sigmoid(w94*a4 + w95*a5 + w96*a6)
>
>
>
>
>
>
>
>

- **GRU：**
>       概述：
>           GRU：LSTM的一种变体，GRU也许是最成功的一种
>       和LSTM的区别：
>           1、将三个门变成两个门
>               将输入门、遗忘门、输出门变为：更新门和重置门
>           2、将单元状态和输出合并为一个状态：h
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>

- **待续：**
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
