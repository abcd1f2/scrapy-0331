### 深度学习 - 双端LSTM序列标注
- **概述**：
>       字标注方法：
>           1、将分词等问题直接变成了一个序列标注问题，而且标注是对齐的
>           2、标注法实际上已经是一个总结语义规律的过程，“李”字是常用的姓氏，一半作为多字词（人名）的首字，即标记为b；而“想”由于“理想”之类的词语，
>               也有比较高的比例标记为e，这样一来，要是“李想”两字放在一起时，即便原来词表没有“李想”一词，我们也能正确输出be，也就是识别出“李想”为一个词
>           3、标注的数目，常用4tag
>               还有6tag和2tag，为什么选择4tag效果更好？
>               因为相比2tag，更多的tag实际上更全面概括了语义规律，相对于6tag，还要总结出第二字、第三字，但是这个角度总结不够合理（不过从新词发现的角度看，6tag更容易发现长词）
>
>       双向LSTM：
>           不管是LSTM或RNN，都是从左往右推进的，因此后面的词会比前面的词更重要，但是对于分词任务来说这是不妥的，
>           因为句子各个字应该是平权的？？？因此出现了双向LSTM，从左往右做一次LSTM，然后从右往左做一次LSTM，然后把两次结果组合起来
>
>       深度学习在分词中的应用:
>           不管是传统的神经网络还是LSTM，它们的做法跟传统的模型都是一样的，都是通过上下文来预测当前字的标签，上下文就是固定的窗口，比如用前后5个字加上当前字来预测当前字的标签。
>           这种做法仅仅是把以往估计概率的方法，如HMM、CRF等，换成了神经网络，整个框架没有变，本质上还是N-gram模型。
>
>
>

- **BiLSTM-CRF模型做基于字的中文命名实体识别**
>       传统的机器学习算法需要手工定义特征模型在训练CRF模型
>
>       学术界比较流行的BiLSTM-CRF模型
>           从宏观上讲，就是用crf是为了获取全局最优的输出序列，相当于对LSTM信息的再利用
>           引用知乎的回答：从网络结构上讲，BiLSTM-crf套用的还是crf这个大框架，不过把LSTM在每个t时刻在第i个tag上的输出，看做是crf特征函数里的"点函数"(只与当前位置有关的特征函数)，
>           然后"边函数"(与前后位置有关的函数)还是crf自带的，这样就将线性链crf里原始的w*f这种形式的特征函数(线性)变成lstm的输出f1(非线性)，
>           这就是在原始crf中引入了非线性，可以更好的拟合数据
>
>           双向LSTM同时考虑了过去的特征(通过前向过程提取)和未来的特征(通过后向过程提取)，一个正向输入序列，一个反向输入序列，再将两者的输出结合起来作为最终的结果。
>
>           算法过程：
>               BiLSTM layer的输出维度是tag size，这就相当于是每个词Wi映射到tag的发送概率，如果BiLSTM的输出矩阵为P，其中，Pi,j代表词Wi映射到tagi的非归一化概率。
>               对于crf来说，如果存在一个转移矩阵A，则Ai,j代表tagi转移到tagj的转移概率。
>               可以使用viterbi解码最优的tag序列
>
>
>       BiLSTM+CRF算法流程：（？？？？？）
>           for epoch
>               for batch
>                   1、BiLSTM+CRF模型前向传播
>                       (1)向后状态的前向传播
>                       (2)向前状态的前向传播
>                   2、CRF层前后向传播
>                   3、BiLSTM+CRF模型后向传播
>                       (1)向后状态的后向传播
>                       (2)向前状态的后向传播
>               done
>           done
>
>           算法流程：
>               1、向前推算：
>                   a、向后的前向传播，向后的hidden layer
>                   b、向前的前向传播，向前的hidden layer
>                   c、输出层的前向传播
>               2、向后推算：
>                   a、输出层的反向传播，所有的输出层δ项首先被计算，然后返回给两个不同方向的隐含层
>                   b、向后的反向传播
>                   c、向前的反向传播
>

- **CRF层作用：**
>       直接用bilstm输出的标签有些并不合理，CRF层可以为最终预测的标签添加一些约束以确保它们有效，crf的作用仅仅是根据标签间的关系做结果调整
>
>       引用知乎参考：
>           简单说就是条件随机场可以把label的上下文学出来。lstm加softmax分类的时候只能把特征的上下文关系学出来，label的没学出来。
>
>           从网络结构上来讲，Bi-LSTM-CRF套用的还是CRF这个大框架，只不过把LSTM在每个tt时刻在第ii个tag上的输出，
>               看作是CRF特征函数里的“点函数”（只与当前位置有关的特征函数），然后“边函数”（与前后位置有关的特征函数）还是用CRF自带的。
>               这样就将线性链CRF里原始的w∗fw∗f这种形式的特征函数（线性）变成LSTM的输出f1f1（非线性），这就在原始CRF中引入了非线性，可以更好的拟合数据
>
>
>
>
>
>
>
>
>
>
>
>
>

- **待续：**
>       参考：https://blog.csdn.net/jerr__y/article/details/70471066   Bi-directional LSTM
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
