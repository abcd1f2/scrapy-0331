### 深度学习 - 优化算法
- **概述**：
>       一阶优化算法：
>           最常用的一阶优化算法是梯度下降
>           函数梯度：导数dy/dx的多变量表达式，用来表示y相对于x的瞬时变化率。往往为了计算多变量函数的导数时，会用梯度取代导数，并使用偏导数来计算梯度。
>                    梯度和导数之间的一个主要区别是函数的梯度形成了一个向量场。
>           对单变量函数，使用导数来分析；梯度是基于多变量函数而产生的。
>
>           1、梯度下降：
>               这是在神经网络中最常用的优化算法
>               梯度下降的功能是：通过寻找最小值，控制方差，更新模型参数，最终使模型收敛
>               Θ = Θ - η*∇(θ).J(θ)，η是学习率，∇(θ).J(θ)是损失函数J(θ)的梯度
>               梯度下降主要用于在神经网络模型中进行权重的更新，即在一个方向上更新和调整模型的参数，来最小化损失函数。
>               注：2006年引入的反向传播技术，使得训练深层神经网络成为可能。反向传播技术是先在前向传播中计算输入信号的乘积及其对应的权重，然后将激活函数作用于这些乘积的总和。
>                   这种将输入信号转换为输出信号的方式，是一种对复杂非线性函数进行建模的重要手段，并引入了非线性激活函数，使得模型能够学习到几乎任意形式的函数映射。
>                   然后在网络的反向传播过程中回传相关误差，使用梯度下降更新权重值，通过计算误差函数E相对于权重参数W的梯度，在损失函数梯度的相反方向上更新权重参数。
>               缺点：
>                   1、传统的批量梯度下降将计算整个数据集梯度，但只会进行一次更新，因此在处理大型数据集时速度很慢且难以控制，甚至内存溢出。
>                   2、权重更新的快慢由学习率η决定的，并且可以在凸面误差曲面中收敛到全局最优值，在非凸曲面中可能趋于局部最优值
>                   3、在训练大型数据集时存在冗余的权重更新
>               改进：
>                   在随机梯度下降中可以得到解决
>
>           2、SGD(Stochastic gradient descent) 随机梯度下降
>               SGD对每个训练样本进行参数更新，每次执行都进行一次更新，且执行速度更快
>               频繁的更新使得参数间具有高方差，损失函数会以不同的强度波动。这其实将有助于我们发现新的和可能更优的局部最小值，而标准梯度下降将只会收敛到某个局部最优值
>               缺点：
>                   1、由于频繁的更新和波动，最终将收敛到最小限度，并会因波动频繁存在超调量
>               改进：
>                   小批量梯度下降算法，则可以解决高方差的参数更新和不稳定收敛的问题
>
>           3、动量
>               SGD方法中的高方差振荡使得网络很难稳定收敛，有研究提出了一种称为动量的技术，通过**优化相关方向的训练和弱化无关方向的振荡**，来加速SGD训练。
>                   即这种新方法将上个步骤中更新向量的分量γ添加到当前更新向量。
>                   V(t)=γV(t−1)+η∇(θ).J(θ)
>                   θ=θ−V(t)  更新参数，通常γ设置为0.9
>               优点：
>                   1、使网络能更优和更稳定的收敛
>                   2、减少振荡过程
>               当其梯度指向实际移动方向时，动量项γ增大；当梯度与实际移动方向相反时，γ减小。这种方式意味着动量项只对相关样本进行参数更新，减少了不必要的参数更新，从而得到更快且稳定的收敛，减少了振荡过程。
>
>
>           4、Mini Batch Gradient Descent 小批量梯度下降：
>               小批量梯度下降，因为对每个批次中的n个训练样本，这种方法只执行一次更新，避免了SGD和标准梯度下降中存在的问题
>               优点：
>                   1、可以减少参数更新的波动，最终得到效果更好和更稳定的收敛
>                   2、可以使用最新的深层学习库中通用的矩阵优化方法，使计算小批量数据的梯度更加高效
>                   3、通常小批量样本的大小从50到256，可根据实际问题有所不同
>                   4、在训练神经网络时，通常都会选择小批量梯度下降算法
>               这种方法有时候还是被称为SGD
>               挑战：
>                   1、很难选择出合适的学习率
>                   2、相同的学习率并不适用于所有的参数更新
>                   3、神经网络中，最小化非凸误差函数的另一个关键挑战是避免陷于多个其他局部最小值中
>           5、adagrad
>               adagrad方法是通过参数来调整合适的学习率η，对系数参数进行大幅更新，对频繁参数进行小幅更新。因此，Adagrad非常适合处理稀疏数据。
>               在时间步长中，Adagrad方法基于每个参数计算的过往梯度，为不同参数θ设置不同的学习率。
>               使用方法：
>                   1、首先，每个参数θ(i)使用相同的学习率，每次会对所有参数θ进行更新。
>                   2、在每个时间步t中，Adagrad方法为每个参数θ选取不同的学习率，更新对应的参数，然后进行向量化。
>                       假设t时刻参数θ(i)的损失函数梯度为g(t,i)，则θ(t+1,i)=θ(t+1,i) - α*g(t,i)
>                       Adagrad方法是在每个时间步中，根据过往已计算的参数梯度，来为每个参数θ(i)修改对应的学习率η
>               优点：
>                   不需要手工调整学习率。大多数参数使用了默认的0.01，且保存不变。
>               缺点：
>                   学习率η总是在降低和衰减
>                   （根据更新参数的公式）因为每个附加项都是正的，在分母中积累了多个平方梯度值，故累积的总和在训练期间保存增长。
>                       这反过来又导致学习率下降，变为很小数量级的数字，该模型完全停止学习，停止获取新的额外知识。
>                   因为随着学习速度的越来越小，模型的学习能力迅速降低，而且收敛速度非常慢，需要很长的训练和学习，即学习速度降低。
>               改进：
>                   另一个Adadelta算法改善了这个学习率不断衰减的问题。
>           6、AdaDelta方法：
>               这是一个AdaGrad的延伸方法，它倾向于解决学习率衰减的问题。Adadelta不是累积所有之前的平方梯度，而是将积累之前梯度的窗口限制到某个固定大小w。
>               与之前无效地存储w先前的平方梯度不同，梯度的和被递归地定义为所有先前平方梯度的衰减平均值。作为与动量项相似的分数γ，在t时刻的滑动平均值仅仅取决于先前的平均值和当前梯度值
>               优点：
>                   1、解决学习率η衰减的问题
>                   2、为每个参数计算出不同的学习率
>                   3、计算了动量项
>                   4、防止学习率或梯度消失等问题的出现
>               改进：
>                   之前的方法中计算了每个参数的对应学习率，但是为什么不计算每个参数的对应动量变化并独立存储呢？这就是Adam算法的改良。
>
>           7、Adam算法自适应时刻估计方法(Adaptive Moment Estimation)：
>               能计算每个参数的自适应学习率。
>               这个方法不仅存储了AdaDelta先前平方梯度的指数衰减平均值，而且保持了先前梯度M(t)的指数衰减平均值，这一点与动量相似，公式略。
>               优点：
>                   实际应用中，Adam方法效果良好。与其他只适应学习率相比，
>                   1、其收敛速度更快
>                   2、学习效果更有效
>                   3、可以纠正其他优化技术中存在的问题，如学习率消失、收敛过慢或是高方差的参数更新导致损失函数波动较大等问题
>                   而标准的SGD、NAG和动量项等方法收敛缓慢，很难找到正确的方向。
>
>       二阶优化算法：
>           二阶优化算法使用了二阶导数（也叫做Hessian方法）来最小化或最大化损失函数。由于二阶导数计算成本高，所以使用并不广泛。
>
>       总结：
>           在构建神经网络模型中，选择出最佳的优化器，以便快速收敛并正确学习，同时调整内部参数，最大程度地最小化损失函数。
>           Adam在实际应用中效果良好，超过了其他的自适应技术。
>           1、数据比较稀疏
>               对于数据集比较稀疏，SGD、NAG和动量项等方法可能效果不好。
>               对于稀疏数据，应该使用某种自适应学习率的方法，另一个好处就是不需要人为调整学习率，使用默认参数就可能获得最优值。
>           2、快速收敛或神经网络比较复杂
>               如果想使训练深层网络模型快速收敛或所构建的神经网络较为复杂，应该使用Adam或其他自适应学习率的方法，这些方法实际效果更优
>
>           文本语言模型类的貌似Adam效果更好，对于分类之类的，貌似AdaDelta效果更好。（仅供参考）
>
>

- **Batch Normalization：**
>       可以加快网络的训练速度       
>
>
>
>
>
>
>
>
>
>

- **待续**
>       参考：https://zhuanlan.zhihu.com/p/27449596
>           https://zhuanlan.zhihu.com/p/32262540   Adam那么棒，为什么还对SGD念念不忘 (2)—— Adam的两宗罪
>           https://blog.csdn.net/hjimce/article/details/50866313   深度学习（二十九）Batch Normalization 学习笔记
>
>
>
>
>
>
>
>
>
>
>
>
>
