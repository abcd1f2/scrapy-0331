### 深度学习 - attention模型
- **概述**：
>       应用：
>           attention模型是当前研究的热点，广泛地可应用于文本生成、机器翻译和语言模型等
>
>       从注意力模型的命名方式看，很明显其借鉴了人类的注意力机制。
>       深度学习中的注意力机制从本质上讲和人类的选择性视觉注意力机制类似，核心目标也是从众多信息中选择出对当前任务目标更关键的信息。
>

- **Encoder-Decoder框架：**
>       要了解深度学习中的注意力模型，就不得不先谈Encoder-Decoder框架，因为目前大多数注意力模型附着在Encoder-Decoder框架下，
>           当然，其实注意力模型可以看作一种通用的思想，本身并不依赖于特定框架，这点需要注意。
>
>       文本处理领域的Encoder-Decoder框架可以这么直观地去理解：
>           可以把它看作适合处理由一个句子（或篇章）生成另外一个句子（或篇章）的通用处理模型。
>
>       对于句子对<Source,Target>，我们的目标是给定输入句子Source，期待通过Encoder-Decoder框架来生成目标句子Target。
>           Source和Target可以是同一种语言，也可以是两种不同的语言。
>
>       文本处理的encoder-decoder流程：
>           1、Source和Target分别由各自的单词序列构成
>               Source = <x1,x2,...xn>
>               Target = <y1,y2,...ym>
>           2、Encoder顾名思义就是对输入句子Source进行编码，将输入句子通过非线性变换转化为中间语义表示C
>               C = f(Source)
>           3、解码器Decoder来说，其任务是根据句子Source的中间语义表示C和之前已经生成的历史信息y1,y2,...yi-1来生成i时刻要生成的单词yi
>               yi = g(C,y1,y2,...yi-1)
>               如：
>                   y1 = g(C)
>                   y2 = g(C,y1)
>           每个yi都依次这么产生，那么看起来就是整个系统根据输入句子Source生成了目标句子Target。
>       应用：
>           如果Source是中文句子，Target是英文句子，那么这就是解决机器翻译问题的Encoder-Decoder框架；
>           如果Source是一篇文章，Target是概括性的几句描述语句，那么这是文本摘要的Encoder-Decoder框架；
>           如果Source是一句问句，Target是一句回答，那么这是问答系统或者对话机器人的Encoder-Decoder框架。
>           对于语音识别来说，Encoder部分的输入是语音流，输出是对应的文本信息
>           一般而言，文本处理和语音识别的Encoder部分通常采用RNN模型，图像处理的Encoder一般采用CNN模型。
>

- **Encode-Decode模型：**
>       Encode-Decode模型也叫编码-解码模型，这是一种应用于seq2seq问题的模型。
>       采用Encode-Decode结构的模型非常热门，因为在许多领域比其他传统模型方法都取得了更好的结果。
>       Encode-Decode结构模型通常将输入序列编码成一个固定长度的向量表示，对于长度较短的输入序列而言，模型能够学习出对应合理的向量表示。
>       seq2seq原理：
>           就是根据一个输入序列X，来生成另一个输出序列Y。
>           seq2seq应用：翻译、文档摘要、问答系统（输入问题，输出答案）等
>           为了解决seq2seq问题，有人提出了encode-decode模型，
>               encode即将输入序列转化为一个固定长度的向量
>               decode即将之前生成的固定向量转化为输出序列
>           具体实现中，encode和decode都不是固定的，可选的有CNN、RNN、BiRNN、GRU、LSTM等，可以自由组合
>               比如在编码时使用BiRNN，解码使用RNN，或者编码使用RNN，解码使用LSTM等
>       缺点：
>           当输入序列非常长时，模型难以学到合理的向量表示。
>           当输入序列较长时，编码器要将整个序列的信息压缩进一个固定长度的向量中，但是这有两个问题：
>               1、语义向量无法完全表示整个序列的信息
>               2、先输入的内容携带的信息会被后输入的信息稀释掉，或者说被覆盖掉。序列越长，现象越严重。
>           那么在解码的时候，开始就没有获取序列足够信息，那么解码的准确度自然需要打折扣。
>

- **attention模型**
>       attention机制克服了Encode-Decode结构的LSTM/RNN模型存在的问题，attention在模型输出时会选择性地专注考虑输入中的对应相关的信息。
>       attention模型在产生输出的时候，还会产生一个"注意力范围"表示接下来输出的时候要重点关注输入序列中的哪些部分，然后根据关注的区域来产生下一个输出。！！！！
>       原理：
>           attention模型最大的区别就是它不要求编码器将所有输入信息都编码进一个固定长度的向量里面。
>           相反，编码器需要将输入编码成一个向量的序列，而在解码的时候，每一步都会选择性的从向量序列中挑选一个子集进行下一步处理。
>           这样，在产生每一个输出的时候，都能够做到充分利用输入序列携带的信息。
>       实现：
>           编码可以使用BiRNN，可以包含当前词前后的信息
>
>       attention模型有人叫做对齐模型，因为在翻译应用中，attention模型你能很好的找到合适的位置，即两种语言下的单词"对齐"了。
>           相比于语言学实现的硬对齐，这个基于概率的软对其更加优雅。
>

- **attention的种类：**
>       注意力机制有两种方法：（仅供参考）
>           1、基于强化学习的
>               通过收益函数来激励，让模型更加关注某个局部的细节
>           2、基于梯度下降的
>               通过目标函数以及相应的优化函数来做的
>

- **Soft Attention模型思想：**
>       Target生成过程如下：
>           y1 = g(C)
>           y2 = g(C,y1)
>       使用的输入句子Source的语义编码C都是一样的，没有任何区别。而语义编码C是由句子Source的每个单词经过Encoder 编码产生的，
>           这意味着不论是生成哪个单词，y1,y2还是y3，其实句子Source中任意单词对生成某个目标单词yi来说影响力都是相同的，这是为何说这个模型没有体现出注意力的缘由。
>           这类似于人类看到眼前的画面，但是眼中却没有注意焦点一样。
>
>       没有引入注意力的模型在输入句子比较短的时候问题不大，但是如果输入句子比较长，此时所有语义完全通过一个中间语义向量来表示，单词自身的信息已经消失，
>           可想而知会丢失很多细节信息，这也是为何要引入注意力模型的重要原因。
>
>       如果引入Attention模型的话，应该在翻译“杰瑞”的时候，体现出英文单词对于翻译当前中文单词不同的影响程度，
>           比如给出类似下面一个概率分布值：（Tom,0.3）(Chase,0.2) (Jerry,0.5)
>           每个英文单词的概率代表了翻译当前单词“杰瑞”时，注意力分配模型分配给不同英文单词的注意力大小。这对于正确翻译目标语单词肯定是有帮助的，因为引入了新的信息。
>               y1 = f1(C1)
>               y2 = f1(C2,y1)
>               y3 = f1(C3,y1,y2)
>           每个Ci可能对应着不同的源语句子单词的注意力分配概率分布
>
>       三阶段计算Attention过程：
>           1、第一个阶段，可以引入不同的函数和计算机制，根据Query和某个Key_i，计算两者的相似性或者相关性
>               最常见的方法包括：求两者的向量点积、求两者的向量Cosine相似性或者通过再引入额外的神经网络来求值
>           2、第二阶段引入类似SoftMax的计算方式对第一阶段的得分进行数值转换，
>               一方面可以进行归一化，将原始计算分值整理成所有元素权重之和为1的概率分布
>               另一方面也可以通过SoftMax的内在机制更加突出重要元素的权重。
>           3、第三阶段计算结果a_i即为value_i对应的权重系数，然后进行加权求和即可得到Attention数值
>
>       在自然语言处理应用里会把Attention模型看作是输出Target句子中某个单词和输入Source句子每个单词的对齐模型，这是非常有道理的。！！！
>

- **常用注意力打分机制：**
>       1、加性模型
>           s = tanh(Wx)
>       2、点积模型
>           s = xq
>       3、缩放点积模型
>           s = xq/λ
>       4、双线性模型
>           s = xwq
>


- **Self Attention：**
>       Self Attention也经常被称为intra Attention（内部Attention）
>
>       自注意力模型（self-Attention model）中，通常使用缩放点积来作为注意力打分函数
>
>       最近一年也获得了比较广泛的使用，比如Google最新的机器翻译模型内部大量采用了Self Attention模型。
>
>       指的不是Target和Source之间的Attention机制，而是Source内部元素之间或者Target内部元素之间发生的Attention机制，
>           也可以理解为Target=Source这种特殊情况下的注意力计算机制。
>
>       Self Attention可以捕获同一个句子中单词之间的一些句法特征（比如图11展示的有一定距离的短语结构）或者语义特征（比如图12展示的its的指代对象Law）
>
>       比如对于机器翻译来说，本质上是目标语单词和源语单词之间的一种单词对齐机制。
>
>       self attention优点：
>           1、很明显，引入Self Attention后会更容易捕获句子中长距离的相互依赖的特征，因为如果是RNN或者LSTM，需要依次序序列计算，对于远距离的相互依赖的特征，
>               要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小。 ！！！
>           2、但是Self Attention在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以远距离依赖特征之间的距离被极大缩短，
>               有利于有效地利用这些特征。
>           3、除此外，Self Attention对于增加计算的并行性也有直接帮助作用。
>           这是为何Self Attention逐渐被广泛使用的主要原因。
>

- **为什么自注意力模型（self-Attention model）在长距离序列中如此强大？**
>       1、卷积或循环神经网络难道不能处理长距离序列吗？
>           当使用神经网络来处理一个变长的向量序列时，我们通常可以使用卷积网络或循环网络进行编码来得到一个相同长度的输出向量序列
>           无论卷积还是循环神经网络其实都是对变长序列的一种“局部编码”：
>               卷积神经网络显然是基于N-gram的局部编码
>               而对于循环神经网络，由于梯度消失等问题也只能建立短距离依赖
>       2、要解决这种短距离依赖的“局部编码”问题，从而对输入序列建立长距离依赖关系，有哪些办法呢？
>           如果要建立输入序列之间的长距离依赖关系，可以使用以下两种方法：
>               (1)、一 种方法是增加网络的层数，通过一个深层网络来获取远距离的信息交互
>               (2)、一种方法是使用全连接网络
>           全连接网络虽然是一种非常直接的建模远距离依赖的模型， 但是无法处理变长的输入序列。不同的输入长度，其连接权重的大小也是不同的。
>           可以利用注意力机制来“动态”地生成不同连接的权重，这就是自注意力模型（self-attention model）。由于自注意力模型的权重是动态生成的，因此可以处理变长的信息序列。
>      为什么自注意力模型（self-Attention model）如此强大：
>           利用注意力机制来“动态”地生成不同连接的权重，从而处理变长的信息序列。
>       3、自注意力模型（self-Attention model）具体的计算流程是怎样的呢？
>           给出信息输入：用X = [x1, · · · , xN ]表示N 个输入信息；通过线性变换得到：查询向量序列Q，键向量序列K和值向量序列V
>           self-Attention中的Q是对自身（self）输入的变换，而在传统的Attention中，Q来自于外部。
>
>

- **attention在语音识别中的应用：**
>       Encoder部分的Source输入是语音流信号，Decoder部分输出语音对应的字符串流
>       Attention机制起到了将输出字符和输入语音信号进行对齐的功能。
>

- **注意力分配概率分布值：**
>       在时刻i，如果要生成yi单词，我们是可以知道Target在生成Yi之前的时刻i-1时，隐层节点i-1时刻的输出值Hi-1的，
>           而我们的目的是要计算生成Yi时输入句子中的单词“Tom”、“Chase”、“Jerry”对Yi来说的注意力分配概率分布，
>           那么可以用Target输出句子i-1时刻的隐层节点状态Hi-1去一一和输入句子Source中每个单词对应的RNN隐层节点状态hj进行对比，
>           即通过函数F(hj,Hi-1)来获得目标单词yi和每个输入单词对应的对齐可能性，这个F函数在不同论文里可能会采取不同的方法，
>           然后函数F的输出经过Softmax进行归一化就得到了符合概率分布取值区间的注意力分配概率分布数值
>
>       Lx代表输入句子Source的长度，aij代表在Target输出第i个单词时Source输入句子中第j个单词的注意力分配系数，而hj则是Source输入句子中第j个单词的语义编码。
>           假设下标i就是上面例子所说的“ 汤姆” ，那么Lx就是3，h1=f(“Tom”)，h2=f(“Chase”),h3=f(“Jerry”)分别是输入句子每个单词的语义编码，
>           对应的注意力模型权值则分别是0.6,0.2,0.2，所以g函数本质上就是个加权求和函数。
>
>       绝大多数Attention模型都是采取上述的计算框架来计算注意力分配概率分布信息，区别只是在F的定义上可能有所不同。
>
>

- **Attention模型的物理意义：**
>       怎么理解Attention模型的物理含义？
>           一般在自然语言处理应用里会把Attention模型看作是输出Target句子中某个单词和输入Source句子每个单词的对齐模型，这是非常有道理的。
>
>       目标句子生成的每个单词对应输入句子单词的概率分布可以理解为输入句子单词和这个目标生成单词的对齐概率，这在机器翻译语境下是非常直观的。
>           传统的统计机器翻译一般在做的过程中会专门有一个短语对齐的步骤，而注意力模型其实起的是相同的作用。
>
>       Google 神经网络机器翻译系统结构图，如下，
> ![avatar](https://github.com/nwaiting/wolf-ai/blob/master/wolf_others/dl_pic/deeplearning_soft_attention_google_translate.jpg)
>       上图为Google于2016年部署到线上的基于神经网络的机器翻译系统，相对传统模型翻译效果有大幅提升，翻译错误率降低了60%，
>           其架构就是上文所述的加上Attention机制的Encoder-Decoder框架，主要区别无非是其Encoder和Decoder使用了8层叠加的LSTM模型
>
>

- **待续：**
>       参：https://cloud.tencent.com/developer/article/1153079   “变形金刚”为何强大：从模型到代码全面解析Google Tensor2Tensor系统
>           https://www.zhihu.com/question/68482809/answer/264632289    目前主流的attention方法都有哪些？
>           https://nndl.github.io/     神经网络与深度学习 (作者：邱锡鹏 )
>           https://zhuanlan.zhihu.com/p/31547842   模型汇总24 - 深度学习中Attention Mechanism详细介绍：原理、分类及应用
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
