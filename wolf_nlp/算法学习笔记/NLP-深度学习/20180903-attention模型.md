### 深度学习 - attention模型
- **概述**：
>       应用：
>           attention模型是当前研究的热点，广泛地可应用于文本生成、机器翻译和语言模型等
>
>
>

- **Encode-Decode模型：**
>       Encode-Decode模型也叫编码-解码模型，这是一种应用于seq2seq问题的模型。
>       采用Encode-Decode结构的模型非常热门，因为在许多领域比其他传统模型方法都取得了更好的结果。
>       Encode-Decode结构模型通常将输入序列编码成一个固定长度的向量表示，对于长度较短的输入序列而言，模型能够学习出对应合理的向量表示。
>       seq2seq原理：
>           就是根据一个输入序列X，来生成另一个输出序列Y。
>           seq2seq应用：翻译、文档摘要、问答系统（输入问题，输出答案）等
>           为了解决seq2seq问题，有人提出了encode-decode模型，
>               encode即将输入序列转化为一个固定长度的向量
>               decode即将之前生成的固定向量转化为输出序列
>           具体实现中，encode和decode都不是固定的，可选的有CNN、RNN、BiRNN、GRU、LSTM等，可以自由组合
>               比如在编码时使用BiRNN，解码使用RNN，或者编码使用RNN，解码使用LSTM等
>       缺点：
>           当输入序列非常长时，模型难以学到合理的向量表示。
>           当输入序列较长时，编码器要将整个序列的信息压缩进一个固定长度的向量中，但是这有两个问题：
>               1、语义向量无法完全表示整个序列的信息
>               2、先输入的内容携带的信息会被后输入的信息稀释掉，或者说被覆盖掉。序列越长，现象越严重。
>           那么在解码的时候，开始就没有获取序列足够信息，那么解码的准确度自然需要打折扣。
>
>
>
>

- **attention模型**
>       attention机制克服了Encode-Decode结构的LSTM/RNN模型存在的问题，attention在模型输出时会选择性地专注考虑输入中的对应相关的信息。
>       attention模型在产生输出的时候，还会产生一个"注意力范围"表示接下来输出的时候要重点关注输入序列中的哪些部分，然后根据关注的区域来产生下一个输出。！！！！
>       原理：
>           attention模型最大的区别就是它不要求编码器将所有输入信息都编码进一个固定长度的向量里面。
>           相反，编码器需要将输入编码成一个向量的序列，而在解码的时候，每一步都会选择性的从向量序列中挑选一个子集进行下一步处理。
>           这样，在产生每一个输出的时候，都能够做到充分利用输入序列携带的信息。
>       实现：
>           编码可以使用BiRNN，可以包含当前词前后的信息
>
>       attention模型有人叫做对齐模型，因为在翻译应用中，attention模型你能很好的找到合适的位置，即两种语言下的单词"对齐"了。
>           相比于语言学实现的硬对齐，这个基于概率的软对其更加优雅。
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>

- **待续：**
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
