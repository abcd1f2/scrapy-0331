### 深度学习 - seq2seq模型
- **概述**：
>       seq2seq：
>           seq2seq 是一个Encoder–Decoder 结构的网络，它的输入是一个序列，输出也是一个序列， Encoder 中将一个可变长度的信号序列变为固定长度的向量表达，
>               Decoder 将这个固定长度的向量变成可变长度的目标的信号序列
>
>       最基础的Seq2Seq模型包含了三个部分，即Encoder、Decoder以及连接两者的中间状态向量State Vector，Encoder通过学习输入，将其编码成一个固定大小的状态向量S，
>           继而将S传给Decoder，Decoder再通过对状态向量S的学习来进行输出。
>
>
>
>       rnn的不同结构：
>           1、one to one
>           2、one to many
>           3、many to one：情感分类
>           4、many to many
>           5、同步many to many
>
>       seq2seq：
>           在many to many结构中，经典的rnn结构的输入和输出序列要是等长，应用场景也比较有限。
>           seq2seq模型可以是输入和输出序列不等长，突破了输入序列和输出序列的大小限制，可以从一个序列到另一个序列的转换
>           应用：
>               Google曾用seq2seq模型加attention模型来实现翻译功能
>               聊天对话模型
>

- **seq2seq缺点：**
>       基础的模型连接Encoder和Decoder模块的组件仅仅是一个固定大小的状态向量，这使得Decoder无法直接去关注到输入信息的更多细节。
>       改进：
>           引入了Attention的概念以及Bi-directional encoder layer等
>

- **seq2seq应用：**
>       Seq2Seq 于 2013年、2014 年被多位学者共同提出，在机器翻译任务中取得了非常显著的效果，随后提出的 attention 模型更是将 Seq2Seq 推上了神坛
>       Seq2Seq+attention 的组合横扫了非常多的任务，只需要给定足够数量的 input-output pairs，通过设计两端的 sequence 模型和 attention 模型，
>           就可以训练出一个不错的模型。
>       文本生成任务都可以基于 Seq2Seq 模型来做，如：
>           1、文本摘要生成
>           2、对话生成
>
>
>
>
>
>
>
>
>
>
>
>

- **待续：**
>       参考：https://zhuanlan.zhihu.com/p/47063917    Attention机制详解（一）——Seq2Seq中的Attention
>           https://cloud.tencent.com/developer/article/1153079     “变形金刚”为何强大：从模型到代码全面解析Google Tensor2Tensor系统
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
