### 深度学习 - CNN
- **概述**：
>       CNN适合图像、语音识别任务的神经网络结构
>
>       参数权重共享：
>           数据窗口滑动，导致输入在变化，但中间滤波器filter的权重（即每个神经元连接数据窗口的权重）是固定不变的，这个权重即所谓的CNN的参数（权重）共享机制
>
>
>
>
>

- **CNN结构：**
>       输入层：
>           输入层，对数据做一些处理，
>               比如去均值（把输入数据各个维度都中心化为0，避免数据过多偏差，影响训练效果）
>               归一化（把所有数据都归一到同样的范围）
>               PCA/白化等等
>           CNN只对训练数据做"去均值"这一步
>       中间层：
>           CONV：卷积计算层，线性乘积求和
>           RELU：激励层，激活函数
>           POOL：池化层，即取区域平均或最大
>       输出层：
>           FC：全连接层
>

- **CNN-卷积**
>       未知图案的局部和标准X图案的局部一个一个比对时的计算过程，便是卷积操作。
>       什么是卷积？
>           对图像（不同的数据窗口数据）和滤波矩阵做内积的操作就是卷积操作，也是卷积神经网络的名字来源。
>           滤波矩阵：一组固定的权重，因为每个神经元的多个权重固定，所以又可以看做一个恒定的滤波器filter
>           如下图，
> ![avatar](https://github.com/nwaiting/wolf-ai/blob/master/wolf_others/pic/nlp_deep_learning_cnn_filter.jpg)
>           非严格意义上讲，红框的部分便可以理解为一个滤波器，即带着一组固定权重的神经元。
>
>       卷积计算参数：
>           深度depth：
>               神经元个数，决定输出的depth厚度，同时代表滤波器个数。
>           步长stride：
>               决定滑动多少步可以到边缘
>           填充值zero-padding：
>               在外围边缘补充若干圈0，方便重初始位置以步长为单位可以刚好滑到末尾位置，通俗讲就是为了总长度被步长整除
>
>
>


- **待续：**
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
