## dl - Seq2Seq Attention
- **概述：**
>       Attention机制最早是在视觉图像领域提出来的，应该是在九几年思想就提出来了，但是真正火起来应该算是2014年google mind团队的这篇论文《Recurrent Models of Visual Attention》，
>           他们在RNN模型上使用了attention机制来进行图像分类。
>       Bahdanau等人在论文《Neural Machine Translation by Jointly Learning to Align and Translate》中，使用类似attention的机制在机器翻译任务上将翻译和对齐同时进行，
>           他们的工作算是第一个将attention机制应用到NLP领域中。
>
>       Attention函数的本质可以被描述为一个查询（query）到一系列（键key-值value）对的映射。
>
>
>

- **注意力机制：**
>       按照认知神经学中的注意力，可以总体上分为两类：
>           1、聚焦式（focus）注意力
>               自上而下的有意识的注意力，主动注意——是指有预定目的、依赖任务的、主动有意识地聚焦于某一对象的注意力；
>               在人工神经网络中，注意力机制一般就特指聚焦式注意力
>           2、显著性（saliency-based）注意力
>               自下而上的有意识的注意力，被动注意——基于显著性的注意力是由外界刺激驱动的注意，不需要主动干预，也和任务无关
>               可以将max-pooling和门控（gating）机制来近似地看作是自下而上的基于显著性的注意力机制
>
>       这个过程实际上是Attention机制缓解神经网络模型复杂度的体现：不需要将所有的N个输入信息都输入到神经网络进行计算，只需要从X中选择一些和任务相关的信息输入给神经网络。！！！
>
>       注意力机制可以分为三步：
>           一是信息输入
>           二是计算注意力分布α
>               注意力打分机制有：加性模型、点积模型、缩放点积模型（self-Attention model常使用，多除了一个（为K的维度）起到调节作用，使得内积不至于太大）、双线性模型
>           三是根据注意力分布α 来计算输入信息的加权平均
>
>       这种编码方式为软性注意力机制（soft Attention），软性注意力机制有两种：
>           1、普通模式（Key=Value=X）
>               目前在NLP研究中，key和value常常都是同一个，即key=value
>           2、键值对模式（Key！=Value）
>

- **Attention机制的变种：**
>       1、变种1-硬性注意力
>           之前提到的注意力是软性注意力，其选择的信息是所有输入信息在注意力 分布下的期望。
>           还有一种注意力是只关注到某一个位置上的信息，叫做硬性注意力（hard attention）。
>           硬性注意力有两种实现方式：
>               （1）一种是选取最高概率的输入信息；
>               （2）另一种硬性注意力可以通过在注意力分布式上随机采样的方式实现。
>       2、变种2-键值对注意力
>           即键值对模式，此时Key！=Value
>       3、变种3-多头注意力
>           多头注意力（multi-head attention）是利用多个查询Q = [q1, · · · , qM]，来平行地计算从输入信息中选取多个信息。
>           每个注意力关注输入信息的不同部分，然后再进行拼接
>           在编码器和解码器中都使用了多头自注意力self-attention来学习文本的表示
>           Self-attention即K=V=Q，例如输入一个句子，那么里面的每个词都要和该句子中的所有词进行attention计算。目的是学习句子内部的词依赖关系，捕获句子的内部结构
>
>

- **Multi-head attention：**
>       多头attention（Multi-head attention）：
>           1、Query，Key，Value首先进过一个线性变换，然后输入到放缩点积attention，注意这里要做h次，其实也就是所谓的多头，每一次算一个头。
>           2、而且每次Q，K，V进行线性变换的参数W是不一样的。
>           3、然后将h次的放缩点积attention结果进行拼接，再进行一次线性变换得到的值作为多头attention的结果。
>       优点：
>           这样的好处是可以允许模型在不同的表示子空间里学习到相关的信息，后面还会根据attention可视化来验证
>       在编码器和解码器中都使用了多头自注意力self-attention来学习文本的表示。！！！
>       Self-attention即K=V=Q，例如输入一个句子，那么里面的每个词都要和该句子中的所有词进行attention计算。目的是学习句子内部的词依赖关系，捕获句子的内部结构
>
>       在模型的超参实验中可以看到，多头attention的超参h太小也不好，太大也会下降。整体更大的模型比小模型要好，使用dropout可以帮助过拟合
>
>       多头注意力机制作用：
>           1、扩展了模型专注于不同位置的能力
>           2、给出了注意力层的多个“表示子空间”（representation subspaces）
>
>

- **self-Attention处理过程：**
>       1、self-attention会计算出三个新的向量，在论文中，向量的维度是512维，我们把这三个向量分别称为Query、Key、Value，这三个向量是用embedding向量与一个矩阵相乘得到的结果，
>           这个矩阵是随机初始化的，维度为（64，512）注意第二个维度需要和embedding的维度一样，其值在BP的过程中会一直进行更新，得到的这三个向量的维度是64低于embedding维度的。
>

- **学习位置编码：**
>       学习位置编码跟生成词向量的方法相似，对应每一个位置学得一个独立的向量。
>       缺点：
>           1、句子长度不能超出位置编码的范围，也就是并不是任何位置都会有对应的学习位置编码向量
>           2、如果训练集里面短句子训练数据比较多的话，则对长句子不友好。
>       Vaswani也在文中提及最后使用正弦位置编码是因为第二个原因。
>       Shaw设计两个K/V相对位置向量的动机是想在使用多头注意力时，带上相对位置信息。不过作者后来通过实验发现，这种信息在机器翻译上并没有什么必要。
>
>       那么如何表达一个序列的位置信息呢？对于某一个单词来说，他的位置信息主要有两个方面：
>           1、一是绝对位置
>           2、二是相对位置
>           绝对位置决定了单词在一个序列中的第几个位置，相对位置决定了序列的流向。作者利用了正弦函数和余弦函数来进行位置编码
>           (每个位置都能得到一个唯一的值作为编码（通过反证法可以证明）)
>       值得注意的是，位置编码是不参与训练的，而词向量是参与训练的。作者通过实验发现，位置编码参与训练与否对最终的结果并无影响。
>       attention中这不是唯一可能的位置编码方法。它的优点是能够扩展到未知的序列长度(例如，当我们训练出的模型需要翻译远比训练集里的句子更长的句子时)。
>
>

- **self-Attention model：**
>       为什么自注意力模型（self-Attention model）在长距离序列中如此强大？
>           1、卷积或循环神经网络难道不能处理长距离序列吗？
>               无论卷积还是循环神经网络其实都是对变长序列的一种“局部编码”
>               卷积神经网络显然是基于N-gram的局部编码；而对于循环神经网络，由于梯度消失等问题也只能建立短距离依赖。
>           2、要解决这种短距离依赖的“局部编码”问题，从而对输入序列建立长距离依赖关系，有哪些办法呢？
>               1、一种方法是增加网络的层数，通过一个深层网络来获取远距离的信息交互
>               2、另一种方法是使用全连接网络
>               全连接网络虽然是一种非常直接的建模远距离依赖的模型， 但是无法处理变长的输入序列。不同的输入长度，其连接权重的大小也是不同的。
>
>       自注意力模型：
>           可以利用注意力机制来“动态”地生成不同连接的权重
>           由于自注意力模型的权重是动态生成的，因此可以处理变长的信息序列（自注意力模型（self-Attention model）强大的原因）
>
>       自注意力模型（self-Attention model）具体的计算流程是怎样的呢?
>           自注意力模型（self-Attention model）中，通常使用缩放点积来作为注意力打分函数
>
>       Self-Attention即K=V=Q，例如输入一个句子，那么里面的每个词都要和该句子中的所有词进行Attention计算。目的是学习句子内部的词依赖关系，捕获句子的内部结构
>
>       self-attention优点：
>           1、每一层的复杂度
>               如果输入序列n小于表示维度d的话，每一层的时间复杂度Self-Attention是比较有优势的
>               当n比较大时，作者也给出了一种解决方案Self-Attention(restricted)即每个词不是和所有词计算Attention，而是只与限制的r个词去计算Attention
>           2、是否可以并行
>               multi-head Attention和CNN一样不依赖于前一时刻的计算，可以很好的并行，优于 RNN
>           3、长距离依赖
>               由于Self-Attention是每个词和所有词都要计算Attention，所以不管他们中间有多长距离，最大的路径长度也都只是 1。可以捕获长距离依赖关系。
>       在长距离依赖上，由于self-attention是每个词和所有词都要计算attention，所以不管他们中间有多长距离，最大的路径长度也都只是1。可以捕获长距离依赖关系。！！！
>
>       使用自注意力机制的原因，论文中提到主要从三个方面考虑：
>           1、每一层的复杂度
>           2、是否可以并行
>           3、长距离依赖学习
>
>       self-attention的特点在于无视词之间的距离直接计算依赖关系，能够学习一个句子的内部结构，实现也较为简单并行可以并行计算。
>
>
>

- **Attention is all you need：**
>       这篇论文主要亮点在于：
>           1、不同于以往主流机器翻译使用基于RNN的seq2seq模型框架，该论文用attention机制代替了RNN搭建了整个模型框架
>           2、提出了多头注意力（Multi-headed attention）机制方法，在编码器和解码器中大量的使用了多头自注意力机制（Multi-headed self-attention）
>           3、训练速度比主流模型更快
>
>       transform：
>           为了更好的优化深度网络，整个网络使用了残差连接和对层进行了规范化（Add&Norm）
>

- **前馈网络：**
>       每个encoderLayer中，多头attention后会接一个前馈网络
>       这个前馈网络其实是两个全连接层
>       这两层的作用等价于两个 kenerl_size=1的一维卷积操作
>

- **线性层和softmax：**
>       从Decoder拿到的输出是维度为（batch_size, max_seq_len, d_model）的浮点型张量，我们希望得到最终每个单词预测的结果，
>           首先用一个线性层将d_model映射到vocab的维度，得到每个单词的可能性，然后送入softmax，找到最可能的单词。
>       线性层的参数个数为d_mode ⋆ vocab_size， 一般来说，vocab_size会比较大，拿20000为例，那么只这层的参数就有512⋆20000个，约为10的8次方，非常惊人。
>           而在词向量那一层，同样也是这个数值，所以，一种比较好的做法是将这两个全连接层的参数共享，会节省不少内存，而且效果也不会差。
>
>
>
>
>
>
>

- **待续：**
>       参考：https://zhuanlan.zhihu.com/p/40920384    真正的完全图解Seq2Seq Attention模型
>           https://zhuanlan.zhihu.com/p/43493999   Attention原理和源码解析
>           https://zhuanlan.zhihu.com/p/53682800   nlp中的Attention注意力机制+Transformer详解
>           https://www.zhihu.com/question/68482809/answer/264632289    目前主流的attention方法都有哪些？
>           https://daiwk.github.io/posts/nlp-self-attention-models.html    自然语言处理中的自注意力机制（Self-Attention Mechanism）
>           https://www.paperweekly.site/papers/notes/148   自注意力机制
>           https://www.cnblogs.com/robert-dlut/p/8638283.html  自然语言处理中的自注意力机制（Self-attention Mechanism）
>           https://cupdish.com/2018/03/28/attention-is-all-you-need/#Decoder-%E9%83%A8%E5%88%86    Attention is all you need 论文阅读报告及代码详解
>           https://lonepatient.top/2019/01/17/BERT-Transformer.html    Transformer原理和实现
>
>
>
>
>
>
>
>
>
>
>
>
