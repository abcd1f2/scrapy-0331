## dl - BatchNormalization
- **概述：**
>       BatchNormalization不仅能够大大加快收敛速度，还自带正则化功能，是Google 2015年提出的。(Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift)
>
>       机器学习的一个重要的假设是：数据是独立同分布的。训练集合测试集的数据是同分布的，这样模型才有好的泛化效果。神经网络其实也是在学习这个分布。
>           
>
>
>
>
>
>
>
>
>
>
>
>
>
>

- **待续：**
>       参考：https://lonepatient.top/2019/01/17/BERT-Transformer.html     Transformer原理和实现
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
