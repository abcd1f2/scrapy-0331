## dl - Transformer
- **概述：**
>       Attention Is All You Need是一篇Google提出的将Attention思想发挥到极致的论文
>       这篇论文中提出一个全新的模型，叫 Transformer，抛弃了以往深度学习任务里面使用到的 CNN 和 RNN ，目前大热的Bert就是基于Transformer构建的
>       Transformer应用领域：
>           这个模型广泛应用于NLP领域，例如机器翻译，问答系统，文本摘要和语音识别等等方向
>
>       transform是目前效果最好的特征抽取器，本质是self attention叠加结构。！！！
>
>

- **Transformer架构：**
>       和Attention模型一样，Transformer模型中也采用了 encoer-decoder 架构。但其结构相比于Attention更加复杂，论文中encoder层由6个encoder堆叠在一起，decoder层也一样
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>

- **待续：**
>       参考：https://zhuanlan.zhihu.com/p/48508221    详解Transformer （Attention Is All You Need）！！
>           https://zhuanlan.zhihu.com/p/44121378       Transformer详解
>           https://terrifyzhao.github.io/2019/01/11/Transformer%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3.html   Transformer模型详解！！
>           http://blog.itpub.net/31562039/viewspace-2375080/   BERT大火却不懂Transformer？读这一篇就够了 ！！
>           https://blog.csdn.net/pipisorry/article/details/84946653    深度学习：transformer模型
>           https://www.jianshu.com/p/d2ae158fc9e5      Transformer详解（二）：Attention机制
>           http://www.mayexia.com/NLP/Attention%E6%9C%BA%E5%88%B6%E5%92%8CTransformer%E6%A1%86%E6%9E%B6%E8%AF%A6%E8%A7%A3/     Attention机制和Transformer框架详解
>           https://cupdish.com/2018/03/28/attention-is-all-you-need/#Decoder-%E9%83%A8%E5%88%86    Attention is all you need 论文阅读报告及代码详解
>           https://blog.csdn.net/malefactor/article/details/78767781   深度学习中的注意力机制(2017版)
>           http://shukebeta.me/NLP-attention-03-self-attention/    Transformer - 多头自注意力编码机制
>           https://lonepatient.top/2019/01/18/BERT-Transformer.html    Transformer原理和实现 ！！！
>           http://fancyerii.github.io/2019/03/09/transformer-illustrated/  Transformer图解
>
>
>
>
>
>
>
>
>
