## dl - BERT
- **概述：**
>       bert的成名史为 LM -》 word embedding -》 ELMO -》 GPT -》 BERT
>
>       BERT效果好及普适性强
>
>       BERT模型中有几个比较重要的应用：
>           1、pre-training
>               预处理在bert模型中占了很重要的部分
>           2、deep
>           3、bidirectional
>           4、Transformaer
>           5、Language understanding
>
>       bert应用：
>           在NLP中的四大类任务，比如：
>               1、句子关系判断类任务（QA、自然语言推理等）
>               2、单句分类任务（文本分类、情感计算等）
>               3、生成式任务（机器翻译、文本摘要等）
>               4、序列标注类任务（分词、词性标注、NER、语义标注等）
>
>
>

- **图像中的预处理：**
>       对于图像来说一般是CNN的多层叠加网络，比如可以先用某一个训练集和对这个网络进行预先训练，在A或者B任务上学会网络参数，然后保存起来。
>           当有任务C时，网络结构采用之前相同的网络结构，在较浅的几层CNN结构，网络参数初始化的时候可以加载之前训练好的参数，其他CNN高层参数任然随机初始化。
>       之后用C任务的训练集来训练网络，有两种做法：
>           1、Frozen
>               浅层加载的参数在训练C任务过程中不动
>           2、Fine-Tuning
>               底层网络参数尽管被初始化了，在C任务训练过程中仍然随着训练的进程不断改变，就是更好地把参数进行调整使得更适应当前的C任务。
>               一般图像或者视频领域要做预训练一般都这么做。
>
>       预训练的好处：
>           1、训练数据少很难很好地训练这么复杂的网络，如果用已经训练好的模型参数来初始化大部分网络结构参数，
>               然后再用C任务比较少的数据量上Fine-tuning过程去调整参数让它们更适合解决C任务，原先训练不了的任务就能解决了
>           2、如果手头任务训练数据也不少，加个预训练过程也能极大加快任务训练的收敛速度
>
>       一般我们喜欢用ImageNet来做网络的预训练，主要有两点：
>           1、一方面ImageNet是图像领域里有超多事先标注好训练数据的数据集合，分量足是个很大的优势，量越大训练出的参数越靠谱
>           2、一方面因为ImageNet有1000类，类别多，算是通用的图像数据，跟领域没太大关系，所以通用性好，预训练完后哪哪都能用，是个万金油
>

- **NLP中的预处理：**
>       下游NLP任务在使用Word Embedding的时候也类似图像有两种做法，
>           1、Frozen
>               就是Word Embedding那层网络参数固定不动
>           2、Fine-Tuning
>               就是Word Embedding这层参数使用新的训练集合训练也需要跟着训练过程更新掉
>
>       word2vec缺点：
>           无法解决多义词问题
>

- **EMLO：**
>       ELMO是“Embedding from Language Models”
>       提出ELMO的论文题目：“Deep contextualized word representation”，2018年最佳论文
>
>       ELMO本身是个根据当前上下文对Word Embedding动态调整的思路。！！！
>
>       ELMO的本质思想：
>           事先用语言模型学好一个单词的Word Embedding，此时多义词无法区分，不过这没关系。
>           在实际使用Word Embedding的时候，单词已经具备了特定的上下文了，这个时候我可以根据上下文单词的语义去调整单词的Word Embedding表示，
>           这样经过调整后的Word Embedding更能表达在这个上下文中的具体含义，自然也就解决了多义词的问题了。
>           所以ELMO本身是个根据当前上下文对Word Embedding动态调整的思路。
>       
>       ELMO采用了典型的两阶段过程：
>           1、第一个阶段是利用语言模型进行预训练
>           2、第二个阶段是在做下游任务时，从预训练网络中提取对应单词的网络各层的Word Embedding作为新特征补充到下游任务中
>
>       每个单词都能得到对应的三个Embedding：
>           1、单词的Embedding，最底层是单词的Word Embedding
>           2、句法层面的Embedding，往上是第一层双向LSTM中对应单词位置的Embedding，这层编码单词的句法信息更多一些
>           3、语义层面的Embedding，再往上是第二层LSTM中对应单词位置的Embedding，这层编码单词的语义信息更多一些。
>
>       ELMO应用：
>           给予这三个Embedding中的每一个Embedding一个权重a，这个权重可以学习得来，根据各自权重累加求和，将三个Embedding整合成一个。
>           然后将整合后的这个Embedding作为X句在自己任务的那个网络结构中对应单词的输入，以此作为补充的新特征给下游任务使用
>           对每个单词的三个词向量进行加权求和
>
>       ELMO缺点：
>           1、特征抽取器选择方面，ELMO使用了LSTM而不是新贵Transformer，Transformer是谷歌在17年做机器翻译任务的“Attention is all you need”的论文中提出的。
>               研究已经证明了Transformer提取特征的能力是要远强于LSTM的
>           2、拼接方式双向融合特征融合能力可能比Bert一体化的融合特征方式弱（目前并没有实验说明这一点）
>
>
>       预训练方法，常用有两种：
>           1、除了以ELMO为代表的这种基于特征融合的预训练方法外
>           2、称为“基于Fine-tuning的模式”，而GPT就是这一模式的典型开创者
>

- **GPT：**
>       GPT是“Generative Pre-Training”的简称
>       GPT也采用两阶段过程，
>           1、第一个阶段是利用语言模型进行预训练
>           2、第二阶段通过Fine-tuning的模式解决下游任务
>
>       和ELMO不同点：
>           1、特征抽取器不是用的RNN，而是用的Transformer
>               特征抽取能力(一般可以认为Transformer > RNN > CNN)
>           2、GPT的预训练虽然仍然是以语言模型作为目标任务，但是采用的是单向的语言模型
>               “单向”的含义是GPT则只采用Context-before这个单词的上文来进行预测，而抛开了下文
>
>       
>       GPT应用：  
>           1、对于不同的下游任务来说，你要向GPT的网络结构看齐，把任务的网络结构改造成和GPT的网络结构是一样的。
>               然后，在做下游任务的时候，利用第一步预训练好的参数初始化GPT的网络结构，这样通过预训练学到的语言学知识就被引入到你手头的任务里来了
>           2、你可以用手头的任务去训练这个网络，对网络参数进行Fine-tuning，使得这个网络更适合解决手头的问题。
>
>       GPT下游改造：
>           对于分类问题，不用怎么动，加上一个起始和终结符号即可
>           对于句子关系判断问题，比如Entailment，两个句子中间再加个分隔符即可
>           对文本相似性判断问题，把两个句子顺序颠倒下做出两个输入即可，这是为了告诉模型句子顺序不重要
>           对于多项选择问题，则多路输入，每一路把文章和答案选项拼接作为输入即可
>
>
>       GPT缺点：
>           单向语言模型
>

- **Bert：**
>       Bert采用和GPT完全相同的两阶段模型：
>           1、语言模型预训练
>           2、使用Fine-Tuning模式解决下游任务
>
>       Bert和GPT区别：
>           1、预训练阶段采用了类似ELMO的双向语言模型
>           2、另外一点是语言模型的数据规模要比GPT大
>
>       Bert最关键两点：
>           把ELMO的特征抽取器换成Transformer，就得到了Bert
>           把GPT预训练阶段换成双向语言模型，就得到了Bert
>           1、特征抽取器采用Transformer
>           2、预训练的时候采用双向语言模型
>

- **如何在Transformer结构上做双向语言模型任务？**
>       提出了一个叫Masked语言模型来解决这个问题，其实这跟word2vec中的CBOW模型思想很类似
>       核心思想：
>           在做语言模型任务的时候，把要预测的单词抠掉，然后根据上文Context-Before和下文Context-after去预测单词。
>
>       Masked双向语言模型：
>           随机选择语料中15%的单词，把它抠掉，也就是用[Mask]掩码代替原始单词，然后要求模型去正确预测被抠掉的单词。
>           直接使用masked的问题：
>               训练过程大量看到[mask]标记，但是真正后面用的时候是不会有这个标记的，这会引导模型认为输出是针对[mask]这个标记的，
>                   但是实际使用又见不到这个标记，这自然会有问题。
>           Bert进行了改造：
>               15%的被随机选中要执行[mask]替身这项光荣任务的单词中，只有80%真正被替换成[mask]标记，10%被狸猫换太子随机替换成另外一个单词，
>                   10%情况这个单词还待在原地不做改动。
>
>      Bert创新点：
>           1、Next Sentence Prediction
>           指的是做语言模型预训练的时候，分两种情况选择两个句子：
>               (1)、选择语料中真正顺序相连的两个句子
>               (2)、第二个句子从语料库中抛色子，随机选择一个拼到第一个句子后面
>           要求模型除了做上述的Masked语言模型任务外，附带再做个句子关系预测，判断第二个句子是不是真的是第一个句子的后续句子。
>           之所以这么做，是考虑到很多NLP任务是句子关系判断任务，单词预测粒度的训练到不了句子关系这个层级，增加这个任务有助于下游句子关系判断任务。
>           可以看到，它的预训练是个多任务过程。这也是Bert的一个创新。
>
>       Bert输入：
>           输入部分是个线性序列，两个句子通过分隔符分割，最前面和最后增加两个标识符号
>           每个单词有三个embedding：
>               1、位置信息embedding
>                   因为NLP中单词顺序是很重要的特征，需要在这里对位置信息进行编码
>               2、单词embedding
>                   单词embedding
>               3、句子embedding
>                   前面提到训练数据都是由两个句子构成的，那么每个句子有个句子整体的embedding项对应给每个单词。把单词对应的三个embedding叠加，就形成了Bert的输入
>

- **BERT运用的主要思想：**
>       主要有三个方面的：
>           1、两阶段模型
>               第一阶段双向语言模型预训练，这里注意要用双向而不是单向
>               第二阶段采用具体任务Fine-tuning或者做特征集成
>           2、特征抽取
>               特征抽取要用Transformer作为特征提取器而不是RNN或者CNN
>           3、双向语言模型
>               双向语言模型可以采取CBOW的方法
>

- **预训练的好处和作用：**
>       本质上预训练是通过设计好一个网络结构来做语言模型任务，然后把大量甚至是无穷尽的无标注的自然语言文本利用起来，预训练任务把大量语言学知识抽取出来编码到网络结构中，
>       当手头任务带有标注信息的数据有限时，这些先验的语言学特征当然会对手头任务有极大的特征补充作用，因为当数据有限的时候，很多语言学现象是覆盖不到的，泛化能力就弱，
>       集成尽量通用的语言学知识自然会加强模型的泛化能力。如何引入先验的语言学知识其实一直是NLP尤其是深度学习场景下的NLP的主要目标之一，
>       不过一直没有太好的解决办法，而ELMO/GPT/Bert的这种两阶段模式看起来无疑是解决这个问题自然又简洁的方法，这也是这些方法的主要价值所在
>

- **bert中文预训练：**
>       bert中文预训练时，用字作为bert输入应该更合适，因为中文的词是没有穷尽的，但是常用的字是可以穷举的。
>
>
>
>
>
>
>
>

- **待续：**
>       参考：https://zhuanlan.zhihu.com/p/49271699    从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史
>           https://blog.csdn.net/Kaiyuan_sjtu/article/details/83991186  NLP大杀器BERT模型解读
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
