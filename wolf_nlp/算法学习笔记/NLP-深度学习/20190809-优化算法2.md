## 深度学习 - 优化算法
- **概述**：
>       深度学习优化算法发展：
>           SGD -> SGDM -> NAG ->AdaGrad -> AdaDelta -> Adam -> Nadam
>
>       优化算法的演变历史，都是基于对数据的某种假设而进行的优化，那么某种算法是否有效，就要看你的数据是否符合该算法的胃口了。！！！
>       算法固然美好，数据才是根本。
>
>       常见的优化方法：
>           1、Batch Gradient Descent
>               在每一轮的训练过程中，Batch Gradient Descent算法用整个训练集的数据计算cost fuction的梯度，并用该梯度对模型参数进行更新
>           2、Stochastic Gradient Descent
>               Stochastic gradient descent 算法每读入一个数据，便立刻计算cost fuction的梯度来更新参数
>               SGD一般都指mini-batch gradient descent
>           3、Mini-batch Gradient Descent
>               SGD一般都指mini-batch gradient descent
>               mini-batch Gradient Descent的方法是在上述两个方法中取折衷, 每次从所有训练数据中取一个子集（mini-batch） 用于计算梯度
>               不仅计算效率高，而且收敛较为稳定
>               缺点：
>                   (1)、在到达最优点的时候并不能够总是真正到达最优点，而是在最优点附近徘徊，因为其更新方向完全依赖于当前batch计算出的梯度，因而十分不稳定
>                   (2)、选一个合适的学习率，采用小的学习率的时候，会导致网络在训练的时候收敛太慢；采用大的学习率的时候，会导致在训练过程中优化的幅度跳过函数的范围，也就是可能跳过最优点
>
>           4、Momentum
>               Momentum算法借用了物理中的动量概念，它模拟的是物体运动时的惯性，主要是基于梯度的移动指数加权平均
>               利用了类似与移动指数加权平均的方法来对网络的参数进行平滑处理的，让梯度的摆动幅度变得更小
>               更新的时候在一定程度上保留之前更新的方向，同时利用当前batch的梯度微调最终的更新方向
>               优点：
>                   可以在一定程度上增加稳定性，从而学习地更快，并且还有一定摆脱局部最优的能力
>               Momentum算法会观察历史梯度v_{t-1}，若当前梯度的方向与历史梯度一致（表明当前样本不太可能为异常点），则会增强这个方向的梯度，若当前梯度与历史梯方向不一致，则梯度会衰减
>           5、Nesterov Momentum
>               在小球向下滚动的过程中，希望小球能够提前知道在哪些地方坡面会上升，这样在遇到上升坡面之前，小球就开始减速
>               缺点：
>                   每一个参数θ_i的训练都使用了相同的学习率α
>           6、Adagrad
>               Adagrad算法能够在训练中自动的对learning rate进行调整，对于出现频率较低参数采用较大的α更新；相反，对于出现频率较高的参数采用较小的α更新
>               Adagrad非常适合处理稀疏数据
>               缺点：
>                   Adagrad的缺点是在训练的中后期，分母上梯度平方的累加将会越来越大，从而梯度趋近于0，使得训练提前结束
>           7、RMSprop
>               RMSprop是Geoff Hinton提出的一种自适应学习率方法
>               进一步优化损失函数在更新中存在摆动幅度过大的问题，进一步加快函数的收敛速度，RMSprop算法对权重和偏置的梯度使用了微分平方加权平均数。
>               Adagrad会累加之前所有的梯度平方，而RMSprop仅仅是计算对应的平均值，因此可缓解Adagrad算法学习率下降较快的问题
>               优点：
>                   (1)、RMSProp算法对梯度计算了微分平方加权平均数。这种做法有利于消除了摆动幅度大的方向，用来修正摆动幅度，使得各个维度的摆动幅度都较小。
>                   (2)、使得网络函数收敛更快（比如当dW或者db中有一个值比较大的时候，那么我们在更新权重或者偏置的时候除以它之前累积的梯度的平方根，这样就可以使得更新幅度变小）
>                       为了防止分母为零，使用了一个很小的数值 ϵ 来进行平滑，一般取值为10^(-8)
>           8、Adam
>               Adam(Adaptive Moment Estimation)是另一种自适应学习率的方法
>               Adam就是将Momentum和RMSprop结合起来使用的一种算法，结合了梯度的移动指数加权平均数和微分平方加权平均数
>               利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率
>               优点：
>                   Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳
>

- **优化算法的常用tricks：**
>       1、各大算法孰优孰劣并无定论。如果是刚入门，优先考虑SGD+Nesterov Momentum或者Adam
>       2、选择你熟悉的算法
>           这样你可以更加熟练地利用你的经验进行调参
>       3、充分了解你的数据
>           如果模型是非常稀疏的，那么优先考虑自适应学习率的算法
>       4、根据你的需求来选择
>           在模型设计实验过程中，要快速验证新模型的效果，可以先用Adam进行快速实验优化；在模型上线或者结果发布前，可以用精调的SGD进行模型的极致优化
>       5、先用小数据集进行实验
>           有论文研究指出，随机梯度下降算法的收敛速度和数据集的大小的关系不大。
>           因此可以先用一个具有代表性的小数据集进行实验，测试一下最好的优化算法，并通过参数搜索来寻找最优的训练参数
>       6、考虑不同算法的组合
>           先用Adam进行快速下降，而后再换到SGD进行充分的调优
>       7、数据集一定要充分的打散（shuffle）
>           这样在使用自适应学习率算法的时候，可以避免某些特征集中出现，而导致的有时学习过度、有时学习不足，使得下降方向出现偏差的问题
>       8、训练过程中持续监控训练数据和验证数据上的目标函数值以及精度或者AUC等指标的变化情况。
>           对训练数据的监控是要保证模型进行了充分的训练——下降方向正确，且学习率足够高；对验证数据的监控是为了避免出现过拟合。
>       9、制定一个合适的学习率衰减策略
>           可以使用定期衰减策略，比如每过多少个epoch就衰减一次；或者利用精度或者AUC等性能指标来监控，当测试集上的指标不变或者下跌时，就降低学习率。
>

- **稀疏数据和模型稀疏：**
>       数据稀疏主要指的是数据特征的稀疏性，高维特征中仅有少数非零特征
>       模型稀疏主要指的是模型参数的稀疏性，高维参数中仅有少数非零参数
>

- **为什么adam适合稀疏数据？**
>       根据adam的公式 当特征稀疏时 对应的学习率会大 当特征稠密时 对应的学习率会小
>       当某维特征稀疏时 该特征对应的参数w的学习率会大
>
>
>

- **待续：**
>       参考：https://zhuanlan.zhihu.com/p/22252270    深度学习最全优化方法总结比较（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam）
>           https://hellozhaozheng.github.io/z_post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E6%95%B4%E7%90%86%E6%80%BB%E7%BB%93/    各种优化方法整理总结
>           https://blog.csdn.net/willduan1/article/details/78070086    深度学习优化算法解析(Momentum, RMSProp, Adam)
>           https://zhuanlan.zhihu.com/p/32230623   一个框架看懂优化算法之异同 SGD/AdaGrad/Adam（Juliuszh 清华大学 计算机科学与技术博士） 优化算法系列
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
