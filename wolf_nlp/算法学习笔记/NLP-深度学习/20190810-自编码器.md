## 深度学习 - 自编码器
- **概述**：
>       自编码器（Auto-Encoder）是一个较早的概念了，比如Hinton等人在1986, 1989年的工作
>       从Hinton 2006年的工作之后，越来越多的研究者开始关注各种自编码器模型相应的堆叠模型
>       自编码器是一种无监督学习方法，可用于数据降维及特征抽取
>       自编码器由编码器（Encoder）和解码器（Decoder）两部分组成
>
>       最常见的三大降维技术：
>           1、PCA
>               给定同样的数据，PCA 总是会给出同样的答案（而其它两种方法却不是这样）
>           2、t-SNE
>           3、自编码器
>

- **PCA：**
>       使用 PCA 就是在另一个角度看待真实数据——这是 PCA 独有的特性，因为其它方法都是始于低维数据的随机表征，然后使其表现得就像是高维数据。
>
>       实现方法有两种：
>           1、Eigen 分解
>           2、奇异值分解（SVD）
>

- **t-SNE：**
>       t-SNE 是一种相对较新的方法，起源于 2008 年的论文《Visualizing Data using t-SNE》
>

- **自编码器（Auto Encoders）：**
>       PCA 和 t-SNE 是方法，而自编码器则是一系列的方法
>       自编码器的目的是，让输出x'尽可能复现输入x，即tries to copy its input to its output。！！！
>       编码y和输入x不同的情况下，系统还能够去复原原始信号x，那么说明编码y已经承载了原始数据的所有信息
>
>       自编码器是一种神经网络，其目标是通过使用比输入节点更少的隐藏节点（在编码器一端）预测输入（训练该网络使其输出尽可能与输入相似），为此该网络需要尽可能多地将信息编码到隐藏节点中。
>       因为自编码器本质上是神经网络，所以它还具有更大的力量，甚至可以在训练时给类和样本加权，从而为数据中的特定现象提供更大的重要性
>
>       1、当选择的特征小于原始特征：
>           即隐层维度小于输入数据维度，即降维操作
>
>       2、当选择的特征大于原始特征：
>           即隐层维度大于输入数据维度
>           比如同时约束h的表达尽量稀疏（有大量维度为0，未被激活），此时的编码器便是大名鼎鼎的“稀疏自编码器”
>           从特征的角度来看更直观些，稀疏的表达意味着系统在尝试去特征选择，找出大量维度中真正重要的若干维
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>

- **待续：**
>       参考：https://kexue.fm/archives/5253   变分自编码器（一）：原来是这么一回事
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
