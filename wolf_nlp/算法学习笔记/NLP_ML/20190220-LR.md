## NLP_ML - LR
- **概述：**
>
>       LR是一种广义线性回归
>       与线性回归的区别：
>           LR和多重线性回归最大的区别就在于它们的因变量不同，其他的基本差不多，这两种回归都归于同一个家族，即广义线性模型。
>           这一家族模型形式都差不多：
>               如果是连续的，就是多重线性回归
>               如果是二项分布，就是LR回归
>               如果是Poisson分布，就是Poisson回归
>               如果是负二项分布，就是负二项分布
>       LR其实仅为在线性回归的基础上，只是特征到结果的映射过程中加了一层函数映射，即sigmoid函数。
>       对输入实例x进行分类的线性表达式θT，其值域为实数域，通过LR模型的表达式可以将线性函数θT将x的结果映射到(0,1)。
>
>       线性回归输出是连续值，LR输出的是离散值，而且前者的损失函数是输出y的**高斯分布**，后者损失函数是输出的**伯努利分布**。
>
>       （最大熵在解决二分类问题时就是LR，在解决多分类问题时就是多项逻辑回归。）
>
>

- **LR：**
>
>       损失函数：cost
>           方法1：
>               用与衡量均方误差[(估计值-实际值)^2/n]最小，即预测的准确性，需要损失函数最小，得到的参数才最优。
>               但是LR的这种损失函数非凸，不能找到全局最优点
>           方法2：
>               L(θ) = ∏P(yi|xi;θ)  似然函数，可以求出似然函数最大的参数估计
>               转换为对数似然函数：l(θ) = log(L(θ))  通过梯度下降法求出参数θ
>               如下图，
> ![avatar](https://github.com/nwaiting/wolf-ai/blob/master/wolf_others/pic/pgm_lr_cost_function.png)
>
>           为了防止过拟合，可以添加L2正则化
>

- **LR优缺点：**
>
>       优点：
>           速度快、简单、容易看到各个特征的权重
>           能容易的更新模型吸收新的数据
>           对LR而言，多重共线性并不是问题，可以结合L2正则化来解决该问题
>
>       缺点：
>           对数据和场景适应能力有限，不如决策树算法那么强
>           当特征空间很大时，逻辑回归的性能不是很好，不能很好的处理大量多类特征或变量
>           容易欠拟合，准确度不是很高
>           使用前提：自变量和因变量是线性关系
>

- **LR应用经验：**
>
>       如果是连续变量，注意scaling，缩放单位即标准化。
>           多维特征的训练数据进行回归采取梯度求解时其特征值必须做scale，确保特征的取值范围在相同的尺度内计算过程才会收敛更快
>
>       LR对样本分布敏感，所以要注意样本的平衡性，样本量足的情况下采用下采样，不足的情况用上采样
>       LR对于特征处理非常重要，常用处理手段包括：
>                               通过组合特征引入特性化因素(FM,FFM)；
>                               注意特征的频度；聚类，hash；
>
>       LR算法调优：
>           选择合适的正则化；
>           正则化系数；
>           收敛阈值e；
>           迭代轮数；
>           调整loss function给定不同权重；
>           Bagging或其他方式的模型融合
>           最优算法选择("newton","lbfgs","liblinear")
>
>       Sklearn中的LR实际上是liblinear的封装
>
>
>
>
>
>

- **待续：**
>       参考：https://blog.csdn.net/lsc989818/article/details/79465260
>
>
>
>
>
>
>
>
>
>
>
>
>
>
