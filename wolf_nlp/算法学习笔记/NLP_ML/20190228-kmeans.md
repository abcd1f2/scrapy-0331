## NLP_ML - ML算法之kmeans
- **概述：**
>       K-means是聚类算法中最简单的一种了，但里面包含的思想不一般。
>
>
>
>
>
>
>

- **Kmeans算法**
>       K-means算法是将样本聚类成k个族(cluster)，具体算法如下图，
> ![avatar](https://github.com/nwaiting/wolf-ai/blob/master/wolf_others/pic/nlp_ml_kmeans_alg.png)
>
> ![avatar](https://github.com/nwaiting/wolf-ai/blob/master/wolf_others/pic/nlp_ml_kmeans_alg_distance.png)
>       如图，对于函数的收敛性，
>           1、固定每个类的质心，调整每个样例所属的类别可以使J减小
>           2、固定类别，调整每个类的质心也可以使J较小
>           两个过程都是内循环中使J单调递减的过程。当J递减到最小，函数收敛。
>

- **k-means应用经验：**
>       应用经验：
>           由于畸变函数J是非凸函数，即不能保证取到全局最小值，一般情况下k-means的局部最优已经满足需求，
>               如果担心陷入局部最优，可以跑多遍K-means，然后选取最小的J作为输出。
>
>       kmeans的几种优化技巧：
>           k-means++
>           k-means对样本可以通过采样，赋予权重
>           可以通过方差值判定样本聚类是否合理
>
>

- **Kmeans与EM的关系：**
>       就是求每个样例x的隐含类别y，然后利用隐含类别将x归类。
>       首先可以对每个样例假定一个y，然后使用样本的极大似然估计来度量假定的是否正确，如果找到的y能够是P(x,y)最大，那么此时的y就是最佳的类别。
>           在给定y的情况下，可以调整其他参数使P(x,y)最大，调整完参数后，发现有更好的y可以指定，那么重新指定y，
>           然后再计算P(x,y)最大时的参数，反复迭代直至没有更好的y可以指定。
>
>       K-meas其实就是EM的体现，E步确定隐含类别变量c，M步更新其他参数u来是J最小化。
>
>
>
>
>
>
>
>
>

- **待续：**
>       参考：https://coolshell.cn/articles/7779.html  K-MEANS 算法
>           https://www.cnblogs.com/jerrylead/archive/2011/04/06/2006910.html   K-means聚类算法
>
>
>
>
>
>
>
>
>
>
>
>
>
>
