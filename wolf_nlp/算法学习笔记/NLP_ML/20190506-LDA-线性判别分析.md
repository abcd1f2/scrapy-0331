## ML - LDA 线性判别分析
- **概述：**
>       总结：学习一个算法的主要是学习算法的思想、算法的idea，其实很多算法都是相通相似的，所以学习一个新算法的时候，掌握其idea最重要
>
>       LDA是在机器学习、数据挖掘领域比较经典和热门的一个算法
>
>       PCA、SVD、LDA这几个模型比较相近，都有自己的特点
>
>
>

- **应用：**
>       LDA在百度搜索里面用过这方面的算法
>
>

- **原理：**
>       LDA与PCA比较相似，LDA是supervised learning，PCA是unsupervised learning。
>       LDA通常作为一个独立的算法，给定训练数据后，将会得到一系列的判别函数，对于新的输入，就可以进行预测了。
>       PCA更像是一个预处理的方法，将原本的数据降低维度，而使得降低了维度的数据之间的方差最大。
>       LDA：
>           首先是最大化投影后的方差，其次是最小化投影后的损失(投影后产生的损失最小)
>           最大化方差方法：
>               运用拉格朗日乘子法，以u的归一化条件为限制，求导等于0，最后得到S*u = λ*u
>               u^T * S*u = λ，当u为最大的特征值λ对应的特征向量时，方差会达到最大值，这个特性向量被称为第一主成分。
>                   然后找出与现有方向正交的所有可能方向中，将新的方向选择为最大化投影方差的方向，以此类推，得到协方差矩阵S的M个特征向量u1,u2,..um，对应于m个最大特征值λ1,λ2,...λm
>               (注：特征值的求解有很多方法，求一个D*D的矩阵的时间复杂度是O(D^3)，也有一些求Top M的方法，如power method，时间复杂度是O(D^2*M)，求特征值是一个很浪费时间的操作。)
>
>               这是一个标准的特征值表达式，λ对应的特征值，u对应的特征向量
>               上式中取得最大值的条件就是λ最大，就是取得最大的特征值的时候。
>               如将一个D维的数据空间投影到M维的数据空间中(M<D)，那取前M个特征向量构成的投影矩阵就是能够使得方差最大的矩阵了。
>
>           最小化损失方法：
>               假设数据x是在D维空间中的点，可以用D个正交的D维向量去完全表示这个空间(这个空间中所有的向量都可以用这D个向量的线性组合得到)
>               在D维空间中，有无穷多种可能找这D个正交的D维向量，哪个组合是最合适的呢?
>               损失最小，当损失函数最小的即为最合适的D维向量的组合
>
>       PCA：
>           PCA的思想是将n维特征映射到k维上（k<n），这k维是全新的正交特征。这k维特征称为主元，是重新构造出来的k维特征，而不是简单地从n维特征中去除其余n-k维特征。
>               就是对原始的空间中顺序地找一组相互正交的坐标轴，第一个轴是使得方差最大，第二个轴是在与第一个轴正交的平面中使得方差最大，第三个轴是在与第1、2个轴正交的平面中方差最大，
>               这样假设在N维空间中，可以找到N个这样的坐标轴，取前r个去近似这个空间，这样就从一个N维空间压缩到r维空间，但是选择的r个坐标轴能够使得空间的压缩损失最小。！！！
>           得到的前k大特征值对应的特征向量就是最佳的k维新特征，而且这k维新特征是正交的。得到前k个u以后，样例xi通过变换可以得到新的样本，
>               其中的第j维就是xi在uj上的投影
>               通过选取最大的k个u，使得方差较小的特征(如噪声)被丢弃
>
>
>
>
>
>
>
>
>
>

- **待续：**
>       参考：http://www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html      机器学习中的数学(4)-线性判别分析（LDA）, 主成分分析(PCA)
>           https://www.cnblogs.com/jerrylead/archive/2011/04/18/2020209.html   主成分分析（Principal components analysis）-最大方差解释
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
