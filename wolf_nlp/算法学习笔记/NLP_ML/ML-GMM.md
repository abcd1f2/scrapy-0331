## NLP_ML - GMM
- **概述：**
>       归纳偏执：
>           没有归纳偏执或者归纳偏执太宽泛会导致overfitting，然而另一个极端--限制过大的归纳偏执也是有问题的。
>               如，数据本身并不是线性的，强行用线性函数去做回归通常并不能得到好的结果。
>
>       从中心极限定理可以看出，Gaussian分布这个假设其实是比较合理的。
>           虽然我们可以用不同的分布来随意地构造 XX Mixture Model ，但是还是 GMM 最为流行。
>           另外，Mixture Model本身其实也是可以变得任意复杂的，通过增加Model的个数，我们可以任意地逼近任何连续的概率密分布。
>
>       GMM，就是假设数据服从Mixture Gaussian Distribution，即数据可以看做是从数个Gaussian Distribution中生成出来的。
>

- **中心极限定理：**
>       中心极限定理，是指概率论中讨论随机变量序列部分和分布渐近于正态分布的一类定理。
>       这组定理是数理统计学和误差分析的理论基础，指出了大量随机变量近似服从正态分布的条件。它是概率论中最重要的一类定理，有广泛的实际应用背景。
>       在自然界与生产中，一些现象受到许多相互独立的随机因素的影响，如果每个因素所产生的影响都很微小时，总的影响可以看作是服从正态分布的。
>       中心极限定理就是从数学上证明了这一现象。最早的中心极限定理是讨论重点，伯努利试验中，事件A出现的次数渐近于正态分布的问题
>

- **大数定理：**
>       概率论历史上第一个极限定理属于伯努利，后人称之为“大数定律”。概率论中讨论随机变量序列的算术平均值向随机变量各数学期望的算术平均值收敛的定律
>       在随机事件的大量重复出现中，往往呈现几乎必然的规律，这个规律就是大数定律。
>       通俗地说，这个定理就是，在试验不变的条件下，重复试验多次，随机事件的频率近似于它的概率。偶然中包含着某种必然。
>
>

- **GMM做clustering原理：**
>       如果有了数据，假定它们是由GMM生成出来的，那么我们只要根据数据推出GMM的概率分布来就可以了，然后GMM的K个Component实际上就对应了K个cluster 了
>       根据数据来推算概率密度通常被称作density estimation，特别地，当我们在已知（或假定）了概率密度函数的形式，而要估计其中的参数的过程被称作“参数估计”
>
>       假设我们有 N 个数据点，并假设它们服从某个分布(记作 p(x))，现在要确定里面的一些参数的值，例如，在GMM中，就需要确定pi_k、u_k 和sigma_k这些参数。
>       思想是，找到这样一组参数，它所确定的概率分布生成这些给定的数据点的概率最大，而这个概率实际上就等于∏p(x_i)，我们把这个乘积称作似然函数 (Likelihood Function)。
>       然后求解对数似然函数的最大值，即得到最优参数
>

- **二维GMM：**
>       如下图，
> ![avatar](https://github.com/nwaiting/wolf-ai/blob/master/wolf_others/math_pic/math_gmm_multi_vari.png)
>
>

- **待续：**
>       参考：http://blog.pluskid.org/?p=39&cpage=1#comments   Clustering (3): Gaussian Mixture Model
>       https://blog.csdn.net/u011582757/article/details/70003351   EM算法--二维高斯混合模型(GMM)
>       http://jermmy.xyz/2017/10/28/2017-10-28-mutil-modal-gaussian/   多维高斯分布
>       https://blog.csdn.net/jinping_shi/article/details/59613054  高斯混合模型（GMM）及其EM算法的理解
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
