## NLP_ML - 模型正则化
- **概述：**
>       L0正则化的值是模型参数中非零参数的个数
>       L1正则化表示各个参数绝对值之和
>       L2正则化标识各个参数的平方的和的开方值
>

- **模型参数的选择：**
>       参数的稀疏有什么好处？
>       解析：
>           1、一个好处是可以简化模型，避免过拟合。因为一个模型中真正重要的参数可能并不多，如果考虑所有的参数起作用，那么可以对训练数据可以预测的很好，但是对测试数据就只能呵呵了
>           2、另一个好处是参数变少可以使整个模型获得更好的可解释性
>
>       参数值越小代表模型越简单吗？
>       解析：
>           因为越复杂的模型，越是会尝试对所有的样本进行拟合，甚至包括一些异常样本点，这就容易造成在较小的区间里预测值产生较大的波动，
>           这种较大的波动也反映了在这个区间里的导数很大，而只有较大的参数值才能产生较大的导数。因此复杂的模型，其参数值会比较大。
>

- **L0正则化：**
>       因为L0正则化很难求解，是个NP难问题，因此一般采用L1正则化。！！！
>
>       稀疏的参数可以防止过拟合，因此用L0范数（非零参数的个数）来做正则化项是可以防止过拟合的
>       从直观上看，利用非零参数的个数，可以很好的来选择特征，实现特征稀疏的效果，具体操作时选择参数非零的特征即可。
>       但因为L0正则化很难求解，是个NP难问题，因此一般采用L1正则化。
>       L1正则化是L0正则化的最优凸近似，比L0容易求解，并且也可以实现稀疏的效果
>

- **L1正则化：**
>       L1正则化在实际中往往替代L0正则化，来防止过拟合。在江湖中也人称Lasso
>       L1正则化之所以可以防止过拟合，是因为L1范数就是各个参数的绝对值相加得到的
>       参数值大小和模型复杂度是成正比的。因此复杂的模型，其L1范数就大，最终导致损失函数就大，说明这个模型就不够好
>
>
>
>
>
>
>
>
>
>

- **待续：**
>       参考：
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
