## NLP_ML - ML算法之kmeans
- **概述：**
>       K-means是聚类算法中最简单的一种了，但里面包含的思想不一般。
>           K-Means适用于凸数据集
>           凸形状是指当选择形状内任意两点，这两点的连线上所有点，也都在形状内。
>           这里凸形状的簇就是在k-means 算法里，归于同一类的多点行成的簇，都是凸形状的
>
>       kmeans变种：
>           1、初始化优化K-Means++
>               a、先初始化一个聚类中心u1
>               b、通过概率在选择一个中心u2，距离u1越远，选中的概率越大
>               重复a、b步骤，选出k个质心
>           2、距离计算优化elkan K-Means
>               思想：
>                   减少不必要的距离计算。利用三角形的两边之和大于第三边，以及两边之差小于第三边，来减少距离的计算
>               缺点：
>                   如果样本特征是稀疏的，有缺失值的，此时某些距离无法计算，则不能使用这个方法
>           3、大数据情况下的优化Mini Batch K-Means
>               思想：
>                   在Mini Batch K-Means中，选择一个合适的批样本大小batch size，来做传统的K-Means。batch size的样本一般通过随机采样得到的。
>                   为了增加算法准确性，可以多跑几次Mini Batch K-Means算法，用得到不同的随机采样集来得到聚类族
>
>

- **聚类的应用：**
>       聚类技能作为一个单独的过程，用于找寻数据内在的分布结构，也可作为分类等其他学习任务的前驱过程。
>       如：
>           商业应用中需要对用户类型进行判别，直接机芯用户类型判断比较困难，可以先对用户进行聚类，根据聚类结果将每个族定义为一个类，然后再基于这些类训练分类模型，用于判别用户类型
>

- **聚类性能度量：**
>       聚类性能度量有两类：
>           1、外部指标
>               聚类结果与某个参数模型进行比较
>               常用方法：
>                   Jaccard系数
>                   FM指数
>                   Rand指数
>                   F值
>                   互信息
>                   平均廊宽
>           2、内部指标
>               直接考察聚类结果而不用任何参考模型
>               评估使用的距离计算：
>                   族内样本间的平均距离
>                   族内样本间的最远距离
>                   族与族最近样本间的距离
>                   族与族中心点间的距离
>               常用方法：
>                   DB指数（值越小越好，公式参考<<机器学习>> 周志华 9.12）
>                   Dunn指数（值越大越好，公式参考<<机器学习>> 周志华 9.13）
>

- **距离度量：**
>       距离计算：
>           1、闵可夫斯基距离（当p=2时，即为欧氏距离，当p=1时，即为曼哈顿距离）
>               可用于计算有序属性（如{1,2,3}这类属性为有序属性，{蓝，绿，红}为无序属性）
>           2、VDM(value difference metric)
>               可用于计算无序属性
>           3、闵可夫斯基距离和VDM结合：
>               处理混合属性
>           当不同属性的重要性不同时，可使用"加权距离"
>
>       非度量距离：
>           如：人和马分别于人马相似，距离较小，但是人和马距离很大
>           这种非度量距离，不再满足直递性
>
>

- **K-Means缺点：**
>       1、K值选取不好把握
>       2、对于不是凸的数据集比较难收敛
>       3、如果各隐含类别的数据不均衡，比如各隐含类别的数据严重失衡，或隐含类别的方差不同，则聚类效果不佳
>       4、代用迭代方法，得到的结果只是局部最优
>       5、对噪声和异常点比较敏感
>
>

- **Kmeans算法**
>       K-means算法是将样本聚类成k个族(cluster)，具体算法如下图，
> ![avatar](https://github.com/nwaiting/wolf-ai/blob/master/wolf_others/pic/nlp_ml_kmeans_alg.png)
>
> ![avatar](https://github.com/nwaiting/wolf-ai/blob/master/wolf_others/pic/nlp_ml_kmeans_alg_distance.png)
>       如图，对于函数的收敛性，
>           1、固定每个类的质心，调整每个样例所属的类别可以使J减小
>           2、固定类别，调整每个类的质心也可以使J较小
>           两个过程都是内循环中使J单调递减的过程。当J递减到最小，函数收敛。
>

- **k-means应用经验：**
>       应用经验：
>           由于畸变函数J是非凸函数，即不能保证取到全局最小值，一般情况下k-means的局部最优已经满足需求，
>               如果担心陷入局部最优，可以跑多遍K-means，然后选取最小的J作为输出。
>
>       kmeans的几种优化技巧：
>           k-means++
>           k-means对样本可以通过采样，赋予权重
>           可以通过方差值判定样本聚类是否合理
>
>

- **Kmeans与EM的关系：**
>       就是求每个样例x的隐含类别y，然后利用隐含类别将x归类。
>       首先可以对每个样例假定一个y，然后使用样本的极大似然估计来度量假定的是否正确，如果找到的y能够是P(x,y)最大，那么此时的y就是最佳的类别。
>           在给定y的情况下，可以调整其他参数使P(x,y)最大，调整完参数后，发现有更好的y可以指定，那么重新指定y，
>           然后再计算P(x,y)最大时的参数，反复迭代直至没有更好的y可以指定。
>
>       K-meas其实就是EM的体现，E步确定隐含类别变量c，M步更新其他参数u来是J最小化。
>
>


- **待续：**
>       参考：<<机器学习>> 周志华
>           https://coolshell.cn/articles/7779.html  K-MEANS 算法
>           https://www.cnblogs.com/jerrylead/archive/2011/04/06/2006910.html   K-means聚类算法
>
>
>
>
>
>
>
>
>
>
>
>
>
>
