### NLP汉语自然语言处理原理与实践（四）
- **概率图模型**
> ### 概述:
>  概率图模型结合了概率论与图论的知识，用图模式(节点和边)表达基于概率相关关系的模型的总称
>
> 我们解决非确定性问题的传统思路就是利用概率论的思想，但是随着问题的复杂性不断增加，传统的概率方法显得力不从心。图模型的引入使人们可以将复杂问题适当分解。变量为**节点**，变量之间的关系为**边**，根据图结构进行训练和计算推理得出最终的结果
>
>** 概率图理论：**
>
> <b>1、概率图模型表示理论</b>
>
> <b>2、概率图模型推理理论</b>
>
> <b>3、概率图模型学习理论</b>
>
> **概率图模型几个基本问题：**
>
> 常用的概率图模型：贝叶斯模型、最大熵模型、条件随机场模型、隐马模型、马尔科夫随机场
>
> 概率图模型两类结构：
>       有向无环图(有向图模型)：贝叶斯网、隐马模型(HMM)
>       无向图(无向图模型)：条件随机场、马尔科夫随机场
>
>
> **三个常见基本问题：**
> - 模型的表示
>
>   贝叶斯网络采用**有向无环图**表示变量关系，马尔科夫随机场采用**无向图**表示变量关系
>
>   隐马尔科夫模型、最大熵模型、条件随机场都是从马尔科夫模型发展出来的
>
> - 模型的学习
>
>  <b>模型的学习精度影响：</b>
>
>  <b>1、语料库样本集对总体的代表性</b>
>
>  影响算法的外因
>
>  <b>2、模型算法的理论基础及所针对的问题</b>
>
>   不同模型因为原理不同，能够处理的语言问题也不同
>
>       * 朴素贝叶斯模型在处理短文本分类精度很高
>       * 最大熵模型在处理中文词性标注表现很好
>       * 条件随机场模型处理中文分词、语义组块等方面精度很高
>       * Semi-CRF在处理命名实体识别方面精度很高
>
>   <b>3、模型算法的复杂度</b>
>
>   这个是属于一个工程问题，算法复杂度对参数估计的精度会产生影响，如果要求参数估计的精度越高，则算法一般就越复杂，学习效率就越低，但在推断阶段的预测结果精度就越高。在中文分词上，集中不同实现的CRF精度就不同
>
> - 模型的预测
>
>   得到观测序列的条件概率最大即为预测结果

- **产生式模型和判别式模型**
> ### 概述：
>       朴素贝叶斯(NB)
>       隐马尔科夫模型(HMM)
>       最大熵模型(ME)
>       条件随机场(CRF)
> <b>判别式模型</b>
>
>   特点：在有限样本条件下建立判别函数，寻找不同数据间的最优分类面，目标是实现分类
>
>   区别：估计条件概率分布
>
>   性能及应用：性能较高，所有的序列标注和结构化学习
>
> <b>产生式模型</b>
>
>   特点：首先建立样本的联合概率分布，再利用模型进行推理预测，要求已知样本无穷或尽可能大
>
>   区别：估计联合概率分布
>
>   性能及应用：性能较低，文本分类和词性标注等
>
> **NLP算法的通用策略：**
>
>       1、如何设计语言模型
>
>       2、如何求解算法策略
>
> **机器学习领域最常用的策略：**
>
>       1、对所考虑的问题建模，先构造一个目标函数
>
>       2、对目标函数进行优化，求得一组最优参数
>
>       3、利用最优参数预测

- **极大释然估计**
> ### 概述：
>   模型的训练过程就是统计学中的参数估计过程。参数估计就是通过若干次实验，观察其结果，利用结果推断出参数大概值的方法，称为极大似然估计
>
> 极大似然估计：已知某个参数能使这个样本出现的概率最大，所以就把这个参数作为估计的真实值。
>
> 极大似然估计：求出联合概率分布的最大值使得似然函数取极大值，求出参数，这种方法就是极大似然估计
>       1、写出似然函数
>       2、对似然函数取对数，并真理
>       3、求导数
>       4、解似然方程
>
- **隐马尔科夫模型HMM**
> ### 概述：
> 马尔科夫链是指时间和状态都是离散的马尔科夫过程。
>
> 在已知系统当前状态的条件下，它未来的演变不依赖过去的演变
>
> 一个马尔科夫过程可以表示为系统在转移过程中，第T+1次结果只受第T次结果的影响，即只与当前状态有关，而与过去状态即初始状态和转移前的所有状态无关
>
> ### 隐马模型(HMM)：
> 一个隐马模型由两组状态集合和三组概率集合构成
>
> 隐状态：真实的状态序列，一个系统在t时刻的状态
>
> 状态转移矩阵：是隐状态空间，即马尔科夫状态空间的状态转移矩阵
>
> 观察状态：显示状态序列
>
> 向量：初始时候观察状态转换到隐状态的概率， 初始化概率向量
>
> 混淆矩阵：发射概率，观察状态转换到隐藏状态的概率矩阵
>
- **最大熵模型**
> ### 概述：
> 1、由于HMM模型属于产生式模型，需要依赖一个联合概率分布，为了得到这个联合概率分布，需要枚举出所有可能的观测序列，这个在NLP的实际运算中非常困难
>
> 2、由于NLP的大量真实语料，观测序列更多地是以一种多重的交互特征形式表现，利用简单的特征函数往往无法涵盖所有的特征
>
> 因此，需要引入一个新的模型，该模型获得的是所有满足约束条件的模型中信息熵极大的标签
>
>
>
>
>
> 
>
