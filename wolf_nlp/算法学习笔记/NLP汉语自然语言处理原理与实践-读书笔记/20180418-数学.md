### NLP汉语自然语言处理原理与实践-数学知识
- **常用概念：**
>       回归：比如在线性回归中，利用一条直线对样本点进行拟合，这个拟合过程就叫做回归
>
>
>
>



- **最小二乘和最大释然：**
> 最大释然估计：从模型中随机抽取n组样本的观测值，最合理的参数估计量应该就是从模型中抽取该n组样本观测值的概率最大
>  最大释然的思想就是什么样的参数才能使我们观察到目前这组数据的概率最大
>
> 最小二乘：得到使得模型能最好地拟合样本数据的参数估计量，就是估计值和观测值之差的平方和最小
>
>
>


- **Sigmoid和softmax总结：**
>       Sigmoid函数：也叫逻辑斯谛函数，logistic
>           逻辑斯蒂函数就是sigmoid函数，几何形状就是一条sigmoid曲线
>           y = 1 / (1 + e^x)
>           hθ(x) = g(θ^T * X)
>                            |x0|
>           z = [θ0,θ1...θn] |x1| = θ^T * X (就是θ向量的转置与X的乘积)
>                            |xn|
>           g(z) = 1 / (1 + e^(-z))
>           最后合并后 hθ(x) = g(θ^T * X) = 1 / (1 + e^(θ^T * X))
>
>       softmax函数：softmax函数
>           y = e^zj / (对e^zk指数求和)
>           当k等于2时，softmax退化为logistic回归，即softmax是logistic回归的一般形式.
>
>           softmax函数，或称归一化指数函数，是逻辑函数(逻辑函数或逻辑曲线是一种常见的S函数，广义的logistic曲线可以模仿人口增长的S形曲线)的一种推广
>           softmax函数能将一个含任意实数的K维向量Z压缩到另一个K维向量中，使得每一个元素的范围都在(0,1]之间，并且所有元素的和为1
>           softmax函数实际上是有限项离散概率分布的梯度对数归一化，在多项式回归和线性判别式分析中，softmax就是计算得到向量X属于j个分类的概率(见https://zh.wikipedia.org/wiki/Softmax%E5%87%BD%E6%95%B0 或者evernote math-softmax函数)
>
>       当对k个类型进行分类时，如果k个类别互斥，那么选择softmax回归分类器，
>       如果某一个样本数据属于k个类别中的多种，选择使用logistic回归算法建立k个独立的二元分类器
>
>

- **相关系数和决定系数：**
>       概述：
>           如果不能对模型的训练和测试的表现进行量化地评估，我们就很难衡量模型的好坏。通常我们会定义一些标准，这些标准可以通过对某些误差或者拟合程度来得到。
>           可以通过计算决定系数R2来量化模型的表现。模型的决定系数是回归分析中十分常见的统计信息，经常被当做衡量模型预测能力好坏的标准。
>           R2的范围为[0,1]，表示目标变量的预测值和实际值之间的相关程度平方的百分比。R2为0表示完全无法预测，为1表示完美的预测。
>           0到1的数值，表示该模型中目标变量中有百分之多少能够用特征来解释。
>           模型也可能出现负值的R2，这种情况下模型所做的预测还不如直接计算目标变量的平均值
>           pakeage：sklearn.metrics import r2_score 
>
>
>
>
>
>
>
>

- **待续：**
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
