### NLP汉语自然语言处理原理与实践-数学知识
- **常用概念：**
>       回归：比如在线性回归中，利用一条直线对样本点进行拟合，这个拟合过程就叫做回归
>
>
>
>



- **最小二乘和最大释然：**
> 最大释然估计：从模型中随机抽取n组样本的观测值，最合理的参数估计量应该就是从模型中抽取该n组样本观测值的概率最大
>  最大释然的思想就是什么样的参数才能使我们观察到目前这组数据的概率最大
>
> 最小二乘：得到使得模型能最好地拟合样本数据的参数估计量，就是估计值和观测值之差的平方和最小
>
>
>


- **Sigmoid和softmax总结：**
>       Sigmoid函数：也叫逻辑斯谛函数，logistic
>           逻辑斯蒂函数就是sigmoid函数，几何形状就是一条sigmoid曲线
>           y = 1 / (1 + e^x)
>           hθ(x) = g(θ^T * X)
>                            |x0|
>           z = [θ0,θ1...θn] |x1| = θ^T * X (就是θ向量的转置与X的乘积)
>                            |xn|
>           g(z) = 1 / (1 + e^(-z))
>           最后合并后 hθ(x) = g(θ^T * X) = 1 / (1 + e^(θ^T * X))
>
>       softmax函数：softmax函数
>           y = e^zj / (对e^zk指数求和)
>           当k等于2时，softmax退化为logistic回归，即softmax是logistic回归的一般形式.
>
>           softmax函数，或称归一化指数函数，是逻辑函数(逻辑函数或逻辑曲线是一种常见的S函数，广义的logistic曲线可以模仿人口增长的S形曲线)的一种推广
>           softmax函数能将一个含任意实数的K维向量Z压缩到另一个K维向量中，使得每一个元素的范围都在(0,1]之间，并且所有元素的和为1
>           softmax函数实际上是有限项离散概率分布的梯度对数归一化，在多项式回归和线性判别式分析中，softmax就是计算得到向量X属于j个分类的概率(见https://zh.wikipedia.org/wiki/Softmax%E5%87%BD%E6%95%B0 或者evernote math-softmax函数)
>
>       当对k个类型进行分类时，如果k个类别互斥，那么选择softmax回归分类器，
>       如果某一个样本数据属于k个类别中的多种，选择使用logistic回归算法建立k个独立的二元分类器
>
>
>
>
>
>
>

- **待续：**
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
