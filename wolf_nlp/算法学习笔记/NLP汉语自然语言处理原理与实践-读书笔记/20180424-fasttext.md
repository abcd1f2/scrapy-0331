### NLP汉语自然语言处理原理与实践-fasttext工具
- **概述：**
>       1、FastText用于高效学习word representation和分类
>           学习word representation：
>               相似的词的表示方式捕获词的抽象属性，方法有Skipgram和CBOW
>               ./fasttext skipgram/cbow -input file.txt -output model
>               训练完成生成两个文件：model.bin：包含模型参数、字典和超参数，可用于计算词向量
>                                   model.vec：每一行包含一个词的词向量的文本文件
>           分类：
>               训练模型的文件默认格式是 __label__<X> <Text>
>               __label__是类的前缀，<X>是分配给文档的类，注：文档中不应有引号，一个文档中所有内容都应该在一行 
>
>       2、ngram使用前提是数据稀疏
>       3、fasttext训练和预测使用的隐藏层和输出层大小一致的
>       4、fasttext的ngram并没有有意保存ngram信息，只是hash并用于检索了
>       5、fasttext的ngram分两种，一种用于分类(supervised，需要预设label)，一种用于词向量(cbow和skipgram)
>
>

-**cbow和skipgram：**
>       skipgram：通过一个词去预测周围的上下文
>
>       cbow(Continuous Bag-Of-Words，即连续的词袋模型)：通过上下文来预测中间的词
>           word2vec输出是神经网络的权重值，用这些值组成词向量
>
>
>
>
>

- **改进和优势：**
>       fasttext将目前用来计算word2vec的网络框架做了小修改，原先使用一个词的山下文的所有词向量之和来预测词本身(CBOW模型)，现在改成用一段短文本的词向量之和来对文本进行分类
>
>

- **训练时的优化：**
>       当需要使用fasttext训练大量数据的时候，可有一些优化的方法，比如：
>       1、预处理数据
>       2、epochs
>       3、use the hierarchical softmax, instead of the regular softmax [Add a quick explanation of the hierarchical softmax].This can be done with the option -loss hs
>

- **参数详解：**
>       -lr 学习率
>       -lrUpdateRate 更改学习率的更新速率
>       -dim 向量大小
>       -loss 损失函数(ns、hs、softmax)
>       -neg 抽样数量
>       -epochs 历元数  迭代数 ?
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
