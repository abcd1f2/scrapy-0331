### NLP汉语自然语言处理原理与实践-fasttext工具
- **概述：**
>       1、FastText用于高效学习word representation和分类
>           学习word representation：
>               相似的词的表示方式捕获词的抽象属性，方法有Skipgram和CBOW
>               ./fasttext skipgram/cbow -input file.txt -output model
>               训练完成生成两个文件：model.bin：包含模型参数、字典和超参数，可用于计算词向量
>                                   model.vec：每一行包含一个词的词向量的文本文件
>           分类：
>               训练模型的文件默认格式是 __label__<X> <Text>
>               __label__是类的前缀，<X>是分配给文档的类，注：文档中不应有引号，一个文档中所有内容都应该在一行
>
>       2、ngram使用前提是数据稀疏
>       3、fasttext训练和预测使用的隐藏层和输出层大小一致的
>       4、fasttext的ngram并没有有意保存ngram信息，只是hash并用于检索了
>       5、fasttext的ngram分两种，一种用于分类(supervised，需要预设label)，一种用于词向量(cbow和skipgram)
>
>

-**cbow和skipgram：**
>       skipgram：通过一个词去预测周围的上下文
>
>       cbow(Continuous Bag-Of-Words，即连续的词袋模型)：通过上下文来预测中间的词
>           word2vec输出是神经网络的权重值，用这些值组成词向量
>
>
>
>
>

- **分层softmax：**
>       在标准的softmax回归中，要计算y=j分类的softmax概率，需要对所有的K(类别的总数)个概率做归一化，在y很大时非常耗时。
>       分层softmax的思想就是使用树的层级结构替代扁平化的标注softmax，使得在计算P(y=j)时，只需要计算一条路径上的所有节点的概率值，无需关注其他的节点，从根节点到叶子节点，实际上做了3词二分类的逻辑回归，复杂度从K降到log(K)
>
>       如在CBOW模型中，输入层由词汇y的上下文单词{x1,x2,...,xn}，xi是onehot编码过的V维向量，V是词汇量，隐含层是N(词向量维度)维向量h，输出层是onehot编码的目标词y
>

- **前向传播：**
>       参考：http://www.52nlp.cn/fasttext
>       假设已经获得了权重矩阵W和W'
>       1、隐含层的输出是C个上下文单词向量的加权平均，权重为W
>       2、计算输出层的每个节点
>       3、通过2步得到的结果作为输入，softmax函数计算最后结果
>

- **反向传播学习权重矩阵：**
>       训练权重矩阵过程中，首先产生随机初始值，然后输入样本到模型中，观测期望输出和真是输出的误差。然后计算误差关于权重矩阵的梯度，并在梯度的方向进行纠正
>       我们对分类的Loss进行改进的时候，我们要通过梯度下降，每次优化一个step大小的梯度，这时候我们就要求Loss对每个权重矩阵的偏导，然后应用链式法则
>       要使用梯度下降，肯定需要一个损失函数，比如使用交叉熵作为损失函数
>
>

- **改进和优势：**
>       fasttext将目前用来计算word2vec的网络框架做了小修改，原先使用一个词的山下文的所有词向量之和来预测词本身(CBOW模型)，现在改成用一段短文本的词向量之和来对文本进行分类
>
>

- **训练时的优化：**
>       当需要使用fasttext训练大量数据的时候，可有一些优化的方法，比如：
>       1、预处理数据
>       2、epochs
>       3、use the hierarchical softmax, instead of the regular softmax [Add a quick explanation of the hierarchical softmax].This can be done with the option -loss hs
>

- **参数详解：**
>       -lr 学习率
>       -lrUpdateRate 更改学习率的更新速率
>       -dim 向量大小
>       -loss 损失函数(ns、hs、softmax)
>       -neg 抽样数量
>       -epochs 历元数  迭代数 ?
>

- **达观应用：**
>       1、同近义词挖掘：
>           基于垂直领域的语料，挖掘同近义词
>       2、文本分类系统：
>           在类标数、数据量都较大时，达观选择fasttext来做文本分类，实现快速训练和预测
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
