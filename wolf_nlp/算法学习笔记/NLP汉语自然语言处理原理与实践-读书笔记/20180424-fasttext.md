### NLP汉语自然语言处理原理与实践-fasttext工具
- **概述：**
>       1、FastText用于高效学习word representation和分类
>           学习word representation：
>               相似的词的表示方式捕获词的抽象属性，方法有Skipgram和CBOW
>               ./fasttext skipgram/cbow -input file.txt -output model
>               训练完成生成两个文件：model.bin：包含模型参数、字典和超参数，可用于计算词向量
>                                   model.vec：每一行包含一个词的词向量的文本文件
>           分类：
>               训练模型的文件默认格式是 __label__<X> <Text>
>               __label__是类的前缀，<X>是分配给文档的类，注：文档中不应有引号，一个文档中所有内容都应该在一行
>
>           优势：Fasttext与gensim词向量区别：
>               词向量word2vec将每个词视为要找到向量表征的最小单位
>               fasttext假定一个词由n个字符组成，就是字符级的ngram，一个词有多个subsrings，这样做的好处是罕见词或者未登录词仍然可以被分解成字符ngram，所以他们可以共享字符级的ngram
>                       当词不在词典中时，word2vec和glove不能对词典中不存在的词提供字的向量，gensim可能会选择两个方案：1、零向量  2、具有低幅度的随机向量
>                       fasttext的做法是，将未登录词或者罕见词分成数据块，并使用这些数据块的向量来创建最终词向量，从而产生出比随机向量更好的向量，字符ngram在更小的数据集上比word2vec更出色
>
>           应用：
>               学习word representation 自然形式的词语一般不能用于任何机器学习的任务，使用这些词的一种方式是将这些词转换为捕获改词的某些属性，word representation用相似的词的表示方法捕获词的抽象属性
>               即使用fasttext找到相似的词，使用这些词表示抽象属性
>           注：如果训练集是中文，会出现乱码问题，将训练集文件编码修改为ansi就可以了
>
>
>       2、ngram使用前提是数据稀疏
>       3、fasttext训练和预测使用的隐藏层和输出层大小一致的
>       4、fasttext的ngram并没有有意保存ngram信息，只是hash并用于检索了
>       5、fasttext的ngram分两种，一种用于分类(supervised，需要预设label)，一种用于词向量(cbow和skipgram)
>
>

- **对fasttext的优化：**
>       在fasttext基础上可以做的优化：
>           1、文本分类模型中，文本表达用类似tf-idf的方式对词进行加权平均
>           2、字典裁剪算法中，一个涵盖所有文本的词不一定有区分能力，比如"the"这种词，可以尝试从保留区分能力的视角来保留字典
>
>
>
>


- **词嵌入：**
>       词表达技术，常见的one-hot，使用高维稀疏向量，不能反映词之间关系，词嵌入技术将词汇的上下文关系嵌入一个低维空间，其中比较有代表性的方法如word2vec、glove。
>
>       word2vec将词的上下文关系嵌入到低维空间，具体而言，word2vec将词的上下文关系转换为分类关系，并以此同时训练词嵌入向量和LR分类器
>           词嵌入将词的上下文关系嵌入到低维空间，word2vec将词的局部上下文转化为了多分类任务，从而训练逻辑回归模型，并将逻辑回归模型中的输入部分作为词嵌入输出
>
>       LR是经典的线性分类模型，二分类的线性分类问题可以表示为输入X和系数W的内积结果W^T*X，结果的正负决定数据的分类。分类器参数通过最小化损失函数l(y,W^T*X)来完成，不同的损失函数定义了不同的分类模型
>       LR也广泛应用在多分类问题中，通过softmax计算数据属于每个类别的概率完成分类，因此分类神经网络的输出层通常也设定为softmax函数，所以多分类LR也可以表示为浅层神经网络
>
>       skipgram：
>           skipgram将词之间的关系变成了|V|多分类问题，|V|是词库的大小。每个词有两个变量X_i和W_i，X_i为词嵌入向量，W_i为线性分类器的系数或者上下文。
>
>
>

- **子词嵌入：**
>       相对于one-hot，词嵌入具有维度低，体现词间关系等特点。但是传统的词嵌入忽略了词的微变形特性，如英语的过去时和进行时等，针对这个问题，提出了子词类学习词的表达，每个词由内部的ngram字母串组成，中文处理也提出类似算法
>       子词嵌入在word2vec基础上，引入子词这个因素，从而使得词的微变形关系也能隐射到嵌入空间中
>
>       学习未知词的表达，比如working，如果在训练数据中没有，但是文本包含了work和ing，那么working的词嵌入向量也能够合理的计算出来
>
>
>
>
>


- **细节：**
>       字符级的ngram：
>           1、在supervised分类时，-minn和-maxn默认为0，即不使用字符级的ngram
>           2、在使用cbow和skipgram，-minn和-maxn默认为3和6
>
>       quantize 子空间量化：
>           1、product quantization 是一种保存数据间距离的压缩技术。PQ 用一个码本来近似数据，与传统的 keams 训练码本不同的是，
>               PQ 将数据空间划分为 k 个子空间，并分别用 kmeans 学习子空间码本。数据的近似和重建均在子空间完成，最终拼接成结果
>           2、在 fasttext 中，子空间码本大小为 256，可以用 1 byte 表示。子空间的数量在 [2, d/2] 间取值。除了用 PQ 对数据进行量化压缩，fasttext 还提供了对分类系数的 PQ 量化选项。
>               PQ 的优化能够在不影响分类其表现的情况下，将分类模型压缩为原大小的1/10
>
>       裁剪字典内容：
>           fasttext 提供了一个诱导式裁剪字典的算法，保证裁剪后的字典内容覆盖了所有的文章。具体而言，fasttext 存有一个保留字典，并在线处理文章，如果新的文章没有被保留字典涵盖，则从该文章中提取一个norm 最大的词和其子串加入字典中。
>               字典裁剪能够有效将模型的数量减少，甚至到原有的1/100
>       模型：
>           fasttext利用 Product Quantization 对字典中的 词嵌入向量进行了压缩，并使用诱导式字典方法，构造涵盖全部文本的字典。两者结合，能够在不明显损害分类算法表现的情况下，将分类模型大小减小数百倍
>

-**cbow和skipgram：**
>       skipgram：通过一个词去预测周围的上下文
>
>       cbow(Continuous Bag-Of-Words，即连续的词袋模型)：通过上下文来预测中间的词
>           word2vec输出是神经网络的权重值，用这些值组成词向量
>
>
>
>
>

- **分层softmax：**
>       在标准的softmax回归中，要计算y=j分类的softmax概率，需要对所有的K(类别的总数)个概率做归一化，在y很大时非常耗时。
>       分层softmax的思想就是使用树的层级结构替代扁平化的标注softmax，使得在计算P(y=j)时，只需要计算一条路径上的所有节点的概率值，无需关注其他的节点，从根节点到叶子节点，实际上做了3词二分类的逻辑回归，复杂度从K降到log(K)
>
>       如在CBOW模型中，输入层由词汇y的上下文单词{x1,x2,...,xn}，xi是onehot编码过的V维向量，V是词汇量，隐含层是N(词向量维度)维向量h，输出层是onehot编码的目标词y
>

- **前向传播：**
>       参考：http://www.52nlp.cn/fasttext
>       假设已经获得了权重矩阵W和W'
>       1、隐含层的输出是C个上下文单词向量的加权平均，权重为W
>       2、计算输出层的每个节点
>       3、通过2步得到的结果作为输入，softmax函数计算最后结果
>

- **反向传播学习权重矩阵：**
>       训练权重矩阵过程中，首先产生随机初始值，然后输入样本到模型中，观测期望输出和真是输出的误差。然后计算误差关于权重矩阵的梯度，并在梯度的方向进行纠正
>       我们对分类的Loss进行改进的时候，我们要通过梯度下降，每次优化一个step大小的梯度，这时候我们就要求Loss对每个权重矩阵的偏导，然后应用链式法则
>       要使用梯度下降，肯定需要一个损失函数，比如使用交叉熵作为损失函数
>
>

- **改进和优势：**
>       fasttext将目前用来计算word2vec的网络框架做了小修改，原先使用一个词的山下文的所有词向量之和来预测词本身(CBOW模型)，现在改成用一段短文本的词向量之和来对文本进行分类
>
>

- **训练时的优化：**
>       当需要使用fasttext训练大量数据的时候，可有一些优化的方法，比如：
>       1、预处理数据
>       2、epochs
>       3、use the hierarchical softmax, instead of the regular softmax [Add a quick explanation of the hierarchical softmax].This can be done with the option -loss hs
>

- **参数详解：**
>       -lr 学习率
>       -lrUpdateRate 更改学习率的更新速率
>       -dim 向量大小
>       -loss 损失函数(ns、hs、softmax)
>       -neg 抽样数量
>       -epochs 历元数  迭代数 ?
>

- **达观应用：**
>       1、同近义词挖掘：
>           基于垂直领域的语料，挖掘同近义词
>       2、文本分类系统：
>           在类标数、数据量都较大时，达观选择fasttext来做文本分类，实现快速训练和预测
>       3、词嵌入的实践：
>           词嵌入的比较有两种方式，a、直接比较：验证词表达保存了人为标注的词间关系
>                                 b、间接比较：通过使用嵌入表达向量进一步学习，比如情感预测，通过模型的表现判断词嵌入质量
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
