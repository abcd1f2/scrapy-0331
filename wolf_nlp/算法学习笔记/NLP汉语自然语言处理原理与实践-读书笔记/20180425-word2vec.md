### NLP汉语自然语言处理原理与实践-word2vec
- **概述：**
>       word2vec解析：http://www.broadview.com.cn/article/282
>       **数据量非常大，使用Skip，数据量不算太大时，使用cbow**
>       如何产生好的词向量：
>           词的表示技术有
>               1、词袋模型(高维、稀疏、词汇鸿沟)
>                   word2vec虽然学习不到反义词这种高层次语义信息，它的思想：具有相同上下文的词语包含相似的语义，使得语义相近的词在映射到欧式空间后具有较高的余弦相似度
>               2、分布表示技术(与独立相对应，基于分布式假说(即上下文相似的词，其语义也相似)，把信息分布式地存储在向量的各个维度中的表示方法，捕捉了句法和语义信息特点)
>                   a、矩阵的分布(LSA、Global Vector模型)
>                   b、聚类的分布表示(聚类的手段构建词与上下文之间的关系，布朗模型)
>                   c、神经网络分布分布表示
>                       NNLM神经网络语言模型
>                       Log双线性语言模型(LBL)
>                       C&W模型
>                       CBOW模型(在输入层直接进行求和)
>                       SkipGram模型
>                       Order模型(把CBOW模型的求和，不考虑词之间的序列顺序，直接求和改成了词向量之间的顺序拼接来保存序列顺序信息)
>
>


- **word2vec训练步骤：**
>       word2vec同其他神经网络一样，大体上有如下步骤：
>           准备好data，即X和Y
>           定义好网络结构
>           定义好loss
>           选择合适的优化器
>           进行迭代训练
>           存储训练好的网络
>
>       SkipGram模型：
>           概述：
>               假如两个词具有相同的输出，则可反推出作为输入的两个词之间具有较高的相似性
>               重点在于体会使用n-gram构造训练数据，设计神经网络背后隐含的思想
>               比如语料库中有10000个单词，隐藏层为300，则权重系数为10000 * 300 * 2
>               网络的输入是one-hot编码的单词，它与隐藏层权重矩阵相乘实际上是取权重矩阵特定的行，即隐藏层实际上相当于一个查找表，它的输出就是输入的单词的词向量
>
>           网络结构：
>               隐藏层就是词的维度(Google推荐300，具体多少合适需要根据验证结果)
>               输出层就是语料库中词的个数
>
>           输出层：
>               通过对所有的输出层神经元的输出进行softmax操作，我们就把输出层的输出规整为一个概率分布
>               输出层输出的是该单词出现在输入单词周围的概率大小，包含单词的前面和单词的后面
>
>           定义损失函数loss：
>               可以使用交叉熵来衡量神经网络的输出与我们的label y的差异大小，就可以定义出loss了
>
>           优化：
>               由于更新权重系数在语料库非常大的时候，计算量非常大，所以训练的时候可以使用相应的技巧
>               1、将词组和短语看做独立的单词，
>                   如"中华人民共和国"，看做一个独立的词会更合理，计算量也会减小
>               2、欠采样，对高频词进行抽样，subsamping：
>                   在扫描文本的词时，会根据一定的概率删除这个词，概率的大小取决于词在语料库中的出现概率，出现概率越高，删除此的概率越大，概率公式：p(word) = (sqrt(x/0.001)+1)*0.001/x，其中x为词频
>               3、负抽样，负采样，negative sampling：
>                   概述：针对训练样本(ants,able)，able这个词是正样本，词表中其余的词都是负样本。负采样是对负样本进行采样，从负样本中挑选几个进行拟合，节约计算资源，Google建议5-20个
>                        对于10000个词表，300维向量中，则有300万参数（如果是100万词表，则需要更新3亿的参数），训练中需要对每个参数计算偏导，然后进行更新，
>                   (蚂蚁金服张家兴老师分享的一个问题匹配模型训练中的一个技巧，思想与负抽样相似)
>                   负抽样解决的问题就是模型难以训练
>                   超多的训练数据，巨大的权重系数更新，使得训练非常缓慢
>                   负抽样的思想就是：
>                       比如skip-gram，中心词对上下文的词类是正面例子，对所有其它的词为负面例子
>                       每一个样本的每一次训练，只更新很小一部分的权重，而不是更新全部，具体负抽样时抽取几个维度的神经元，取决于具体问题，Google建议5~20个，
>                       比如我们的词表中有10000个单词，词向量为300，即隐藏层有300个神经元，那么输出层权重矩阵大小为300*10000，如果我们抽取5个负的维度(输出为0的维度)，加上输出为1的维度，只更新这6个维度所对应的神经元，那么需要更新的权重系数为300 * 6=1800个，这只占输出层所有权重系数的0.06%
>                       负抽样应该怎么抽取？负抽样是按照单词在语料库中出现的次数多少来的，出现次数越多，越可能被抽中，概率公式略
>               4、层次softmax：
>                  模型输出不再使用one-hot加softmax，而是使用Huffuman树加softmax回归，加快的原因是输出层不适用one-hot表示，softmax回归就不需要对那么多0(负样本)进行拟合，仅仅只需要拟合（多个logistic二分类）输出值在Huffuman树中的一条路径。
>
>           达观应用案例：
>               参考：https://mp.weixin.qq.com/s?__biz=MzA5NzY0MDg1NA==&mid=2706953858&idx=1&sn=cb83cb61330cdb77275e846bec6e7433&scene=21#wechat_redirect
>               1、特征降维：
>                   特征维度过高的时候，容易出现特征之间具有较高的相关性，这种情况下可以利用词向量工具对特征进行聚类，将相关特征聚类到一个维度里面
>               2、特征扩展：
>                   针对短文本处理时，一个case往往提不出表意很强的特征，导致类别间区分度不强。这种情况下可以利用词向量对主要特征进行扩展，在不损失精度的前提下提高召回率
>
>

- **word2vec：**
>       构建方法：
>           1、数数的统计构成矩阵（one-hot模型，统计了词频）
>           2、概率构成矩阵
>
>
>       gensim构建词向量步骤：
>           1、model= Word2Vec()  建立一个空的模型对象
>           2、model.build_vocab(sentences)  遍历一次语料库建立词典
>           3、model.train(sentences，total_examples = model.corpus_count，epochs = model.iter)  第二次遍历语料库建立神经网络模型
>           4、print(model['man'])  获取词向量
>           5、model.most_similar(['男人']) 计算一个词的最近似的词，倒排序
>           6、model.most_similar(positive = ['woman','king'],negative = ['man'],topn = 2)  支持词语的加减运算
>           7、model.similarity('女人', '男人')  计算两词之间的相似度
>           8、list_sim1 = model.n_similarity（list1，list2） 计算两个集合之间的余弦似度
>           9、model.doesnt_match('breakfast cereal dinner lunch'.split()) 选出集合中不同类的词语
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
