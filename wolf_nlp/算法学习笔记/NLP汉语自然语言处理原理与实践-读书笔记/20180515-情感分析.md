### NLP汉语自然语言处理原理与实践-情感分析
- **概述：**
>       1、确定一个词是积极还是消极，是主观还是客观，主要依赖词典
>           英文中有资源较全的词典资源，SentiWordNet，含有积极消极、主观客观和情感强度值
>           中文领域有一些资源，但是做的不是很完善，质量不高、不细致，缺乏主客观词典
>       2、识别一个句子是积极还是消极，主观还是客观，
>           词典方法：当有情感词典时，直接匹配进行计算
>           机器学习方法：将分好的积极消极数据使用算法进行训练，训练得到的分类器就可以进行分类是积极还是消极
>                       但是判断主客观比较麻烦，一般需要人工标注
>       3、情感挖掘升级到意见挖掘
>           这一步需要从评论中找出产品的属性，比如手机的屏幕、电池等都是它的属性，然后需要分析评论是如何评价这些属性的。
>
>       情感分析的主要方法：
>           1、基于词典的
>               短文本中，基于词典的情感分析效果更好
>               通过对文本进行段落拆分、句法分析、计算情感值
>               由于不同词汇组合起来能达到不同的情感程度或者相反的情感极性，所以以句子为最基本的情感分析粒度较为合理
>           2、基于机器学习的
>               适合长文本的情感分析
>               类似于分类问题，进行有监督学习
>
>

- **实践：**
>       使用贝叶斯训练数据，进行分类
>       可能会出现共线性问题，自变量之间出现共线性，说明自变量所提供的信息是重叠的，处理的方法：
>       1、删除不重要的自变量减少重复信息，但从模型中删除自变量时应该注意为相对不重要并从偏相关系数检验证实为共线性原因的那些变量中删除。
>           如果删除不当，会产生模型设定误差，造成参数估计严重有偏的后果。
>       2、多重共线性问题的实质是样本信息的不充分而导致模型参数的不能精确估计，因此追加样本信息是解决问题的一条有效途径。但是实际工程中，这个方法效率不高（样本数据不好获取）。
>       3、利用非样本先验信息
>       4、改变解释变量的形式
>       5、逐步回归法（最常用也最有效）
>           逐步回归是一种常用的消除多重共线性、选取最优回归方程的方法
>           当数据的特征数高于样本数，或者特征之间高度相关时，会导致X^TX奇异，从而限制了LR和LWLR的应用。这时需要考虑使用缩减法。
>           缩减法：
>               可以理解为对回归系数的大小施加约束后的LR，也可以看作是对一个模型增加偏差（模型预测值与数据之间的差异）的同时减小方差（模型之间的差异）。
>               一种是岭回归（L2），一种是lasso法（L1），由于计算复杂，一般用效果差不多但是实现容易的前向逐步回归法
>           前向逐步回归：
>               前向逐步回归算法是一种贪心算法，每一步都尽可能减小误差，一开始，所有权重都设置为1，然后每一步所做的决策是对某个权重增加或减少一个很小的值
>           主要基于假设：
>               在线性条件下，哪些变量组合能够解释更多的因变量变异，则将其保留
>               首先模型中只有一个单独解释因变量变异最大的自变量，之后尝试将加入另一自变量，看加入后整个模型所能解释的因变量变异是否显著增加；这一过程反复迭代，直到没有自变量再符合加入模型的条件。
>
>       6、主成分回归
>           主成分分析作为多元统计分析的一种常用方法在处理多变量问题时具有一定的优越性，对于一般的多重共线性问题适用，尤其是对共线性较强的变量之间。当采取主成分提取了新的变量后，
>           往往这些变量的组内差异小而组间差距大，起到了消除共线性问题
>
>

- **总结：**
>       （从Twitter的一次情感分析中得出的一些总结）
>       朴素贝叶斯：是最简单且速度最快的分类器，许多研究表明这个分类器取得了最好的效果，使用高阶的n元词组提高准确率
>               一元词串+二元词串+三元词串为特征并使用朴素贝叶斯分类器进行训练，准确率最高
>       最大熵模型：基于一元、二元和三元词的分类器准确率最高
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>

- **待续：**
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
