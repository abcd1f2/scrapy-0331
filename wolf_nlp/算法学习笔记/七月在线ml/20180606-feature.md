### 七月在线ML -- feature
- **概述：**：
>       数据和特征决定模型的上限
>       更好的特征作用：
>           更强的灵活性
>           更好的效果
>           更简单的算法
>
>       数据特征的重要性：
>           具体业务结合
>
>       阿里天池：
>           各种交叉特征做到上万维的特征
>

- **数据与特征处理：**
>       数据清洗：
>           garbage in,garbage out
>           算法大多数时候就是一个加工机器，最后的产品取决于原材料的好坏
>           去掉脏数据
>       数据采用：
>           正负样本的不均衡问题
>               LR对正负样本比重比较敏感
>               解决方法：
>                   正样本 >> 负样本  数据量大 使用下采样方法（如原有5份数据，现在采集1份数据做处理）
>                   正样本 >> 负样本  数据量小
>                       oversampling（自己构造数据）
>                       采集更多数据
>                       修改损失函数（也叫代价敏感学习），如负样本偏少，则可在计算负样本的损失函数时，添加计算权重
>           数据采样
>           分层抽样
>               在每一类中抽取原来比例的数据
>
>       特征处理：
>           1、数值型（连续值）
>               幅度调整/归一化，LR等对数据敏感，比如房子面积，可以使用当前面积除以range(max-min)
>               log等变化，指数数据的转换
>               统计值max、min、mean、std
>               离散化(pandas库有相关处理)
>                   1、等步长（工程中使用）
>                   2、等频（样本量切分 工程中使用）
>               hash分桶
>               每个类别下对应的变量统计值histogram（分布状况 直方图映射）
>                   通过爱好比重分布 映射为样本的向量
>               数值型 =》 类别型
>                   one-hot编码或哑变量
>                       （比如衣服颜色红、黄、蓝三种颜色，不能直接编码成1、2、3，因为这样就认为的加上了一个大小关系的数据，需要平等的编码三个类别，如[1,0,0]、[0,1,0]、[0,0,1]）
>                       独热向量编码后，会增加特征的维度，变成一个稀疏的特征向量
>                       造成维度灾难（比如电商的用户id，如果直接用独热向量编码，维度直接爆掉，解决方案：比如有1000w用户，先聚类成1000类，然后对每一个用户，先查询具体在哪一类，所以最后仅对1000类进行独热向量编码）
>                   Hash与聚类处理
>                       比如先设置n个桶，每个桶代表一个类型，计算每个样本属于每个类型的概率或者频度等，这样就将特征规整为了一个n维度的特征向量
>                   统计每个类别变量下的各个target的比例，转成数值型
>           2、类别型
>           3、时间型
>               既可以看做连续值，也可以看做离散值
>               连续值：浏览网页时长等
>               离散值：一天中的时间点、星期几、工作日、周末等
>           4、文本型
>               ont-hot
>               tf-idf（反应词的重要性）
>               word2vec
>           5、统计型
>               加减平均
>               分位线
>               次序
>               比例型
>           6、组合特征型
>               多个特征组合成新的特征
>
>       特征的选择：
>           原因：
>               部分特征的相关度太高，消耗计算性能
>               部分特征对预测结果由负影响
>           特征选择 和 降维：
>               特征选择：剔除和预测结果关系不大的特征
>                   1、过滤型：
>                      评估单个特征和结果值之间的相关程度，留下top相关的特征
>                       缺点：没有考虑到特征之间的关联作用，可能把有用的关联特征剔除
>                   2、包裹型：
>                       把特征选择看做一个特征子集搜索问题，筛选各种特征子集，用模型评估效果
>                       RFE算法（递归的特征删除）
>                   3、嵌入型
>                       最常用的方式是用正则化来做特征选择
>                       L1和L2正则的区别：
>                           L1是截断型
>                               可能把某些不是很重要的特征的权重置为0（将特征不相关的权重置为0，减小了计算量）
>                           L2是缩放型
>                               缩放权重系数
>               降维：线性变换等
>
>
>
>
>
>
>
>
>

- **待续：**
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
