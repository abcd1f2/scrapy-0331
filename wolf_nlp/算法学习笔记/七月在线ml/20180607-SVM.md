### 七月在线机器学习 -- SVM
- **概述：**
>       NIPS机器学习的顶级会议
>       SVM：时间复杂度O(N^3)
>       适用中小数据集
>       在大数据情况下，跑不完数据
>
>       kernel mathod：
>           线性方法 -> 核方法
>           拉格朗日乘子法：
>               求最小值： f1 = 1/2|W|^2
>               约束条件： f2 = yi(W*X + b) - 1，当f2=0的约束
>               W：支持向量的线性组合
>               将一个带条件的函数极值算法变换成一个不带条件的函数极值算法
>               L = f1 - ∑αi * f2
>                 = ∑αi - 1/2∑∑αi * αj * yi * yj * xi * xj
>               总结：L取决与xi * xj
>               核方法作用：
>                   kernel(xi,xj) = φ(xi) * φ(xj)
>                   比如有一个核函数为f(x1,x2) = (x1*x2+1)^n
>                   则L的最后优化中的xi * xj则变成优化(x1*x2+1)^n
>                   对于L的优化中，并不需要知道如何变换，只需要知道变换后，两个x变量的乘积
>                   是优化目标和决策函数具有非线性变换的能力
>                   核方法作用是计算出经过变换后的空间的xi和xj的乘积
>               优化：
>                   优化的目标就是计算最优的αi参数
>                   SMO算法：
>                       固定(αi,αj)，迭代计算最后收敛
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>

- **待续：**
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
