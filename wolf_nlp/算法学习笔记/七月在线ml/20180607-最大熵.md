### 七月在线机器学习 -- 最大熵模型
- **概述：**
>       熵：不确定性的描述
>       条件熵(joint entropy) H(P(y|x))
>           H(X|Y) = H(X,Y) - H(Y)
>       联合熵（condition entropy） H(x,y)
>       互信息（mutual infomation）
>           I(X:Y) = H(X) - H(X|Y) = H(X) + H(Y) - H(X,Y)
>       相对熵（KL divergence）：
>           学到的分布好坏的指标
>           衡量两个概率分布的区别大小
>       最大熵原理：
>           在学习概率模型的时候，在所有可能的概率模型中，熵最大的模型是最好的模型
>           两个约束条件：
>               1、学习概率和经验概率相同
>               2、在满足1条件下的熵最大
>
>       LR、max entropy和softmax区别：
>           最后的形式都类似于softmax
>
>

- **EM：**
>       EM：expectation maxmization 期望最大化
>       混合模型（mixture models）：
>           mixture of Gaussians：
>               
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>

- **待续：**
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
