### 哈工大NLP -- ngram语言模型
- **概述：**：
>       噪声信道模型：
>           普适性模型
>           无法避免的噪声信道模型
>           贝叶斯公式计算
>
>       模型：完成某种特定功能，实现某个目的的算法和数据结构的总称
>
>       香农游戏：
>           根据前面的n个字母或者单词预测下一个字母或者单词
>           后面出现的词依赖于前面出现的词，距离越近依赖性越强
>
>
>       ngram语言模型：
>           统计语言模型面临的问题：
>               参数空间过大
>               数据稀疏（zipf）
>
>           运用马尔科夫假设
>
>           最大相似度估计
>               条件概率估计值
>               p=c(w1w2...wn)/c(w1w2...wn-1)
>
>           数据平滑技术：
>               降低以出现的ngram条件概率分布，以使未出现的ngram条件概率分布非零
>               拉普拉斯方法：
>                   （最简答的，加1平滑法）
>                   副作用：加1后导致未出现的概率太大，所有加的数可以为小于1
>               good-turing估计（good和图灵）：（实际工程中使用）
>                   1、对于语料库中真实出现的ngram：
>                       P=r/N，r=(r+1)N(r+1)/N(r)，N(r)表示出现r次的ngram个数
>                   2、对于语料库中未出现的：
>                       p=N/(N(0)*N)
>                   需要构建一个频度对应ngram个数
>               其他平滑技术：
>                   back-off平滑
>                   线性插值平滑
>                   witten-Bell平滑
>               数据平滑效果：
>                   构造高鲁棒性语言模型的重要手段
>                   语料库的规模越小，数据平滑技术的效果越显著
>
>           动态自适应基于缓存的语言模型：
>               少数词汇在局部文本中大量出现，模型做出自适应的调节，能够描述这种情况
>               方法或者思想：将最近出现的词加入到缓存中，作为独立的训练数据，估算独立数据的条件概率值，称为动态估计
>                           动态估计与静态估计结果，采用线性插值等一些方法，作为最后的条件概率
>
>           评价方法：
>               实际使用：
>                   （没有针对性）
>                   不能确定具体是哪一个步骤的问题
>               基于交叉熵与迷惑度的方法：
>                   熵的应用
>                   句子熵的计算
>                   迷惑度的计算
>               KL(Kullback-Leibler)距离：
>                   KL距离是衡量两个概率分布之间差异的量度
>                   相关熵
>                   非量度：不满足交互律和三角不等式
>                   如计算两个词的语义相关性：
>                       1、基于语义学词典，计算语义距离
>                       2、基于统计的方法
>                           a、类似word2vec方法
>                           b、相关熵的量度进行计算
>                               先计算上下文的概率分布，然后根据相关熵计算上下文的概率分布的差异
>                               差异越小，语义相关度越高
>                   相关熵评价语言模型：
>                       计算实际模型的统计概率和理想模型的统计概率的差异，差异越小，模型越好
>                       语言与其模型的交叉熵
>

- **实际应用：**
>       音字转换系统（网格计算）：
>           1、使用贝叶斯转换求最大的子串概率
>           2、使用dp算法
>
>       语言模型构造实例：
>           1、ngram语言模型压缩数据结构设计（类似于leveldb中数据存储设计），扩展性好
>           bigram设计：
>                   版权信息、head数据信息、body区数据信息
>           头结构   head数据中每一条的具体数据，包含body偏移量
>           体结构   body结构中每一条具体数据信息（词汇+频度）
>
>           trigram设计：
>                   版权信息、head数据信息、body区数据信息
>           头结构   head数据中每一条的具体数据，包含body偏移量
>           体结构   body结构中每一条具体数据信息 包含body偏移量 （词汇+偏移量）
>           体结构   body结构体中每一条具体数据信息 （词汇+频度）
>

- **最大熵模型：**
>       条件判别模型的代表
>       将语言学规则建立在坚实的数学基础上的模型
>       最大熵模型是基于最大熵原理的统计预测模型
>       最大熵原理：
>           在一定的限制条件下，尽可能地选择熵最大的概率分布作为预测结果，而对不知道的情形（限制条件外），不做任何假设
>       自然语言处理中应用：
>           词性标注：
>               比如一个词有多个词性，则总的概率为1，则可以使用最大熵原理计算每种词性的概率分布
>               应用最大熵原理，在一定限制条件下给出的概率估计
>               在最大熵模型中，特征是一个关于事件的二值函数。即X->{0,1}，出现即为1，X=A * B，A为预测值的上下文，B为待预测的值，即词性
>           文档级特征：
>               比如在篮球的文章中，火箭则应被标记成队名，而不是武器
>           原子级（词）特征：
>               一个词的词性标注和上下文具有相关性
>
>       最大熵模型在自然语言处理中的限制条件：
>           模型特征的期望值等于训练语料库中观察到的特征的期望值。
>           比如：训练语料中某种特征出现的期望值为3次，利用最大熵构造的模型，计算出来事件期望出现的次数为3次
>           由特征模板，得到训练库中的特征，然后统计特征的频度，训练出来的权重W，最后根据权重计算出来的联合概率，最后计算出来的频度就是训练库中的特征频度
>           牛顿法进行优化、拉格朗日极值定理
>
>       最大熵模型的使用方法：
>           1、局部概率
>               之前工程中最大熵进行命名实体识别，利用局部概率值直接标注
>           2、viterbi、binsearch（边裁剪空间边搜索，组合优化算法中比较常用的策略）等寻找全局最优
>
>       应用：
>           词性标注
>           情感极性分析
>           浅层句法分析
>
>       最大熵模型在文本挖掘中对数据缺失的敏感问题
>       最大熵模型是CRF的一个简化形式
>
>
>


- **待续：**
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
