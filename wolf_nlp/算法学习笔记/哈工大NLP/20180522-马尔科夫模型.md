### 哈工大NLP -- 马尔科夫模型
- **概述：**：
>       生成式模型
>       源于语音识别，特别是隐马的出现，使语音识别从孤立、小词表、非连续语音进入到大词表、连续语音
>       ngram:
>           马尔科夫假设
>           链式
>
>       马尔科夫假设：
>           有限历史假设
>           时间不变假设
>
>       马尔科夫特征：
>           X=(x1,x2...xn)随机变量序列，其中每个随机变量的取值在有限集S={s1,s2...sn}，称为状态空间
>           如果这个序列符合马尔科夫假设，那么这个过程称为马尔科夫过程
>
>       马尔科夫形式化表示：
>           抽象的核心部分
>           一个马尔科夫模型是一个三元组（S,π,A）其中
>               S是状态集合
>               π是初始状态概率
>               A是状态间的转移概率
>

- **隐马尔科夫模型：**
>       隐马尔科夫模型：
>           相对于马尔科夫模型，多了隐状态到显示状态的概率分布（发射概率）
>           状态转移序列隐藏了起来
>           HMM是一个五元组(S,K,π,A,B)
>
>           三个基本问题：（应用背景都来自于语音识别）
>               1、评价
>                   给定模型，如何高效计算某一输出字符序列的概率
>                   方法：dp算法
>                   前向算法（求和得到）、后向算法
>               2、解码
>                   给定模型和输出序列，计算最大概率的状态序列
>                   viterbi
>               3、学习：
>                   给定一个输出字符的序列，如何调整参数使得这个序列的概率最大
>                   Baum-Welch算法：（保姆.维奇算法）
>                       （参数更新算法，也用到了前向变量和后向变量，所以也被称为向前向后算法）
>                       （类似于梯度算法的思想）
>                       !! 能保证每次更新的新的参数比原来参数得到的计算序列的概率大，具有数学理论基础
>                       缺点：
>                           该算法只能得出局部最优解，所以仍然是目前制约语音识别精度的提高
>                           参数训练算法的重要性
>                   EM最大期望值算法的一种特殊形式，是一种无指导的机器学习算法，较有指导的机器学习算法差
>           网格：
>               词网格分词
>               音字转换的网格
>
>       词性标注：
>           能达到90以上准确率
>           词性标注：
>               找到一个字串最大的词性标注概率
>               作用：句法分析的前期步骤
>               难点：兼类词
>
>           最大相似度估计
>
>       音字转换：（90以上的准确）
>           发射字符集：拼音
>           状态集：词汇集合
>
>       基于HMM的词网格分词：
>           状态集：词汇
>           发射集：空集
>
>

- **应用：**
>       ngram可以描述连续的约束关系，但是对于远距离、递归的约束关系，则不能处理
>       语言间的长距离的约束关系，远距离的搭配关系（如一只小猫）
>       递归修饰的约束关系（一支美丽又可爱又芳香的小花）
>       规则（上下文无关文法规则）与统计相结合处理解决长距离约束、递归修饰中心词的问题
>       规则的颗粒度的权衡（颗粒度需要较小）
>       规则由于效率问题，所以规则的作用有限
>       词网格 --> 元素网格（元素首词之间的转移概率）
>
>
>
>
>
>
>

- **待续：**
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
