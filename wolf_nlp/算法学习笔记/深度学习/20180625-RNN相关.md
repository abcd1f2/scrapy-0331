### 深度学习 - 神经网络基础1
- **概述**：
>       隐层比较多（>2）的神经网络叫深度神经网络
>       深度网络能够表达力更强
>       一个仅有一个隐藏层的神经网络能拟合任何一个函数，但是需要很多神经元，但是深层神经网络能拟合同样的函数，也是为了拟合一个函数，
>       所以要么使用一个浅而宽的网络，要么使用一个深而窄的网络。
>       深层网络的特点：
>           1、相比浅层网络往往更节约资源
>           2、不太容易训练，需要大量的数据和很多技巧才能训练好一个深层网络
>
>       神经网络组成部分：
>           1、输入权值W
>           2、偏置项b
>           3、激活函数
>           4、输出
>               y = f(W*X + b)
>

- **感知器：**
>       感知器的训练：
>           将权重W和偏置b初始化为0，然后，利用下面的感知器规则迭代修改W和b，直到训练完成
>               Wi <-- Wi + ▽Wi
>               b  <-- b + ▽b
>           其中：
>               ▽Wi = η(t - y)xi
>               ▽b = η(t - y)
>           Wi为与输入xi对应的权重项，b是偏置项。可以把b看作是值永远为1的输入xb对应的权重。
>           t是训练样本的实际值，一般称之为label
>           y是感知器的输出值，
>           η是学习速率的常数，作用是控制每一步调整权的幅度
>

- **激活函数：**
>       激活函数的性质：
>           1、非线性：
>               当激活函数是非线性的时候，一个两层的神经网络就可以逼近基本上所有的函数了。但是，如果激活函数是恒等激活函数的时候(即f(x)=x)，就不满足这个性质，
>               而且如果MLP使用的是恒等激活函数，那么其实整个网络跟单层神经网络是等价的
>           2、可微性：
>               当优化方法是基于梯度的时候，这个性质是必须的
>           3、单调性：
>               当激活函数是单调的时候，单层网络能够保证是凸函数
>           4、f(x) ≈ x：
>               当激活函数满足这个性质的时候，如果参数的初始化是random的很小的值，那么神经网络的训练将会很高效，所以初始值的设置很重要
>           5、输出值的范围：
>               当激活函数输出值是有限的时候，基于梯度的优化方法会更加稳定，因为特征的表示受有限权值的影响更显著；
>               当激活函数的输出是无限的时候，模型的训练会更加高效，不过在这种情况下，一般需要更小的learning rate
>
>       常用的激活函数：
>           **1、sigmoid：**
>               sigmoid曾经使用的很多，近年来，使用的越来越少，
>               缺点：
>                   a、sigmoid有一个致命的缺点，当输入非常大或者非常小的时候，这些神经元的梯度接近于0。所以，需要注意参数的初始值来尽量避免这种情况
>                       如果初始值设置的很大的话，大部分神经元可能都会处在saturation的状态而把gradient kill掉，这会导致网络很难学习
>                   b、sigmoid的输出不是0均值，这是不可取的，因为这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入
>                       导致的结果：如果数据进入神经元的时候是正的，那么w计算出的梯度也会始终是正的
>                                  如果是按batch去训练，那么那个batch可能得到不同的信号，所以这个可以可以避免一下。因此非0均值这个问题虽然会产生一些不好的影响，不过跟前面的gradients kill问题相比还是要好很多的。
>           **2、tanh：**
>               tanh跟sigmoid很像，实际上，tanh是sigmoid的变形
>               特点：
>                   a、tanh是0均值的，因此在实际应用中，tanh比sigmoid更好
>           **3、ReLU：**
>               Relu变的越来越受欢迎
>               f(x) = max(0,x)
>               **优点：**
>                   ReLU得到的SGD的收敛速度会比sigmoid/tanh快很多（有人解释为它是linear，而且是non-saturating）
>                   相比于sigmoid/tanh，ReLU只需要一个阈值可以得到激活值，而不用去算一大堆复杂的运算
>               **缺点：**
>                   训练的时候很脆弱，很容易就die了
>                   比如一个非常大的梯度流过一个ReLU神经元，更新过参数之后，这个神经元再也不会对任何数据有激活现象了，如果这个情况发生了，那么这个神经元的梯度就永远都会是0
>                   实际操作中，如果你的learning rate很大，那么很有可能你网络中40%的神经元都dead了，
>                              如果设置了一个合适的较小的learning rate，这个问题发生的情况不会太频繁
>               ReLU的变种：
>                   1、Leaky ReLUs：就是用来解决dying ReLU问题，**但是效果，现在没有明确的结果，不确定会肯定变好**
>                               ax  (x<0)
>                       f(x) =  x   (x>=0)
>                       a是一个很小的常数，这样，即修正了数据分布，又保留了负轴的值，使得负轴信息不会全部丢失
>                   2、Parametric ReLU：对于Leaky ReLU中的a，通常都是通过先验知识人工赋值的，Parametric ReLU的方法是将a作为参数进行训练（损失函数对a的导数可以求出）,论文中指出，使用了ReLU后，最终效果比不用提高了1.03%
>                   3、Randomized ReLU：是Leaky ReLU的random版本（a是random的）
>                       核心思想：训练过程中，a是从一个高斯分布U(l,u)中随机出来的，然后在测试过程中进行修正
>                   4、Maxout：Maxout的拟合能力非常强的，可以拟合任意的凸函数。
>                       作者从数学上证明了，只需要2个Maxout节点就可以拟合任意的凸函数了，前提是隐隐含层节点的个数可以任意多
>                       优点：
>                           Maxout具有ReLU的优点（计算简单，不会saturation），同时又没有ReLU的缺点（容易go die）。
>                       缺点：
>                           参数double了，参数变多了
>
>       选择激活函数：
>           1、如果使用ReLU，一定要小心设置learning rate，注意不要让你的网络出现很多dead神经元，如果这个问题不好处理，可以尝试ReLU的其他变种
>           2、最好不要用sigmoid，可以试试tanh，不过可以预期它的效果可能比不上ReLU或Maxout
>           注：通常很少会把各种激活函数串起来在一个网络中使用
>

- **待续：**
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
