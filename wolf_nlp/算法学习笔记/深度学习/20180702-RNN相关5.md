### 深度学习 - 神经网络基础-LSTM
- **概述：**
>       由于梯度爆炸或消失的原因导致RNN很难训练，导致了它在实际应用中，很难处理长距离的依赖
>       LSTM：
>           1、改进的RNN，成功解决了原始RNN的缺陷，
>           2、但是LSTM结构复杂
>       GRU：
>           1、结构比LSTM好
>           2、效果和LSTM一样好
>
>       递归神经网络：
>           有时候仅仅拥有处理序列的能力还不够，还需要处理比序列更为复杂的结构（树结构）-递归神经网络（RNN）
>

- **LSTM：**
>       在RNN中，误差项δ从t时刻传递到k时刻，当t-k很大时，前面提到的优化技巧效果都不够好
>       从前面公司中，权重数组W最终的梯度是各个时刻梯度之和，当从某时刻开始，得到的梯度几乎为0时，后面的梯度就不会对最终的梯度有比较大的贡献，这就是原始RNN无法处理长距离依赖的原因
>       LSTM：
>           原始RNN的隐层只有一个状态s，对于短期输入非常敏感
>           LSTM增加了一个状态c，来保存长期的状态
>           **三个输入两个输出：**
>               X：当前时刻的输入
>               ht-1：上一时刻的输出
>               ct-1：上一时刻的输出
>               ht：当前时刻的输出值
>               ct：当前时刻的单元状态
>
>           **三个开关：**
>               开关1（遗忘门）：负责控制继续有多少保存上一时刻的长期状态c到当前状态
>               开关2（输入门）：负责把当前网络的输入有多少保存到长期状态c
>               开关3（输出门）：负责把长期状态c有多少作为当前的输出
>
>               开关：
>                   开关在算法中通过门来实现，门实际上就是一层全连接层
>
>               输出公式：
>                   ot = δ(W*(ht-1, xt) + b)
>                   ht = ot · tanh(ct)
>
>       训练：
>           训练算法框架：
>               LSTM的训练算法仍然是BP算法
>               **训练算法步骤：**
>                   1、前向计算每个神经元的输出值，
>                   2、反向计算每个神经元的误差项δ值，与RNN一样，反向传播包括两个方向：
>                       一个是时间的反向传播，即从当前时刻开始，计算每个时刻的误差项
>                       一个是将误差项向上一层传播
>                   3、根据相应的误差，计算每个权重的梯度
>           LSTM需要学习的参数共8组：
>               遗忘门的权重矩阵Wf和偏置bf
>               输入门的Wi和bi
>               输出门的Wo和bo
>               计算单元状态的Wc和bc
>
>       ![avatar](http://baidu.com/xxx.png)
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>

- **GRU：**
>       概述：
>           GRU：LSTM的一种变体，GRU也许是最成功的一种
>       和LSTM的区别：
>           1、将三个门变成两个门
>               将输入门、遗忘门、输出门变为：更新门和重置门
>           2、将单元状态和输出合并为一个状态：h
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>

- **待续：**
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
