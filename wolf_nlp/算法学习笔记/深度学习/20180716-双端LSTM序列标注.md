### 深度学习 - 双端LSTM序列标注
- **概述**：
>       Bi-directional LSTM
>       参考：https://blog.csdn.net/jerr__y/article/details/70471066
>
>       字标注方法：
>           1、将分词等问题直接变成了一个序列标注问题，而且标注是对齐的
>           2、标注法实际上已经是一个总结语义规律的过程，“李”字是常用的姓氏，一半作为多字词（人名）的首字，即标记为b；而“想”由于“理想”之类的词语，
>               也有比较高的比例标记为e，这样一来，要是“李想”两字放在一起时，即便原来词表没有“李想”一词，我们也能正确输出be，也就是识别出“李想”为一个词
>           3、标注的数目，常用4tag
>               还有6tag和2tag，为什么选择4tag效果更好？
>               因为相比2tag，更多的tag实际上更全面概括了语义规律，相对于6tag，还要总结出第二字、第三字，但是这个角度总结不够合理（不过从新词发现的角度看，6tag更容易发现长词）
>
>       双向LSTM：
>           不管是LSTM或RNN，都是从左往右推进的，因此后面的词会比前面的词更重要，但是对于分词任务来说这是不妥的，
>           因为句子各个字应该是平权的？？？因此出现了双向LSTM，从左往右做一次LSTM，然后从右往左做一次LSTM，然后把两次结果组合起来
>
>       深度学习在分词中的应用:
>           不管是传统的神经网络还是LSTM，它们的做法跟传统的模型都是一样的，都是通过上下文来预测当前字的标签，上下文就是固定的窗口，比如用前后5个字加上当前字来预测当前字的标签。
>           这种做法仅仅是把以往估计概率的方法，如HMM、CRF等，换成了神经网络，整个框架没有变，本质上还是N-gram模型。
>
>
>

- **BiLSTM-CRF模型做基于字的中文命名实体识别**
>       传统的机器学习算法需要手工定义特征模型在训练CRF模型
>
>       学术界比较流行的BiLSTM-CRF模型
>           从宏观上讲，就是用crf是为了获取全局最优的输出序列，相当于对LSTM信息的再利用
>           引用知乎的回答：从网络结构上讲，BiLSTM-crf套用的还是crf这个大框架，不过把LSTM在每个t时刻在第i个tag上的输出，看做是crf特征函数里的"点函数"(只与当前位置有关的特征函数)，
>           然后"边函数"(与前后位置有关的函数)还是crf自带的，这样就将线性链crf里原始的w*f这种形式的特征函数(线性)变成lstm的输出f1(非线性)，
>           这就是在原始crf中引入了非线性，可以更好的拟合数据
>
>           双向LSTM同时考虑了过去的特征(通过前向过程提取)和未来的特征(通过后向过程提取)，一个正向输入序列，一个反向输入序列，再将两者的输出结合起来作为最终的结果。
>
>           算法过程：
>               BiLSTM layer的输出维度是tag size，这就相当于是每个词Wi映射到tag的发送概率，如果BiLSTM的输出矩阵为P，其中，Pi,j代表词Wi映射到tagi的非归一化概率。
>               对于crf来说，如果存在一个转移矩阵A，则Ai,j代表tagi转移到tagj的转移概率。
>               可以使用viterbi解码最优的tag序列
>
>
>       BiLSTM+CRF算法流程：（？？？？？）
>           for epoch
>               for batch
>                   1、BiLSTM+CRF模型前向传播
>                       (1)向后状态的前向传播
>                       (2)向前状态的前向传播
>                   2、CRF层前后向传播
>                   3、BiLSTM+CRF模型后向传播
>                       (1)向后状态的后向传播
>                       (2)向前状态的后向传播
>               done
>           done
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>

- **待续：**
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
