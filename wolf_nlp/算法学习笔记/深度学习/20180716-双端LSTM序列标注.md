### 深度学习 - 双端LSTM序列标注
- **概述**：
>       Bi-directional LSTM
>       参考：https://blog.csdn.net/jerr__y/article/details/70471066
>
>       字标注方法：
>           1、将分词等问题直接变成了一个序列标注问题，而且标注是对齐的
>           2、标注法实际上已经是一个总结语义规律的过程，“李”字是常用的姓氏，一半作为多字词（人名）的首字，即标记为b；而“想”由于“理想”之类的词语，
>               也有比较高的比例标记为e，这样一来，要是“李想”两字放在一起时，即便原来词表没有“李想”一词，我们也能正确输出be，也就是识别出“李想”为一个词
>           3、标注的数目，常用4tag
>               还有6tag和2tag，为什么选择4tag效果更好？
>               因为相比2tag，更多的tag实际上更全面概括了语义规律，相对于6tag，还要总结出第二字、第三字，但是这个角度总结不够合理（不过从新词发现的角度看，6tag更容易发现长词）
>
>       双向LSTM：
>           不管是LSTM或RNN，都是从左往右推进的，因此后面的词会比前面的词更重要，但是对于分词任务来说这是不妥的，
>           因为句子各个字应该是平权的？？？因此出现了双向LSTM，从左往右做一次LSTM，然后从右往左做一次LSTM，然后把两次结果组合起来
>
>       深度学习在分词中的应用:
>           不管是传统的神经网络还是LSTM，它们的做法跟传统的模型都是一样的，都是通过上下文来预测当前字的标签，上下文就是固定的窗口，比如用前后5个字加上当前字来预测当前字的标签。
>           这种做法仅仅是把以往估计概率的方法，如HMM、CRF等，换成了神经网络，整个框架没有变，本质上还是N-gram模型。
>           
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>

- **待续：**
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
