### 深度学习 - 优化算法
- **概述**：
>       一阶优化算法：
>           最常用的一阶优化算法是梯度下降
>           函数梯度：导数dy/dx的多变量表达式，用来表示y相对于x的瞬时变化率。往往为了计算多变量函数的导数时，会用梯度取代导数，并使用偏导数来计算梯度。
>                    梯度和导数之间的一个主要区别是函数的梯度形成了一个向量场。
>           对单变量函数，使用导数来分析；梯度是基于多变量函数而产生的。
>
>           1、梯度下降：
>               这是在神经网络中最常用的优化算法
>               梯度下降的功能是：通过寻找最小值，控制方差，更新模型参数，最终使模型收敛
>               Θ = Θ - η*∇(θ).J(θ)，η是学习率，∇(θ).J(θ)是损失函数J(θ)的梯度
>               梯度下降主要用于在神经网络模型中进行权重的更新，即在一个方向上更新和调整模型的参数，来最小化损失函数。
>               注：2006年引入的反向传播技术，使得训练深层神经网络成为可能。反向传播技术是先在前向传播中计算输入信号的乘积及其对应的权重，然后将激活函数作用于这些乘积的总和。
>                   这种将输入信号转换为输出信号的方式，是一种对复杂非线性函数进行建模的重要手段，并引入了非线性激活函数，使得模型能够学习到几乎任意形式的函数映射。
>                   然后在网络的反向传播过程中回传相关误差，使用梯度下降更新权重值，通过计算误差函数E相对于权重参数W的梯度，在损失函数梯度的相反方向上更新权重参数。
>               缺点：
>                   1、传统的批量梯度下降将计算整个数据集梯度，但只会进行一次更新，因此在处理大型数据集时速度很慢且难以控制，甚至内存溢出。
>                   2、权重更新的快慢由学习率η决定的，并且可以在凸面误差曲面中收敛到全局最优值，在非凸曲面中可能趋于局部最优值
>                   3、在训练大型数据集时存在冗余的权重更新
>                   在随机梯度下降中可以得到解决
>
>           2、SGD(Stochastic gradient descent) 随机梯度下降
>               SGD对每个训练样本进行参数更新，每次执行都进行一次更新，且执行速度更快
>               频繁的更新使得参数间具有高方差，损失函数会以不同的强度波动。这其实将有助于我们发现新的和可能更优的局部最小值，而标准梯度下降将只会收敛到某个局部最优值
>               缺点：
>                   1、由于频繁的更新和波动，最终将收敛到最小限度，并会因波动频繁存在超调量
>               小批量梯度下降算法，则可以解决高方差的参数更新和不稳定收敛的问题
>
>           3、Mini Batch Gradient Descent 小批量梯度下降：
>               小批量梯度下降，因为对每个批次中的n个训练样本，这种方法只执行一次更新，避免了SGD和标准梯度下降中存在的问题
>               优点：
>                   1、可以减少参数更新的波动，最终得到效果更好和更稳定的收敛
>                   2、可以使用最新的深层学习库中通用的矩阵优化方法，使计算小批量数据的梯度更加高效
>                   3、通常小批量样本的大小从50到256，可根据实际问题有所不同
>                   4、在训练神经网络时，通常都会选择小批量梯度下降算法
>               这种方法有时候还是被称为SGD
>               挑战：
>                   1、很难选择出合适的学习率
>                   2、相同的学习率并不适用于所有的参数更新
>                   3、神经网络中，最小化非凸误差函数的另一个关键挑战是避免陷于多个其他局部最小值中
>
>
>       二阶优化算法：
>           二阶优化算法使用了二阶导数（也叫做Hessian方法）来最小化或最大化损失函数。由于二阶导数计算成本高，所以使用并不广泛。
>
>
>
>
>
>
>
>
>
>
>
>
>
>

- **待续**
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
