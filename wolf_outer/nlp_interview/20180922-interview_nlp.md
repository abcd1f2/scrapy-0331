### interview - nlp
- **概述：**
>       http://cheesezhe.github.io/categories/%E6%B1%82%E8%81%8C%E7%BB%8F%E9%AA%8C/     自然语言处理面试经验
>
>       1、请列出几种文本特征提取算法
>           文档频率、信息增益、互信息、X^2统计、TF-IDF
>       2、几种自然语言处理开源工具包
>           CRF++、tensorflow、gensim、scipy
>       3、无监督和有监督算法的区别
>       4、kNN，kMeans，决策树，随机森林等
>       5、数据库以及线程死锁产生的原理及必要条件，简述如何避免死锁
>       6、
>
>       1.机器学习常用的分类算法，Logistic回归，SVM，Decision Tree，随机森林等相关分类算法的原理，公式推导，模型评价，模型调参。模型使用场景
>       2.机器学习常用的聚类算法，Kmeans,BDSCAN，SOM（个人论文中使用的算法），LDA等算法的原理，算法（模型）中参数的确定，具体到确定的方法；模型的评价，例如LDA应该确定几个主题，Kmeans的k如何确定，DBSCAN密度可达与密度直达。模型使用场景
>       3.特征工程：特征选择，特征提取，PCA降维方法中参数主成分的确定方法，如何进行特征选择
>       4.Boosting和bagging的区别
>       5.数据如何去除噪声，如何找到离群点，异常值，现有机器学习算法哪些可以去除噪声
>       6.HMM与N-gram模型之间的区别
>       7.梯度消失与梯度爆炸
>           1、梯度剪切这个方案主要是针对梯度爆炸提出的，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸
>           2、另外一种解决梯度爆炸的手段是采用权重正则化（weithts regularization）比较常见的是l1正则，和l2正则
>           3、梯度消失和梯度爆炸哪种经常出现？可以看到，当w越大，其wx+b很可能变的很大，而根据上面sigmoid函数导数的图像可以看到，wx+b越大，导数的值也会变的很小。因此，若要出现梯度爆炸，其w既要大还要保证激活函数的导数不要太小。
>       8.奥卡姆剃须刀原理
>       9.TCP三次握手的原理，为什么是三次而不是其他次
>       10.进行数据处理时，如何过滤无用的信息（例如利用正则表达式提取或者其他方法），数据乱码的处理
>       11.交叉熵与信息熵，信息增益与信息增益率，gini系数，具体如何计算
>       12.BIC准则（贝叶斯信息准则）与AIC（赤池信息准则）
>       13.需要手写代码（此次面试：字符串的操作）
>       14.前向传播与反向传播
>       15.常见的损失函数
>
>       1、LSTM的LSTM的实现原理
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>

- **待续：**
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
